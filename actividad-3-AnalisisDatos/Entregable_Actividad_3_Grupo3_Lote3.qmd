---
title: "Actividad-3"
format: html
editor: visual
---

```{r Importar Liberias}
getwd()
setwd("/Users/alejandrocamposlamas/Proyectos/ACTIVIDADES_UNIR/actividad-3-AnalisisDatos")

library(readxl)
library(dplyr)
library(caret) # para matriz de confusión
library(rpart) #Para realizar el árbol
library(rpart.plot) #Para dibujar el árbol
library(factoextra) # para clusteres
library(NbClust)
library(zoo) # para ts diario
library(forecast)
library(dplyr)
library(lubridate)

getwd()

set.seed(123)

```

## 1. ¿De qué tipo de variables se compone mi base de datos? Realize un análisis descriptivos de las varaibles de la base de datos (Estadística descriptiva, correlación)

Cargamos los datos en una variable.

Tenemos variables númericas, númericas booleanas y de caracteres (char).

Procedemos a seleccionar las variables que necesitaremos para el ejercicio y convertir las variables a sus tipos pertinentes.

```{r}


datos <- read_excel("datos/DataSet SQL_Act3_ADMN.xlsx", 
    sheet = "Var Discreta Adq Bicicleta")

# str(datos)
# unique(datos$Country)

```

```{r Cargar y Transformar}
#| include: false
# cargamos datos
datos <- read_excel("datos/DataSet SQL_Act3_ADMN.xlsx", 
    sheet = "Var Discreta Adq Bicicleta")


# vemos la estructura y convertimos los datos a su formato adecuado
# str(datos)
#colSums(is.na(datos)) # verificamos que no existe ningun null
# datos[is.na(datos)] <- 0 # valores nulos se reemplazarían por 0
# Quitamos la columna PersonType dado que todos su valores son IN
datos <- datos%>% select(!PersonType)

#quitamos aquellas paises que son agrupaciones
datos <- datos %>% 
  filter(!(Country %in% c("Northwest", "Southeast", "Central", "Southwest", "Northeast")))


# a factores datos que son categorías
datos$Education <- as.factor(datos$Education)
datos$PersonID <- as.factor(datos$PersonID)
datos$CustomerID <- as.factor(datos$CustomerID)
# Los booleanos le creamos una columna condicional
#BikePurchase
#HomeOwnerFlag

datos <- datos %>%
  mutate(
    BikePurchase_f = factor(BikePurchase, labels=c("FALSE", "TRUE")),
    HomeOwnerFlag_f = factor(HomeOwnerFlag, labels=c("FALSE", "TRUE"))
  )

print(datos)

```

Hemos considerado crear columnas condicionales para aquellas variables que fuera numéricas y booleanas de tipo 0, 1. De esta forma podremos hacer un descriptivo estadístico más limpio y utilizar los factores para los modelos.

Además hemos eliminado la columna "PersonType" dado que no aporta ninguna información relevante.

Por último hemos quitado aquellos valores de Country que se referían a regiones y no países: "Northwest", "Southeast", "Central", "Southwest", "Northeast"

A continuación procedemos a realizar el análisis descriptivo:

```{r análisis descriptivo}

# describimos los estadísticos numéricos
datos %>% select(where(is.numeric)) %>% summary(use="complete.obs")

# observamos cómo se relacionan las variables
datos %>% select(where(is.numeric)) %>% cor(use="complete.obs")
 
```

**Descriptivo**

Vemos como nuestra medida de negocio TotalAmount tiene una ditribución sesgada a la derecha puesto que la media es significativamente mayor que la mediana, esto quiere decir que puede que existan algunos valores atípicos, como por ejemplo su valor máximo (13295.38), pero también es posible que el sesgo sea una característica de cómo se comporta este negocio.

**Correlación**

Observamos algunas variables que correlacionan fuerte, como TotalAmount y BikePurchase; cuando la compra incluye bicicletas la facturación incrementa. Y TotalChildern y Age; cuanta más edad mayor número de hijos.

## 2 ¿Podría estimar un modelo que le permitiera **clasificar** si un individuo realizará una compra o no?

*Use un modelo de regresión logística y un árbol de decisión. (utilice la pestaña Var Discreta Adq Bicicleta)*

En este caso la variable a predicir sería BikePurchase. Procedemos a crear los subsets necesarios para modelar (Train & Test)

```{r split, train, test }

#split: seleccionamos el monto de entrenamiento y lo guardamos en la variabble de train
indice <- sample(1:nrow(datos), size = 0.8 * nrow(datos))

# Train: entrenamos al modelo con el subset de testeo
train <- datos[indice, ]

# test: tomamos el monto a testar
test <- datos[-indice, ]



```

Una vez tenemos los subsets de entrenamiento y testeo pasamos entrenar al modelo

Hemos hecho dos iteraciones, eliminaod "Gender" dado que tiene un P-value \> 0.05 para evitar overfitting,

```{r entrenamiento logit}
modelo_logit <- glm(BikePurchase ~ Country + MaritalStatus + HomeOwnerFlag_f + Education + YearlyIncome + Occupation, data = train, family = "binomial")

summary(modelo_logit)


```

Procedemos a hacer el árbol:

```{r entrenamiento arbol}

# 1. Ajuste del modelo 
modelo_arbol <- rpart(BikePurchase ~ Country + MaritalStatus + HomeOwnerFlag_f +  Education + YearlyIncome + Occupation, 
                      data = train, 
                      method = "class") 

# 2. Visualización
rpart.plot(modelo_arbol,main = "Árbol de Decisión: Predicción de Bike Purchase",     
           type = 2, 
           extra = 104, 
           nn = TRUE)


```

## 3 ¿Cómo de buenos son los modelos? ¿Cuál o cuáles son las variables más importantes a la hora de realizar una compra por parte del cliente? (utilice la pestaña Var Discreta Adq Bicicleta)

Hacemos la matriz de confusión del Logit

```{r matriz confusión logit}
pred_logit_prob <- predict(modelo_logit, newdata = test, type = "response")

# 1# 1. Convertimos tus probabilidades a clases (0 o 1) con el corte de 0.5
pred_clase <- ifelse(pred_logit_prob > 0.5, 1, 0)

# 2. Convertimos AMBOS a factor asegurando que tengan los mismos niveles

pred_factor <- factor(pred_clase, levels = c("0", "1"))
real_factor <- factor(test$BikePurchase, levels = c("0", "1"))

# 3. Magia: Generamos la matriz completa
confusionMatrix(data = pred_factor, reference = real_factor)
```

## 

hacemos la matriz de confusión del arbol:

```{r matriz confusión arvol}

# 1. Pide la clase directamente (type = "class")
pred_clase_arbol <- predict(modelo_arbol, newdata = test, type = "class")

# 2. Aseguramos que sea factor y tenga los mismos niveles que la realidad
# Nota: predict con type="class" ya suele devolver factor, pero esto es doble seguridad
pred_factor_arbol <- factor(pred_clase_arbol, levels = c("0", "1"))
real_factor_arbol <- factor(test$BikePurchase, levels = c("0", "1"))

# 3. Matriz
confusionMatrix(data = pred_factor_arbol, reference = real_factor_arbol)


```

Ambos modelos presentan un rendimiento global moderado (Accuracy \~60%), siendo el **Árbol de Decisión muy lgeramente superiro, casi idéntico**.

Sin embargo, al analizar los errores, vemos un comportamiento interesante: el modelo de árbol es **'agresivos' prediciendo ventas**:

-   Es buenos detectando potenciales clientes (**Sensibilidad \~ 70%**).

-   Pero fallan bastante al filtrar a los que no están interesados (**Especificidad \~50%**). Es decir, el modelo tiende a pensar que mucha gente comprará la bici cuando en realidad no lo hará (muchos Falsos Positivos)."

El modelo de logit justamente al revés, es bueno prediciendo a aquellos clientes que no están interesados siendo su Especifidad \~ 70%

Respecto a lkas as variables más importantes son las siguientes: País, Educación e Ingresos.

```{r}
# Ver qué variables usó realmente el árbol y cuánto importaron
modelo_arbol$variable.importance
```

## 3 Con el total de la muestra, aplicando técnicas de aprendizaje no supervisado ¿sería capaz de definir tipologías de clientes diferentes? Puede usar las variables que considere oportunas (utilice la pestaña Var Discreta Adq Bicicleta)

Utilizaremos Clusterización con técnica K-means para descubrir patrones naturales.

Para utilziar el algoritmo de Kmeans solo pueden haber variables numéricas, por lo que el resto habría que convertirlo en variables numéricas, y los escalamos dado que tenemos variables booleanas

```{r Clusterizacion}

datos_cluster_input <- datos %>%
  select(where(is.numeric)) %>%
  select(-BikePurchase)

datos_escalados <- scale(datos_cluster_input)
```

```{r kmeans}
clusterizacion <- kmeans(datos_escalados, centers = 4, nstart = 25)
```

Hemos elegido clusterizar por 4 elementos (previamente hizimos la técnica del códo y salían de recomendación entre 3 y 4 elementos)

```{r interpretación clustering}
resumen_clusters <- datos_cluster_input %>%
  mutate(Cluster = clusterizacion$cluster) %>%
  group_by(Cluster) %>%
  summarise(across(everything(), mean)) 

print(resumen_clusters)
```

Como podemos ver por ejemplo el primer grupo es el qué más compra y se caracteriza principalmente por no tener casa y tener pocos hijos.

Pero vamos a hacer un árbol de decisión para saber qué variables son las más importantes.

```{r arbol de cluster}
datos_para_arbol <- datos_cluster_input %>%
  mutate(Cluster = as.factor(clusterizacion$cluster)) 

# 2. Creamos el árbol explicativo
# "Cluster ~ ." significa: Predice el Cluster usando TODAS las demás columnas
arbol_explicativo <- rpart(Cluster ~ ., 
                           data = datos_para_arbol, 
                           method = "class")

# 3. Lo visualizamos
rpart.plot(arbol_explicativo, 
           type = 2, 
           extra = 101, 
           nn = TRUE,
           box.palette = "BuGn",
           main = "Reglas que definen cada Cluster")
```

```         
```

## 5 Realice una predicción de las ventas totales (utilice la pestaña ST Ventas totales, la primera columna) para los próximos 2 meses.

```{r cargar datos}
ventas <- read_excel("datos/DataSet SQL_Act3_ADMN.xlsx", 
    sheet = "ST Ventas Totales ")

#colnames(ventas)
class(ventas$OrderDate) # Nos aseguramos que la columna de fecha está bien factorizada en fecha
min(ventas$OrderDate)
#unique(ventas$OrderDate)
```

Dado que los datos están registrados por día, y la actividad solicita una predicción por mes, procedemos a agrupar los registros por mes

```{r }

ventas_mensuales <- ventas %>%
  mutate(Mes = floor_date(OrderDate, "month")) %>%
  group_by(Mes) %>%
  summarise(TotalSales = sum(Sales...2, na.rm = TRUE))

# 2. Crear la serie temporal 

series_ventas <- ts(ventas_mensuales$TotalSales, start = c(2011, 5), frequency = 12)

# 3. Graficar la serie
plot(series_ventas, main="Ventas Mensuales", col="blue", lwd=2)
```

```{r}
# Realizar la predicción
modelo <- auto.arima(series_ventas)
prediccion <- forecast(modelo, h = 2) # h=2 son los 2 meses que te piden

# Ver los valores de la predicción
print(prediccion)

# Graficar la predicción
plot(prediccion, main="Predicción Ventas Próximos 2 Meses")
```
