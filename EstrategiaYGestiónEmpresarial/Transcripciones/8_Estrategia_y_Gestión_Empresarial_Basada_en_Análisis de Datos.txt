El de grabar en la nube. Ah, listo, listo. Perdón, perdón, gracias. Gracias, perdón. Gracias, gracias, gracias. Se estaba poniendo a prueba a ver si alguien caía y tal. Tenía un punto positivo en el examen, pero lo ha conseguido atención telefónica. Muchas gracias. Para un día que se nos olvida grabar y están por aquí nuestros compis. Muchas gracias. Soy muy despistado, soy muy despistado, perdonad. Venga, pues empezamos como el que no quiere la cosa. Vamos a darle para atrás. Venga, tema 5, Data Warehouse, almacén de datos. ¿Qué conceptos vamos a tratar hoy? Vamos a hablar sobre los procesos ETL, qué beneficios nos aportan. Vamos a hablar sobre Data Warehouse, una pieza clave dentro del esquema de sistema de información e inteligencia de negocios. Y luego vamos a intentar, bueno, pues hacer un ejemplo sobre, oye, cómo podemos calcular escenarios de rentabilidad de promociones que queremos lanzar, promociones segmentadas, incluso saber o estimar la rentabilidad que cada promoción podemos alcanzar en cada una de ellas antes de lanzarla, ¿vale? En esas promociones segmentadas para incluso decidir qué promoción lanzamos, bueno, pues calcular una estimación sobre la rentabilidad esperada de cada una de ellas, ¿no? Vamos a ver de qué manera el tener la información en un data warehouse nos permite hacer este tipo de análisis. Si nos da tiempo a verlo hoy enteramente, ya en estos 40-38 minutos que nos queda, si no, seguiremos la semana que viene en una parte de la resolución, sin ningún tipo de problema. Como decía, lo primero que vamos a comentar, lo primero que tenemos que ver es esto de los procesos ETL, ¿a qué se refieren? Bueno, pues la principal necesidad que tenemos en un esquema de inteligencia de negocios es construir, consolidar toda la información que tenemos de nuestros clientes, en este caso si estamos hablando de clientes, o de nuestro negocio. Si nuestro negocio es supply chain, logística, paquetes, lo que fuere. Necesitamos consolidar toda la información que tenemos. Esa consolidación se lleva a cabo en este almacén de datos, en ese data warehouse que os decía. En entornos cloud también puede recibir el nombre de Data Lake, ¿vale? Lago de datos. Bueno, pues sea como fuere, el concepto es exactamente el mismo. Pues bien, para conseguir esa consolidación, ¿vale? lo que necesitamos es poder acceder a todos los orígenes de información que tenemos, bien sea o bien el CRM, que os decía, todos los contactos que tenemos con el cliente desde el punto de vista de comunicaciones personalizadas que mandamos por los múltiples canales que tenemos, el email, el SMS SMS con tecnología RCS, que os decía las app push, web push, etc todas esas comunicaciones personalizadas que hacemos a los clientes, pues necesitamos saber el envío y las vueltas que se llaman, quién ha interactuado con cada uno de ellos, quién ha abierto el email, quién ha hecho click, dónde lo ha hecho, etc. Todo eso es un origen de información saliente, pero también la entrante. También podemos tener información de nuestros clientes en función de las llamadas que hacen a nuestro contact center, si han llamado para algún tipo de información al respecto de nuestro servicio, del producto, lo que fuere. O han tenido una incidencia ya, tienen una reclamación, le damos servicio. Bueno, pues toda esa información, entrantes desde luego que es súper, súper relevante desde el punto de vista luego de gestión personalizada de nuestros clientes. Por supuesto, otro origen es la facturación, el billing. Otro origen, el transaccional. Saber quién ha comprado, qué ha comprado, por qué canal, qué productos, el SKU, el precio de las unidades. Todo eso es súper, súper, por supuesto, súper relevante. Luego, oye, pues ficheros que tengamos por ahí, ¿no? Ficheros incluso que nos manda nuestro proveedor de enriquecimiento de datos o, bueno, pues un fichero que tenemos por ahí. Siempre en todas las empresas hay ficheros Excel que guarda la gente, ¿no? Hojas de gestión, etcétera. Bueno, pues toda esa información, digamos que es necesario consolidarla. Consolidarla. Pues bien, lo primero para consolidar esa información hay que acceder a ella. Luego habrá que transformarla a procesos de data quality etc. y luego cargarla. Esas tres fases son las fases que se encargan este tipo de procesos. Los procesos ETL, ETL no es más que un acrónimo, ¿vale? Extracción, transformación y la L es carga, ¿vale? En inglés, load, ¿vale? Entonces, ya sabemos para qué sirven, ¿vale? Sirven primero para acceder a la información, la extracción, allá donde estén los datos, ¿vale? En todas las cajas que tenemos en todos los puntos de venta, en nuestra plataforma, digamos, de Marketplace, en nuestra web, en nuestro e-commerce, en nuestra app, en nuestras tiendas, acceder a toda la información que tenemos, sea cual sea el repositorio o el operacional, transformarla, es decir, toda esa información hay que depurarla, hay que consolidarla, hay que hacer procesos de depuración, procesos de calidad del dato. si hay campos que en alguna transacción o en algún registro pueden venir con datos missing, tener las reglas preestablecidas para o bien mantener ese dato missing o bien incorporar un dato que puede ser la media, la mediana, el dato más frecuente, lo que fuere, ¿vale? Y luego cargarlo, ¿vale? Establecer de qué manera toda esa información de manera periódica, diaria, en real time, mensual, quincenal, semestral, anual, de como fuere, ¿vale? Toda esa información, ¿cómo la vamos a cargar, vale? En nuestro Data Warehouse con el fin de asegurar que la disponibilizamos, pues, para hacer todo tipo de reportings, para hacer todo tipo de analítica, para hacer procesos de data mining, ¿vale? etc, etc, etc esto es lo que os comentaba ¿vale? es un proceso por tanto básico ¿vale? lo pongo en mayúsculas porque para mí, sin ser técnico ¿vale? es uno de los principales y el gran desconocido valedores, principales valedores y el gran desconocido de que podamos extraer bueno, o que podamos tomar las decisiones más correctas ¿Y por qué digo esto? Porque si nos aseguramos de tener acceso a toda la información, a todos los orígenes de información, si nos aseguramos de generar estos procesos, esta fase de limpieza, depurar la información, asegurar que lo que cargamos está en tiempo y en forma, es decir, lo cargamos a nuestro data warehouse con el fin de tener una estructura que facilite el acceso y con el fin de asegurar que la información a la que se accede es de calidad, las decisiones que vamos a tomar en base al análisis, pues seguro que son de calidad. Si nos fallan, para hacer más ETLs otra vez, infinitos... ¿A qué te refieres, Alejandro? Perdón, no sé a qué te refieres. Para hacer más ETLs otra vez, infinitos... Sí, porque luego viene al Data Warehouse y luego para el reporting hay que volver a hacer más ETLs. Ah, bueno, vale, vale. Y luego otro ETL y otro ETL. Vale, yo... Ah, vale, no, no. está bien tirada, ¿vale? Porque yo solo he pintado ETLs aquí, pero por supuesto, en la parte de reporting, fundamentalmente en la parte de reporting, ¿vale? Pues habrá también ETLs, ¿vale? ¿Por qué? Porque tendremos aquí procesos de consolidación de la información en función de lo que queremos mostrar y de la manera en la que queremos mostrar y los KPIs que queremos mostrar. Pues para mostrar esos KPIs y los podemos analizar a través de determinadas dimensiones de análisis, pues aquí también habrá ETLs que consoliden esa información a ese nivel de granularidad. Cuando creamos algún tipo de modelo predictivo, pues evidentemente esto no es flor de un día. El modelo predictivo se desarrolla, se implementa y se utiliza. En la fase de implementación es importante también asegurar que ese modelo predictivo se actualiza con la periodicidad que se estime. ¿vale? Diaria, semanal, mensual. Para hacer eso, para conseguir eso, pues habrá que, digamos, realimentar el modelo con las variables actualizadas, ¿vale? A la fecha de actualización. Ahí, para eso, también tendremos ETLs, ¿vale? Así que, fantástica nota, Alejandro, muchas gracias que no te había pillado, ¿vale? Pensaba que había dicho algo. Venga, muchas gracias. Oye, me cortáis cuando queráis. Yo veo el chat por ahí porque no os veo, pero me cortáis o levantáis la mano cuando consideréis. Visto entonces, súper importante los procesos, súper importante los procesos. ¿Cuáles son algunas consecuencias de no aplicar este proceso? En la diapos se ve mejor que en la pantalla. Puede haber duplicados. Estos son casos reales. No os digo la empresa, pero son casos reales. donde yo hice algunas cosas de analítica para esa empresa. Esa empresa donde el usuario se daba de alta a través de su email y donde a nivel interno, en base de datos, el ID del usuario era el email. ¿vale? entonces evidentemente ¿qué podía fallar? pues todo ¿no? no había procesos de deduplicación, es decir, si el usuario se registraba y pasado tres meses se volvía a registrar pues con otro email pues también era válido, ¿vale? aunque la dirección fuera la misma y el nombre y los apellidos y el DNI fueran el mismo no había un proceso de deduplicación y por lo tanto un mismo usuario podía estar varias veces de tal manera que dependiendo de con el email con el que entraras, bueno, pues podías comprar un día o con otro email podías comprar otro, con lo que conlleva eso desde el punto de vista de desagregación del dato, ¿vale? Yo tenía cuatro usuarios, cuando no eran cuatro usuarios era realmente uno o realmente eran tres o realmente eran dos y donde si tenía un usuario que tenía dos emails y tenía, por tanto, como dos clientes, un día compraba con un email, otro día compraba bajo el otro email, un día compraba 100 euros, otro día compraba 50, realmente el usuario lo que compra son 150, ¿vale? Pero para mí esa visión no la tenía, no lo podía consolidar porque para mí a nivel interno eran dos clientes distintos, ¿vale? Para evitar eso, ¿vale? Primero, hay que intentar evitar que la clave principal, pues, sea una clave como el email Y antes de eso incluso, si existe esa situación, lo que hay que intentar es que caracteres en blanco, adelante o detrás del email, se borren a la hora de incorporar ese dato en base de datos. Eso es un proceso, por ejemplo, de transformación. Si tú tienes una cadena de caracteres, un stream, un campo stream, lo que intentas es, mediante un proceso de transformación, eliminar los blancos que haya como primer carácter y como últimos carácter. Para evitar esa situación siempre que damos al email, damos a la barra del espacio y luego le damos a aceptar. Ese email será distinto del que no tienes espacio porque son caracteres diferentes. Para evitar esas situaciones, para evitar estos duplicados, para evitar o minimizar la toma de decisiones incorrectas, para eso existen procesos en la TED de transformación, esos procesos de limpieza y de data quality y de sanity check que comentábamos antes. Por otro lado, las direcciones. Fijaros las direcciones que tenía esta gente. No era un campo obligatorio. A lo mejor hay que pensar si lo necesitamos sí o sí hacerlo obligatorio, pero además de hacerlo obligatorio, normalizarlo. ¿Por qué? Porque si tú lo haces obligatorio, pues la gente o bien intenta poner el nombre correcto, como esta persona de aquí, o inventa cualquier cosa. Por ejemplo, este de aquí. O te pone un punto o te pone ZZZ. ¿Para qué nos sirve tener bien las direcciones? Pues, evidentemente, si tenemos que hacer un delivery, ¿vale? Bueno, pues tener la dirección, evidentemente, pues es mandatory. Pero más allá de eso, si sabemos dónde vive el cliente, podemos enriquecer la información, ¿vale? Con fuentes de datos externas. Y, por lo tanto, ese origen de información adicional que podríamos tener, pues no lo vamos a disponibilizar. Y eso nos puede coartar, nos puede limitar determinados procesos de segmentación de nuestros clientes, procesos de toma de decisiones de apertura o no de una hacienda física. no sabemos dónde vive la gente, no sabemos de dónde viene la gente que nos compra, bueno, pues tiene muchísimas implicaciones y, por supuesto, si queremos hacer algún tipo de comunicación offline, ¿vale?, de mandar, pues esto, los típicos folletos de papel, que, bueno, cada vez se mandan menos, pero todavía hay muchas empresas que invierten gran cantidad de dinero en envíos offline, pues, evidentemente, si no tenemos la dirección informada, pues no nos va a valer, no nos va a valer para nada, podemos, digamos, mandar folletos offline a través de procesos de comunicación. de buzoneo genérico. Entonces, yo creo que eso claramente pone en valor la necesidad de todos estos procesos de ETL. No vamos a hablar o no vamos a ejemplificar, desde el punto de vista técnico, procesos ETL. Al que le pique el gusanillo por conocer, oye, pues, por ejemplo, y Alejandro, que te veo ahí puesto, pues Pentaho, ¿vale? Hasta donde yo sé, pues es una herramienta de tele que es gratuita, ¿vale? Que la usan en empresas, ¿vale? O sea, no tiene que ser mala. y que, bueno, pues para hacer determinadas cosillas, si estáis ahora ahí en el entorno BigQuery y este tipo de cosas, pues podéis trastear para ejemplificar un poquito más desde el punto de vista técnico, ¿no? Pero bueno, pues si no, yo creo que hay herramientas por ahí. No estamos en este máster para tener ese nivel técnico, ¿vale? Eso es un perfil mucho más de IT, pero bueno, al que le pique el gusanillo, pues que sepa que, por ejemplo, esa herramienta, siendo gratuita, pues oye, la puede trastear. Data Warehouse, nuestra segunda parte o segundo punto a tratar en la clase de hoy. Bueno, pues lo que os decía, al final lo que pretendemos con el Data Warehouse es centralizar la información, que sea un repositorio útil para los usos que se le van a dar. Vale, existen distintos usos. Nosotros vamos a pensar más desde un punto de vista analítico, desde un punto de vista de reporting y de analytics. Bueno, pues deberemos tener esa base de datos, ese data warehouse, ese almacén de datos, pues estructurada de manera que nos permita llevar a cabo esos análisis o esos reportings de una manera optimizada. Luego, yo creo que veremos un ejemplo, pero os lo adelanto. Hay dos formas de ver, por ejemplo, si tienes los tickets de compra de los clientes, podemos ver lo que compran los clientes desde dos puntos de vista. Podemos verlo a un nivel agregado, a nivel de ticket. Nosotros le llamamos cabecera de ticket. Y sabes que este cliente X te ha comprado tal día por valor de 150 euros en tal sitio y tal. Y tienes el detalle del ticket agregado. Y luego tienes lo que es el escayú, el detalle del ticket. Y tienes que ese ticket de 150 euros lo que tienes es desglosado en tantas líneas como productos haya comprado. Y en cada línea donde reflejas qué parte de esos 150 euros ha gastado en cada uno de los productos, tienes información sobre a lo mejor unidades o lo que fuere. Si tiene un descuento o no tiene un descuento, etc. Son dos formas distintas de ver la misma información, pero desde el punto de vista de computación, desde el punto de vista de agregación, evidentemente los usos son muy distintos. Luego, si queréis, entramos en detalle. Pero pensar, si yo tengo 2 millones de clientes o tengo 5 millones de tickets mensuales, ¿vale? Tickets, 5 millones, fijaros los productos que puede haber ahí, las líneas, si voy a bajar a detalle de ticket, puedo tener 50 millones de líneas o más. ¿Vale? Entonces, desde el punto de vista de computación es muy distinto si a partir del detalle tengo que crearme la cabecera o que si ya tengo la cabecera de ticket calculada, ¿vale? Que me permite tener esa visión algo más global de manera agregada, con lo cual ahorro tiempo de computación y por lo tanto también pasta. Venga, pues desde una visión un poquito más departamental del data warehouse existen los que se denominan los data marts. Los data marts son como, digamos, data warehouse algo más específicos. ¿Vale? Pues un departamento en concreto, o bien compras, o bien facturación, o bien una parte de facturación, una parte de performance, o bien una parte de animación comercial, o bien una parte de fidelidad. No necesitan tener acceso a toda la información de la empresa, necesitan únicamente, bueno, pues tener acceso a una parte de ella. Bueno, pues se les gestiona lo que se denominan unos datamars, que son o bien hacer como un slicing del data warehouse, es decir, hacer una disección del data warehouse, sacar la información necesaria para ese departamento de compras o de facturación y montarle una base de datos en paralelo, de tal manera que se actualice en la periodicidad que corresponda y que solo accede a estas personas. Bueno, pues tienen ahí la información suficiente, pues fantástico. Y a partir de ahí, pues pueden hacer todo tipo de análisis, reporting, etc. Con una ventaja. Antiguamente esto existía, lo que sucedía es que cada departamento o cada equipo tenía sus Excel que difícilmente compartían, no había procesos de data quality, no existían, pues digamos, un libro blanco desde el punto de vista de data governance, ¿no? No existía, pues, oye, cómo tenemos que calcular o qué criterios son los que aplican a la hora de calcular determinados KPIs y cada departamento, pues, hacía y deshacía como quería, ¿no? Eso, bueno, pues tenía evidentemente sus contras, que eran todas y una de ellas era, bueno, pues que no existía una información consolidada de la que partías en Y por lo tanto, pues los datos podían estar desactualizados, etcétera. En este no es el caso, ¿vale? Porque estos data marts se generan a partir del data warehouse. Entonces, ese no es el caso. La información está perfectamente consolidada en el data warehouse y lo que tenemos, lo que hacemos es tener una visión parcial de ese data warehouse que utilizan estos equipos. Esto cada vez más, o cada vez, bueno, se utiliza mucho, ¿vale? No generando unos data marts específicos, si fuera una base de datos relacional, ¿vale? Como ahora mismo en cloud, pues se crean datasets específicos, ¿no? De tal manera que si tú estás en BigQuery, accedes a un esquema, ¿vale? A un dataset donde tienes distintos sistemas informacionales. Nosotros, por ejemplo, mi equipo, pues accedemos al Data Warehouse corporativo, pero no accedemos a todos los datasets. Evidentemente, al dataset, por ejemplo, de recursos humanos, pues no accedemos. Y luego, si alguien quiere acceder a un determinado dataset, pues le tiene que validar. Yo, por ejemplo, si alguien quiere acceder a un dataset de cliente, yo soy en España quien le valida o no el acceso. Lo comento qué necesidad, qué funcionalidad tiene, qué usos le va a dar al dato y le traslado esa información y la comento con el DPO, el Data Protection Officer. Bueno, pues si entre los dos decidimos que esa persona tiene que tener acceso a esa información, pues yo le valido la información, le valido el acceso a la información. Entonces, los accesos, evidentemente, pues están restringidos. Como concepto, por supuesto, el Data Warehouse es un elemento imprescindible en el proceso de toma de decisiones. Es el centro de nuestro sistema de información. Es el core de nuestro sistema de información donde se incorporan todo tipo de fuentes externas. El Instituto Nacional de Estadística, de Investigaciones de Mercado, de Datos de Mercado, Datos de Competidores, Procesos de Enriquecimiento, lo que fuere. Por supuesto, todos nuestros inputs de analítica avanzada. Si lo que desarrollamos es un modelo predictivo, un sistema de recomendación de producto, ese input lo utilizaremos para tomar decisiones, para segmentar a nuestros clientes. El output de todos esos modelos se incorpora, por supuesto, con una determinada estructura al data warehouse. Y, por supuesto, todo lo relacionado con las comunicaciones, la navegación web, el transaccional que os comentaba, todo el dato que tenemos a nivel de socio demográfico de los clientes, las encuestas que hacemos, toda la navegación de la app, etc. etcétera, se incorpora, por supuesto, y disponible en nuestro Data Warehouse. ¿Y para qué? Pues para tomar las decisiones, digamos, más correctas. Para tomar esas decisiones, las más correctas posibles, en la parte de construcción de este Data Warehouse, recordemos la importancia capital que tiene nuestro proceso. o nuestros procesos LL. Dos formas sencillas de construir nuestro esquema de Data Warehouse. Sobre todo en base de datos Oracle. Porque en Cloud, por temas de arquitectura, se pueden construir mucho más aplanadas. Pero imaginemos que estamos en una estructura de Teradata, un Oracle, una base de datos relacional en sí. Tenemos dos formas de montar una estructura de relación entre las tablas que tenemos en nuestro Data Warehouse. El primero de ellos es el modelo estrella. Donde tenemos en todos los esquemas de datos, tenemos tablas de hechos, que son las tablas que contienen meramente los datos. Por ejemplo, las ventas. La tabla de ventas es una tabla de hechos. Porque contiene los datos en sí. Y luego están las tablas de dimensiones. Las tablas de dimensiones que son las tablas que contienen las variables a través de las cuales contienen, perdón, las dimensiones de análisis a través de las cuales podemos analizar los datos contenidos en las tablas de hechos, en este caso las ventas. Un ejemplo, una tabla, una dimensión sería el tiempo, otra dimensión sería la tienda, otra dimensión sería el cliente y otra dimensión sería el producto. ¿De qué manera se construye un modelo estrella? Pues de la forma en la que lo visualizamos. tenemos la tabla de hechos y bueno, pues cada una de las dimensiones ¿vale? todas las variables por las cuales podemos analizar dentro de esa dimensión pues los datos que tenemos en la tabla de hechos todas las dimensiones están en la misma tabla todas las dimensiones de la variable o de la dimensión tiempo están en la tabla de tiempo ¿vale? está el time key que sería la hora exacta está el día, está el mes, está el año está el semestre, etcétera, etcétera a nivel de producto Pues, evidentemente, tengo el ID de producto y tengo su color, la marca, la descripción, la sección, la subsección, el tipo, el subtipo, todas las dimensiones por las cuales yo puedo analizar las ventas a través de la dimensión producto. Cada una de las dimensiones se relaciona por una o varias variables. En este caso, la dimensión tiempo se relaciona con la dimensión ventas a través del time key, que sería la hora exacta. Exacto. A través, las ventas a través del ID de cliente, ¿vale? Se relacionan ambas tablas. A través con la tienda, pues se relaciona a través del ID Store y, bueno, pues si es a través de la web, pues a través de un código de tienda que tiene la 58 en nuestro caso, es la tienda, la web, ¿no? Y a través del producto, pues a través del producto aquí. Como digo, estas relaciones, y ahora lo vamos a ver, pueden ser de uno a uno, de uno a varios, ¿vale? O de varios a varios, etcétera, etcétera, ¿vale? Dependiendo de cómo se relacionen, de cuáles son las, digamos, las claves principales de cada tabla y, bueno, cómo se relacionan esas claves principales entre cada tabla de dimensión con la tabla o las tablas de edición. Lo interesante, lo importante, es que en el modelo estrella no existe una jerarquía en las tablas de dimensiones. Lo que os digo, todas las variables por las cuales podemos analizar las ventas, en este caso, a través de la dimensión tiempo, todas las variables están contenidas en la misma tabla. Y esa es la diferencia con respecto al modelo Snowflake. El modelo Copo de Nieve, pues sí que existe una jerarquía, sí que existe una jerarquía en las tablas de dimensiones. Yo puedo tener mi tabla de ventas, mi tabla de hechos y luego puedo tener jerarquías a nivel, por ejemplo, tienda. Lo que antes tenía a nivel de tienda, todas las dimensiones por las cuales yo puedo clasificar, categorizar la tienda, en qué región, qué tipo de tienda, qué dirección, qué ciudad, qué código postal, todas esas dimensiones por las que yo puedo categorizar una tienda y puedo analizar las ventas a través de cada una de esas dimensiones, en el modelo estrella están todas contenidas en la misma tabla. En cambio, en el Snowflake, pues distintas dimensiones de la variable tienda están en distintas tablas. Tengo, por ejemplo, aquí el ID de tienda con su nombre y su teléfono, luego tengo su dirección, luego tengo las poblaciones, evidentemente tengo las provincias y evidentemente tengo el país. Podría poner aquí comunidad autónoma. Bueno, veis que existe una relación, ¿vale? Va de más general el país a más particular, que es el ID tienda. Y cada una de ellas, ¿vale? Todas están relacionadas, pues, en este caso, por el ID de población, aquí con el ID provincia, ¿vale? Porque las poblaciones pertenecen a una provincia, porque las provincias pertenecen a una comunidad autónoma, si tuviera la tabla, y porque las comunidades autónomas pertenecen a un país, ¿vale? Si quiero analizar las ventas a nivel de país, no hay ningún problema porque tengo la jerarquía, ¿eh? Tengo la jerarquía, ¿vale? Entonces, este es el modelo, pues, más usual, ¿vale? El modelo más usual, yo diría, y esto corregirme seguro que algunos de vosotros lo sabéis mejor que yo, posiblemente en cloud, a lo mejor el modelo más usual puede ser este porque tenemos un aplanamiento de las dimensiones, no existe una necesidad, ¿no? Desde el punto de vista de optimización de las consultas, tan grande como se tenía en los modelos de base de datos relacionales, implicaban este tipo de jerarquías. Pero que conozcáis ambas solo, al menos desde el punto de vista de concepto. Porque, evidentemente, los arquitectos, los data engineers y demás, son los que se dedican cuando queremos incorporar un determinado origen de información nuevo, son ellos los que diseñan de qué manera es más óptimo incorporar esa información en nuestra base de datos, dependiendo, evidentemente, del tipo de uso que le vamos a dar. Esto es otro ejemplo súper sencillo. Si tenemos formularios de alta de nuestro programa de fidelidad, tanto en la web como en nuestros puntos de venta físicos, necesitamos aquí una consolidación de toda esa información para generar esos procesos de duplicación y de data quality. puede una persona haberse dado de alta en el formulario web y luego en el formulario de una tienda física, por ejemplo. Bueno, pues evidentemente desduplicaremos ese cliente porque será el mismo, aplicaremos los filtros y los procesos de transformación necesarios y lo consolidaremos en el Data Warehouse. Y podemos tener distintas tablas donde contenemos distinta información de nuestros clientes. Podemos tener una tabla consolidada con toda la información del cliente, pero luego podemos tener una tabla de las profesiones, de nuestros clientes y podemos tener una tabla a nivel de edad y sexo, dependiendo del uso y dependiendo de nuestras necesidades. Lo mismo cuando tenemos más orígenes de información, tenemos distintos puntos de venta, incluso tenemos por distintas webs, tenemos la nuestra, tenemos distintos marketing de afiliación y demás, la información de ventas online nos viene de distintas webs. necesitamos consolidar toda esa información, aplicar esos procesos de duplicación y de depuración y data quality que comentamos ¿vale? Y luego, oye, pues incorporar esta información en la estructura tal y como queramos, ¿no? Puede haber una tabla de clientes, podemos tener una snapshot que se calcule, pues en este caso a nivel de mes un proceso de cálculo de nuestras ventas a nivel de mes por cliente, ¿vale? Es un cálculo que a cierre de mes me dice que este cliente ha gastado tanto en ese mes. O podemos tenerlo tanto gasto a nivel de sección o de tipo de producto en cada mes. Esos procesos que son procesos pesados, si los hacemos de manera repetitiva, pero que son muy útiles si los hacemos de manera agregada y de manera periódica, en este caso a nivel de mes, nos ahorra muchísimo tiempo de computación para luego mostrar esa información en algún reporting o lo que fuere. puedo tener esta información, este snapshot creado a nivel de cliente que se relacionará con esta tabla de clientes a través de él y de cliente pero puedo tener luego una tabla exactamente digamos del mismo tipo, un snapshot pero donde tenga las ventas agregadas por artículo, ¿vale? y donde tenga una tabla a nivel de artículo ¿vale? y donde, oye, pues evidentemente pues entre ambas tablas, pues no existe una relación a través del ID cliente, porque no existe, ¿vale? Tendré una consolidación, una relación entre ellas, pues a través del, por ejemplo, del ID artículo. Aquí os dejo, y con esto ya casi terminamos lo de los escenarios de rentabilidad, lo vemos la semana que viene, ¿vale? Aquí os dejo un ejemplo, ¿vale? También un poquito más, digamos, más trabajado, donde vemos la tabla de clientes, la tabla de pedidos que nos pueden hacer los clientes, y veis que este tiene, seguro que ya lo habéis dado. Este tiene, un cliente puede tener varios pedidos, por eso la relación es uno a varios, ¿vale? Por eso lo de aquí las aspas estas, ¿vale? De igual manera que los pedidos, un pedido puede tener varios tickets, ¿vale? Por eso la relación entre pedidos y tickets, ¿vale? Pues es de uno a varios. Y a nivel de producto, un producto puede estar en varios tickets, ¿vale? Por eso la relación aquí es de uno a varios, de productos a tickets. Yo os planteo, oye, con esta herramienta, el Grow SQL, hacer algún tipo de diagrama de un e-commerce o de un retail o de o de un de una empresa de logística que hablamos varias veces o de un contact center, intentar hacer este ejercicio para ver de qué manera podéis identificar las relaciones entre las tablas, de qué manera podemos luego a través de eso evidentemente crear marcos de visualización, el famoso cuadro de mando integral, que permita, a través de procesos de transformación, procesos de agregación, esas ETLs que comentábamos antes con Alejandro, pues permitir crear una o X tablas que contengan la información y que yo pueda mostrar esa información luego en un cuadro de mando. La parte de estrategia promocional lo dejamos para la semana que viene. Creo que tengo por ahí un par de mensajes. Parecen tablas relacionadas en Power BI. Pues pueden ser. Os dejo ese ejemplo en el ejercicio, en el documento. Y os planteo hacer, oye, pues si os parece, la semana que viene, como es hora y media, podemos dedicarle 20 minutitos a ver la parte de escenario de rentabilidad, ¿vale? Brujulear, que os he puesto yo en el PDF, brujulear en Excel, ¿vale? A ver quién me lo puede llegar a explicar, ¿vale? La semana que viene. brujulear también bueno, pues de qué manera podríamos hacer algún ejercicio desde el punto de vista de crear un entorno una arquitectura de datos como las que hemos comentado, ¿vale? un entorno de clientes donde tengo productos, donde tengo pedidos donde tengo tickets, donde tengo devoluciones donde tengo, ¿vale? en esa situación, pues hacer algún ejercicio de esos que posiblemente, pues no sé si en la clase del Master Sergi, pues lo mismo lo estáis viendo por ahí y tal, ¿vale? Entonces, bueno, pues si os queréis compartir algo de eso la semana que viene, pues yo totalmente encantado, ¿vale? Vale, comenta Alejandro, ¿en las bases de datos se puede modelar como han mostrado en la imagen? Sí, sí, claro, claro, Alejandro, totalmente. De hecho, te diría incluso, ¿vale? Que lo que se hace en un Power BI es prácticamente lo mismo que se puede hacer en un data warehouse, porque lo que tienes en un Power BI es, tú diseñas un esquema de datos. ¿Vale? Tú diseñas un esquema de datos. ¿Vale? Si tienes datos que provienen de... Tienes dos tablas de hechos. Por ejemplo, las llamadas son contact center y las ventas serían dos tablas de hechos. ¿Vale? Y luego puedes tener alrededor de esas tablas de hechos pues tener un montón de dimensiones sobre las que analizarlas. La dimensión cliente, la dimensión tiempo, la dimensión región, la dimensión provincia o geográfica. Si estamos en un contact center donde podemos recibir llamadas de distintos países, la dimensión producto. Entonces, no todas las tablas, no todas las dimensiones se relacionan con todas las tablas de hechos. ¿vale? a lo mejor la dimensión producto no tiene sentido que se relacione con la tabla de llamadas lo invento, ¿eh? ¿vale? entonces montar si queréis algún ejercicio de estos lo pintamos, me lo enseñáis y lo comentamos, ¿eh? la semana que viene si os parece y los 90 minutos que tenemos pues los podemos dedicar primero a la parte de promociones ¿vale? luego a esta parte si queréis un ratito y luego ya comentamos la resolución si os parece bien y así aprovechamos yo creo que un poquito mejor la clase de la semana que viene. ¿Os parece? ¿Sí? Venga. ¿Dudas, cuestiones? Sí, Ana, por ahí, ok. Alejandro, ok. Verónica la ha visto también hacer así. A Iván también le he visto hacer así. A Aurelio, a Delfina también. Bueno, bueno, ¿cuántas manos veo por ahí? Pocas cámaras, muchos así, así que fantástico. Me quedo con los estos. La semana que viene más cámaras. Ahí, Leticia, muy bien, muy bien. Hola, hola, hola a todos. Venga, pues nada más por mi parte. Vale, gracias Cristian, Auremir por ahí, gracias, ok. Nada, me alegro que estemos todos de acuerdo. Fantástico. Vale, nada, gracias. Gracias a vosotros por la atención y por la predisposición. En el metro no se puede, claro que no. Si no te dirían esta mujer que las pasa, está un poco... ¿Eh? Nada, nada, gracias a vosotros y a vosotras. Nos vemos la semana que viene, entregar la actividad, voy a poder intentar avanzar todo lo que pueda en la corrección ¿vale? y nos vemos la semana que viene, así aprovechamos con esas tres cositas los 90 minutos, ¿vale? y no se hace tan pesa, ¿vale? venga, ánimo, que solo nos queda una clase antes de las navidades ánimo, un abrazo fuerte, chao gracias, chao, chao
