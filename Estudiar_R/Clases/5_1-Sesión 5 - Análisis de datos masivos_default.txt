Bienvenidos a esta sesión. Así que bueno, veremos la parte teórica un poco más rápido y ya pasaremos a la parte práctica. Dentro de la parte práctica, enviamos comentado que también nos gustaría hablar de las significancias, ¿de acuerdo? Entonces, hoy veremos lo que sería un poco más la parte de Arima, que la vimos un poco más deprisa y corriendo la semana pasada, y un poquito la parte de aprendizaje supervisado, ¿vale? Que sea lo que haríamos hoy un poquito más rápido. entonces sin más dilación os voy a compartir pantalla y vamos a comenzar con la actividad 1 como siempre os digo cualquier duda que tengáis os voy a poner el chat en mi segunda pantalla por lo tanto podéis comentarlo y os intentaré ayudar en todo lo que pueda y más, vale? entonces un momentito vale, de acuerdo entonces aquí está vale y entiendo que estoy compartiendo pantalla vale Entonces, para entrar en la actividad 1, para aquellos que seáis completamente nuevos, tenéis aquí la parte de actividades y test. Y cuando clicamos aquí tenéis cuatro actividades. La primera actividad es importante, ¿vale? Estáis agrupados en una serie de grupos, ¿de acuerdo? Y dentro de cada grupo debería haber una persona que se encarga de ser el representante de equipo, ¿de acuerdo? El representante de equipo no es más que la persona que cuando termine la actividad subirá no solo la actividad, que eso lo tenéis que subir todos, seáis representantes de equipo o no representantes de equipo, sino subirá un documento que se llama la hoja de control, además de la actividad. Entonces, en esta hoja de control se valora la participación que ha tenido cada alumno, cada compañero en la elaboración de la entrega. Entonces, la actividad número uno, aún así, tenéis que entregarla todos. La única diferencia siendo representante de equipo es que además del resultado de la actividad número uno, el representante de equipo necesita entregar un formulario que sería la hoja de control. Entonces, le daríais a descargar el material con las instrucciones y esto os generaría el tema o el ejercicio que tenéis que intentar desarrollar. En este caso nos dan una serie de datos, en este caso serían los precios de los datos, en este caso de los aguacates. Nos explican un poco qué corresponde a cada una de las columnas y nos hacen una serie de preguntas. primero, a qué tipo de variables se compone mi base de datos si tenemos variables numéricas, si tenemos variables de tipo texto y para aquellas que son variables de tipo numérico pues realizar el análisis exploratorio diagramas de análisis exploratorio estamos considerando por ejemplo el summary que realizábamos con él la media, la varianza, primer cuantil, segundo cuantil, etc. después, dice bueno De la base de datos además lo que queremos es la variable del precio de venta. Bueno, pues la variable del precio de venta será esta columna que tenéis aquí. Entonces tenéis que seleccionar esa columna y guardarla en un sitio aparte. Nos pregunta también la matriz de covarianza y la matriz de correlación, que también lo hemos hecho en clase. Y después que intentéis analizar un poco matriz de covarianza y matriz de correlación. sobre todo la correlación que lo veremos también, intentaremos verlo hoy para repasarlo, que básicamente era positiva, pues las dos variables se mueven en la misma dirección. Negativa, pues cuando una variable aumenta la otra disminuye. Y aquí teníamos también lo que sería si era por encima del 0,5 o por debajo de menos 0,5, decíamos que era fuerte y si estaba entre medios decíamos que era débil. después la posible relación entre los precios y el volumen de ventas que nos está preguntando hacer una regresión como hemos hecho hasta ahora una regresión lineal entre el precio y el volumen es decir lo que quiero predecir sería el precio sería mi Y y luego el volumen son los datos con los que yo querría predecir además nos preguntan que lo hagamos con logaritmos os puse también el ejercicio en el cual tenéis los logaritmos que era aplicar la regresión pero tomando logaritmos y por último nos preguntan una predicción del precio de venta de los aguacates orgánicos vendidos en Albany a 3 meses esto lo que nos está diciendo es que al final tenemos el volumen que se ha vendido por cada una de las fechas por lo tanto tendríamos una serie temporal y la idea es que hicieseis una predicción muy probablemente con Arima ¿Cómo se va a corregir? Lock Lock, como el que hemos visto en clase Jonathan En principio Lock Lock Vale, entonces ¿Cómo se va a corregir? Aquí tenéis las distintas evaluaciones, ¿de acuerdo? Siempre os aconsejo que lo miréis en base a lo que os estamos comentando aquí, ¿vale? Por ejemplo, identifique y hace una buena descripción estadística para que sea excelente Tenemos que todos los tipos serían correctos y aparte analizáis un poco los estadísticos descriptivos para que sea un punto que sea adecuado pues os olvidáis un tipo de variable, ¿vale? que sea insuficiente pues no tenéis no hacéis un análisis por ejemplo ah pues yo que sé, que la media sea 5 implica que el valor medio es 5 pero fijarte que la media no coincide con la mediana y entonces puede ser que haya outliers, bueno hay distintas que analicéis un poquito los datos ¿vale? bueno y aquí tenéis pues el número 2 el número 3, el número 4 en función de esto si os dais cuenta es uno por cada uno de las variables y así os podemos evaluar o podréis ver un poquito mejor que existe yo sé que os aconsejo que lo argumentéis más que nada porque es lo que se os va a pedir de cara al examen si os equivocáis al hacer o no sale el algoritmo pero me argumentáis como lo haríais ¿de acuerdo? para mí no es igual de correcto evidentemente, pero no tendréis un cero en esa puntuación, a lo mejor os doy un punto, ¿vale? Porque considero que sí que sabéis hacerlo, pero por X motivos no os ha salido el programa, ¿vale? Esto también de cara al examen, ¿vale? Yo, por ejemplo, hay más profesores y habría que hablarlo con ellos, ¿vale? Pero, por ejemplo, los exámenes que corrijo yo sola no me importa tanto que el resultado del código sea el correcto, sino que el proceso por el cual lo habéis utilizado sea el correcto, ¿vale? Pero bueno, en vuestro caso, como hay más profesores correctores, pues cuando tengamos las reuniones consiguientes, pues ya os lo comentaré. Dicho esto, ¿hasta aquí alguna duda de la tarea de la entrega? Yo tengo una pregunta. Por supuesto. El diagrama de cajas no es más que el summary, ¿verdad? vale, el diagrama de caja sería un boxplot es cierto que todavía no hemos visto boxplot, entonces realmente con el summary en principio lo podéis hacer de hecho es que en la rúbrica no os pregunta nada del boxplot, el boxplot se utiliza más que nada por si alguien lo sabe calcular porque ya habría visto R, ¿de acuerdo? y si no, es para que veáis si hay outliers, si hay valores anómalos o no hay valores anómalos, ¿vale? en este caso también la podríais ver cuando la media es muy diferente a la mediana significa que hay valores que son raros, que son anómalos. No necesitaríais hacer, por ejemplo, un boxplot. Pero igualmente si miras la rúbrica, realmente no te lo piden. Te dice, oye, muéstrame los estadísticos y extrae conclusiones al respecto. O sea, no te están pidiendo un diagrama de cajas. Que si lo sabes hacer, genial. Pero no es lo que... O sea, siempre es un extra, pero no es lo que se os va a pedir para tener los dos puntos. ¿Sí? ¿Queda claro, Freddy? Sí, profe, muchas gracias De nada Vale, y ahora lo que se estaba comentando es ¿Hasta cuándo tenéis la actividad? Vale, hasta el 1 de diciembre 2025 a las 11.59 hora española Por favor, si alguien no vive en España tiene otro uso horario y le sale la hora española por defecto, que hable con su tutor del máster para configurarla a la hora de su país, ¿vale? para que no tengáis que estar haciendo los cálculos de 5 horas para adelante y 5 horas para atrás básicamente ¿sí? Pero si tengo otra pregunta, disculpe Claro, dime ¿La media recortada recorta los valores de forma ordenada, ordenándolos de mayor a mayor o es como viene presentada la tabla? No, en principio sí que los ordena, es como hacer una mediana, en la mediana también lo estás ordenando ¿vale? De hecho la media recortada te sirve para hacer los outliers, por lo tanto si tú tienes un outlier en la posición 50, tú estás quitando el 10% de los datos de 100 por ejemplo si fuese por las 10 primeras filas no te serviría porque no te quitaría el dato anómalo, entonces tanto la mediana como la media recortada aunque nosotros no lo vemos por detrás sí que hace ese orden y selecciona de menor a mayor los últimos y los primeros Sí, pero fue por el outlier mismo, hay un valor de 64 millones que estaba bien fuera de mano. Vale, pues en principio te debería servir con la media recortada, sino en realidad en el día a día, lo que más se suele utilizar es cuando veas que hay un valor que es un outlier, generalmente se suele atender a utilizar la mediana. lo que pasa es que aquí también os explicamos la media recortada si lo queréis, pero sí que se utiliza más la mediana porque entendemos que la mediana es mucho más fiable que la media en el momento en el que tienes valores anómalos ¿sí? podrías utilizar la mediana perfectamente si no quieres hacer la media recortada como quieras gracias profe vale, entonces de la actividad, ¿alguna duda más? queda claro que hay que hacer con los códigos que están colgados en la plataforma en principio deberíais ser capaces de hacer todo, ¿vale? ¿alguna duda, alguna cuestión en este punto antes de que empecemos con el ejercicio 4 que nos quedó de Arima o todo más menos bien? entiendo que bien, si no hay preguntas es que bien, vale, pues entonces os doy la teoría que vais a ver que es mucho más rápida, ¿vale? y nos ponemos de lleno la parte práctica ¿de acuerdo? entonces ¿qué vamos a ver hoy? Bueno, hoy lo que vamos a ver es lo que sería cómo modelamos los datos. Realmente, cuando hacemos una regresión lineal ya estamos haciendo un modelo de datos, entonces no os vamos a descubrir nada nuevo, pero sí que os vamos a dar un poco la teoría. Cuando hablamos de modelo de datos es la parte en la que realmente construimos la fórmula matemática. Cuando hemos hecho una regresión lineal, que os comentaba, pues estos son los coeficientes que luego multiplican a las variables que se van sumando y esto hace una recta. Bueno, pues esto sería el modelo matemático. Y se utiliza tanto en estadística, econometría, investigación operativa, finanzas, en todos los sitios y también en el aprendizaje automático, que es el machine learning, que es lo que vamos a estudiar a partir de ahora. Bueno, que ya hemos empezado a estudiar, ¿vale? Entonces, ¿qué se trata? Se trata de utilizar datos para dar una serie de hipótesis, ¿de acuerdo? Para poder decir si una hipótesis es cierta o una hipótesis es falsa. Esto sería el contraste de hipótesis y luego hay otra parte que es la que generalmente tiene ahora mismo mucho más repercusión que sería la parte de analizar datos para predecir algo. Podemos predecir un valor numérico, aquí estaríamos hablando de predicción en base a un histórico, yo tengo un histórico de las ventas voy a predecir cuál es la venta que me va a pasar mañana, esto sería predicción. clasificación, ¿cómo sería? pues imaginamos que nosotros tenemos un conjunto de personas y sabemos las características de unas personas que han sido clientes y las características de unas personas que no han sido clientes ¿qué puedo utilizar con ello? querer saber si una persona va a volver a ser cliente o no va a ser cliente predecir si será cliente o no cliente aquí estaríamos hablando de una clasificación, es decir, yo tengo dos categorías cliente y no cliente y quiero predecir en base a ejemplos anteriores si una nueva persona será cliente o no será cliente o sea, pertenecerá a una categoría o a la otra esto sería clasificación puede ser dos categorías o muchas más categorías y luego tendríamos el agrupamiento aquí por ejemplo está todo el tema de la segmentación de clientes tenemos un conjunto de clientes muy diferentes no tengo ninguna relación, ninguna categoría simplemente un conjunto de clientes Y yo quiero que el algoritmo, el ordenador, me vaya encontrando relaciones entre ellos, ¿de acuerdo? Pero que yo no sepa, ¿vale? Es decir, me los va a hacer grupos, pero esos grupos no tienen por qué ser ninguna categoría o yo tengo por qué conocerlos, ¿vale? entonces serían los tres tipos y veremos los tres tipos a lo largo de esta asignatura la predicción ya la hemos medio visto porque vendría siendo la regresión con un par de etapas más entonces tendríamos la construcción de un modelo pues que es al final siempre va a haber las mismas etapas la primera es seleccionar las variables es decir que variables voy a utilizar para predecir o para clasificar o para agrupar después ejecutar el modelo y por último ver si el modelo ha sido aceptable o no ha sido aceptable, ¿vale? Que estaríamos en la valoración del modelo. Entonces, este primer tema vamos a definir lo que es un modelo de datos y sus características, ver un poquito lo que serían las etapas y distinguir entre el aprendizaje supervisado y el aprendizaje no supervisado, ¿vale? Entonces, bueno, un poco lo que os he comentado, ¿vale? ¿Qué tipos de modelos solemos tener? Pues predicción, clasificación, agrupación, que también técnicamente se llama clasterización, y reducción de la dimensión. Predicción, tengo lo que comentábamos, tenemos un histórico, por ejemplo, de transacciones de ventas y queremos predecir cuánto será el precio a futuro. O, por ejemplo, tenemos una producción, el elemento de aguacates, y nosotros sabemos datos como las temperaturas, precipitaciones y presiones. y yo quiero decir, una vez que yo entre en un modelo con esos datos, voy a querer predecir para otra temperatura, otra presión y otra situación atmosférica, voy a querer predecir cuánto me va a producir el aguacate. Por ejemplo, estamos prediciendo números, ¿de acuerdo? Así que tendríamos predicción. Clasificación. Lo que comentamos, conocemos una serie de categorías, sabemos ejemplos de esas categorías y vamos a predecir otra cosa completamente nueva en una de esas dos categorías. Por ejemplo, tenemos un conjunto de fotos, tenemos fotos que son gatos y fotos que son perros. Le doy al algoritmo, esta foto es un gato, esta foto es un perro, esta foto es un gato, esta foto es un perro. Y ahora tengo una tercera foto, que yo no sé qué es, y le digo al algoritmo, oye, en base a lo que te he dado antes, ¿esto qué sería? ¿Un perro o un gato? Aquí estaríamos hablando de clasificación. tanto predicción como clasificación serían de aprendizaje supervisado ¿por qué? porque tenemos ejemplos, porque tenemos un historial detrás un histórico o ejemplos y dentro del aprendizaje no supervisado, es decir, no tenemos la solución de antemano no tenemos ejemplos, tendríamos la agrupación o clasterización y la reducción de dimensión ¿la clasterización para qué nos sirve? pues para lo que comentaba, tenemos un conjunto de datos no sabemos cómo agruparlos y voy a decirle al ordenador que él entienda todo lo que está pasando por detrás y me los agrupe ¿qué sería la reducción de dimensión? pues tengo muchísimas variables y yo no, sale muy lento mis métodos son muy lentos entonces son algoritmos que me permiten de a lo mejor 200 variables quedarme con 10 entonces sería este ejemplo un poco lo que hemos comentado al final se utiliza en diversas áreas muchísimas más de las que vienen aquí ¿de acuerdo? por ejemplo, predicción, determinar valores futuros también podemos tener aquí no solo regresiones lineales sino series temporales un ejemplo de aquí pues tenemos el histórico de lo que pasa entre 1950 y 1962 aplicamos ARIMA y tratamos de encontrar lo que pasará en el futuro ¿vale? en este caso hemos predicho pues desde probablemente el 61 hasta el 62 cosa que antes no tenía entonces esto sería una predicción ¿qué estamos hablando cuando hablamos de una clasificación? pues un poco lo que vemos, tenemos dos categorías en este caso los circulitos y los mases y yo le añado un tercer objeto y lo que quiero es que el algoritmo diga está en los círculos o en los mases, aquí es círculos imágenes, perros, gatos lo que os he comentado, distintas categorías pues esto sería una clasificación porque yo ya conozco que hay dos grupos, uno con círculos y otro con mases. Luego tendríamos la agrupación y clasterización. Si os dais cuenta, todos son círculos, pero yo puedo encontrar que hay un grupito que está más cerca, que estaría entre el eje X1 y 2, y luego hay como otro grupo que estaría entre el 3 y el 7. De hecho, podría incluso haber más grupos. Aquí veis, podemos incluso dividir aún más. pero en un inicio yo no lo conozco, en un inicio tengo todos mis datos iguales. Pues esta sería la clasterización, en este caso en tres grupos, el grupo negro, el grupo rojo y el grupo verde, que yo no conozco de antemano. Y muy importante, que no tengo por qué saber de antemano qué características tienen. Voy a tener que analizarlos posteriormente para saber. Pues el grupo 1 es la gente con más poder adquisitivo, me lo invento, y que son empresarios. Y el grupo 2, por ejemplo, son gente de salario más o menos medio y que son funcionarios, por ejemplo, para que os hagáis una idea. Pero esto lo voy a tener que saber una vez tengan los grupos, analizar cada variable o las características que tienen cada uno de ellos. Y por último tendríamos lo de reducir la dimensión. Aquí veis un ejemplo de, en este caso son flores, Son flores de diversos tipos. De hecho, esto es un dataset que se llama Iris Dataset, que es uno que suele utilizar mucho de prueba. Y vemos que está en tres dimensiones. ¿Tengo problemas para hacerlo en tres dimensiones? No me gusta y lo que quiero es pasarlo a una dimensión menos, a dos dimensiones. Como con los algoritmos de reducción. En este caso, un ejemplo que es el principal componente análisis, que es el PCA. que también lo veremos entonces, importante, aunque esto ya más o menos lo hemos dicho cuando hacíamos la regresión lineal que es, tenemos variables independientes que son los datos que me ayudan a predecir por ejemplo, si estamos hablando de predecir la producción en base a las variables meteorológicas mis variables independientes son las variables meteorológicas las que me ayudan a predecir, ¿vale? o a clasificar o a clasterizar ¿Qué sería la variable dependiente? La variable que yo estoy estudiando, por ejemplo, en ese mismo ejemplo de la producción de aguacates en función de sus características meteorológicas, la variable dependiente o a veces también se llama variable objetivo o target, sería la producción de aguacates. Entonces, en función de qué variables tengamos, tanto independientes como dependientes, pues escogeremos los datos que utilizaremos, el modelo que aplicaremos y entrenar o no el modelo. Ya esto lo iremos profundizando según vayamos viendo las diversas etapas. Consulta. Sí, por supuesto, dime. Referente a variables dependientes. en modelos clásicos tenemos múltiples variables independientes y una sola variable dependiente esta parte me gustaría que aclare ¿por qué podrían existir muchos dependientes? vale, en principio pueden existir varias dependientes pero los modelos los puedes aplicar a una única variable ¿por qué pueden existir varias variables dependientes? porque tú a lo mejor tienes, me lo invento en el caso de los aguacates que quieres predecir la producción y que quieres predecir el precio, por ejemplo, por el cual lo vas a vender. Estás intentando estudiar dos variables, por lo tanto puedes tener más variables dependientes. Sí que es cierto que al nivel que vamos a darlo solo vamos a hacer algoritmos, y la mayoría de los algoritmos son así, que solo puedes predecir una variable dependiente. Entonces, ¿qué harías? Repetirías el algoritmo para cada una de las variables dependientes que tengas. aquí luego entran aspectos de multicolinearidad por ejemplo yo no puedo predecir una variable por ejemplo no puedo utilizar la producción para predecir el precio y luego querer utilizar el precio para calcular el volumen ¿por qué? porque si yo ya he pasado los datos del volumen para calcular el precio cuando yo vaya a hacer el caso contrario al final estoy entre comillas dando la solución esto son cosas que se llaman multicolinearidad que básicamente es que dependen muy fuertemente una variable de otra pues claro, si predecimos una variable en base a otras, al final acaba. Pero sí Irina, sí que puedes tener en una tabla varias variables dependientes, porque sí que es cierto que tú puedes querer analizar distintas variables o predecir distintas variables, pero los algoritmos que vamos a utilizar nosotros solamente tenemos una variable objetivo, que es una variable dependiente. ¿Sí? Sí. Perfecto. vale, entonces, etapas bueno, seleccionamos el modelo al final a vuestro nivel o al nivel que vamos a dar aquí en esta asignatura se basa mucho en predecimos número utilizamos un algoritmo de predicción ya puede ser con series temporales o puede ser regresión lineal u otros tipos de regresiones, ¿vale? clasificamos en categorías que conocemos o hacemos una clasificación binaria utilizaremos sobre todo dos que sería la regresión logística y luego hay otro que será el random forest bueno y los árboles de decisión veremos los que nos dé tiempo y si no tenemos digamos que los datos o la solución o los datos etiquetados que sería más técnicamente como se llama y estamos ante un aprendizaje no supervisado pues entonces tendríamos que estaríamos hablando de una clasterización ¿vale? ¿entrenamiento del modelo? pues únicamente o sobre todo se hace se hace esta discomposición en entrenamiento y testeo cuando estamos hablando de predicciones o de clasificaciones, que son cuando descompondremos el dataset este conjunto de datos lo utilizo para crear el modelo y me reservo otro conjuntito pequeño para probar como de bueno es ese modelo ¿de acuerdo? cuando hablamos de clasificación o reducción de dimensión como realmente nosotros no tenemos la solución pues lo que se hace es aplicar el modelo desde el inicio a todo el conjunto de datos, es decir, no vamos a hacer una descomposición en entrenamiento y luego con otros datos testeo, sino que se hace todo el conjunto de datos desde un inicio entonces, realmente también es muy importante si tenemos un modelo que es más sencillo que es más fácil de interpretar y nos da un resultado igual siempre intentad aquel modelo que sea fácil de interpretar y que podáis interpretar bien, ¿de acuerdo? por mucho que el modelo sea muy difícil y muy complejo, si hay un modelo más sencillo que nos da la misma solución pues mejor utilizar eso ¿vale? Se suele escoger al final la precisión como de bien lo puedes interpretar y la flexibilidad, es decir la complejidad que tiene el modelo en función de estas tres variables lo que hacemos es al final una función de tradeoff que básicamente es decir que nos compensa más o en cual obtenemos mayor beneficio ¿vale? De acuerdo entonces ¿cómo seleccionamos variables? creo que esto más o menos lo conocéis todo que es básicamente qué variables voy a utilizar o que me aportan información, es decir, qué variables creo que están relacionadas aquí existen técnicas, ¿vale? tenéis por ejemplo la correlación, podéis mirarlo por la correlación, o hay otra técnica que no veremos ahora pero hay regularizaciones y métodos de laso que me permiten escoger variables de manera automática, ¿vale? No creo que lleguemos a ese nivel, pero bueno, existir existe para que sepáis, ¿vale? El problema es que también muchísimas veces, ¿de acuerdo? Al final tenemos muchísimos datos, ¿de acuerdo? ¿Y qué pasa? Cuando tenemos muchos datos pues que a veces los algoritmos se vuelven lentos. Ya no lentos, sino que a veces también tenemos problemas de que hay mucha dependencia de unas variables con otras y el algoritmo no nos da un resultado correcto, ¿vale? En ese caso lo que se puede hacer es algoritmos de reducción de la dimensión, es decir, los algoritmos que hemos comentado antes que estarían dentro del aprendizaje no supervisado, los podemos utilizar también en esta parte previa para reducir el número de variables que tenemos en cuenta. Una vez que hemos seleccionado las variables pasaríamos a entrenar el modelo que básicamente es, yo tengo la fórmula que quiero hacer el algoritmo que quiero hacer y le voy a decir, pues muy bien, sobre este conjunto de datos, aprende como si fuese un niño pequeño, aprende de este conjunto de datos, que si estamos hablando del aprendizaje supervisado sería con estas variables independientes, esta es la solución esta es la variable dependiente lo que queremos es que aprenda de esas relaciones, ¿vale? y cree el modelo ese modelo ya cuando le dé otras variables independientes, otros valores de las mismas variables independientes será capaz de darme un resultado en base a lo que ha visto antes. Y por último veremos cómo de bueno es nuestro modelo. Es decir, no solo entrenar el modelo sobre datos, sobre la parte del training, del entrenamiento, sino también darle esas nuevas muestras, esos nuevos datos de testeo que luego pueda comparar. Cuando estamos hablando del aprendizaje no supervisado, hay otras maneras, porque hemos dicho que no descomponemos en un conjunto de entrenamiento y un conjunto de testeo. entonces podéis estar preguntándoos bueno, cuando tengo el no supervisado que ya lo veremos realmente cómo puedo saber si es bueno o es malo porque hay unas métricas hay unas funciones características que me permitirán saberlo ¿de acuerdo? ¿qué dos puntos nos podemos encontrar? primero el balance entre el sesgo y la varianza y después el sobreentrenamiento ¿entonces qué es el balance entre el sesgo y la varianza? bueno, si tenemos bajo sesgo y tenemos baja varianza vemos que en la diana ha acertado, sería lo ideal ¿qué ocurre si tenemos bajo sesgo y alta varianza? pues que es cierto que tenemos una mayor dispersión pero aún así está como más o menos centrado ahora, cuando nuestros datos están sesgados o tienen un alto sesgo es decir, por ejemplo, queremos descomponer entre fumadores y no fumadores Sí, ahí voy. Tenemos entre fumadores y no fumadores. Imaginemos que en nuestro conjunto de datos tenemos un 80% que son fumadores y un 20% que son no fumadores. ¿Qué significa esto? Que mi modelo va a dar una mayor probabilidad a fumadores casi con toda seguridad. Entonces cuando nos digan fumador no vamos a saber si nos está diciendo fumador porque las variables realmente me están diciendo independientes están relacionadas con el caso fumador o si me está haciendo fumador porque el 80% de los datos es fumador y como casi todo es fumador pues tendemos a pensar que la predicción sea fumador. Entonces esto sería por ejemplo un sesgo que se introduce cuando tenemos una clasificación y tenemos una descompensación de las clases que queremos predecir o que queremos clasificar. ¿Sí, Jonathan? ¿Entiendes la parte de sesgo? es básicamente que el modelo no nos está dando un valor porque realmente haya una lógica detrás, sino porque existe otra razón, ¿vale? Por ejemplo, que tiene muchísima más representación en la muestra que otro. Para solucionar esto, aquellos que seáis un poco más avanzados en R o que ya habéis empezado con R, se suele utilizar técnicas como sería el caso del SMOT, que lo que hace es crear una muestra ficticia para igualar el conjunto de datos. ¿se ha entendido mejor el sesgo? también por otra parte existe otra definición de sesgo ¿qué limitantes existen en el modelo? vale, no entiendo la palabra de qué limitantes existen, o sea, ¿a qué te refieres con limitantes? si tenemos algún tipo de problema o de forma que no podemos realizar el modelo de datos o sea, el modelado de datos vale, de acuerdo limitantes no hay muchos más que tengas un dato fiable lo que pasa que a pesar de que casi siempre vais a poder hacer un modelo de datos hay veces que no os va a salir un buen modelo de datos entonces yo creo que no es tanto limitante que limitante sea por ejemplo que haya variables de tipo texto la mayoría no funcionan con variables de tipo texto pero hay una solución que nos permitirá pasarla a numérico por lo tanto no hay tanto limitante en eso yo creo que no es tanto limitante sino que el dato sea fiable y que el resultado sea bueno porque por ejemplo cuando tenemos el sobre entrenamiento lo que significa es que sobre el conjunto de entrenamiento es muy muy muy bueno o sea es perfecto, muy bueno pero cuando le doy datos nuevos por ejemplo no generaliza bien, no me da una buena solución esto sería por ejemplo el sobre entrenamiento que también se llama overfitting y es un problema bastante serio, ahí donde lo veis ¿vale? es este también el caso contrario, que el modelo sea tan tan tan tan tan tan simple que sea como que no tenga nada, que esto sería el underfitting entonces yo creo que el limitante no es tanto eso como lo otro, sí que es cierto que cuando tenéis muchísimas variables con muchísimos datos puede haber un limitante en temas de tiempo o cuando utilicemos el random forest sí que es cierto que matemáticamente por detrás el random forest se define de una determinada forma y al final puede ser que no te converja la solución. Pero estos son casos muy concretos. En general vas a poder hacerlo siempre. A pesar que hay casos concretillos que te funcionará mejor un método que otro. Y luego tendríamos el caso de alta varianza y alto sesgo que es lo peor de todo. Y os estaba contando que también, y esto es importante, existe otro tipo de sesgo que también a veces hablamos de otro tipo de sesgo cuando hablamos de este tipo de algoritmos y es el sesgo, digamos el sesgo ético. Se suele conocer con el nombre de fairness, os lo voy a poner por aquí, si puedo escribir, se suele conocer con el nombre en inglés, ¿vale? Y todo este tema ha llegado a lo que se llama el explainable AI, ¿de acuerdo? Que sería la IA explicable, ¿no? ¿Qué significa este término de fairness? Que lo escucharéis muchísimo. Es básicamente que el modelo sea justo. ¿Qué ocurre? Que históricamente puede pasar que por determinadas razones, por ejemplo, me lo invento, pero por ejemplo razones de raza, de sexo, de religión, que son variables que son muy complicadas. No deberían en principio ser variables por las cuales, por ejemplo, digan si nos dan una hipoteca o no nos dan una hipoteca. ¿Qué ocurre? Que a lo mejor históricamente puede haber ocurrido que la gente de una determinada raza haya recibido menos hipotecas porque tenía menos recursos o porque la situación era la que era. entonces también este término puede haber sesgo en estas variables y esto sí que es muy peligroso porque aquí en la Unión Europea de hecho están empezando a definir que es un sistema prohibido, que no es prohibido y lo están mirando muchísimo entonces cuando tenemos variables un tanto conflictivas sí que es cierto que se utilizan métricas funciones como puede ser el smooth para crear datos ficticios y que si la variable por ejemplo raza hubiese tenido algún tipo de importancia en el modelo, al crear yo datos ficticios con el resto de razas se iguale y no sea el caso. Esto se hace bastante. Sería otra definición del sesgo. El sesgo en lo que serían las variables un poco más privadas o personales. Y todo esto está relacionado con el fairness y con la IA explicable. Que ahora está muy de moda. entonces bueno esto sería un poco qué problemas nos podemos encontrar y cómo los podemos solucionar es importante que no existe una solución perfecta normalmente iremos probando un poco un poco un poco hasta que encontremos la mejor solución al final es si estábamos haciendo la división en subconjuntos es decir si nosotros teníamos dos conjuntos y hacíamos la subdivisión en dos subconjuntos porque estábamos en el caso del aprendizaje supervisado, pues entonces lo que normalmente se intenta hacer es reducir, o sea, si por ejemplo hemos cogido un 80% para entrenamiento y un 20% para testeo y tenemos un sobreajusto, un overfitting, lo que intentaremos hacer es reducir un poco el entrenamiento y abrir un poco el testeo o ir jugando con este tipo de porcentajes, para que me dé un mejor valor. también yo os recomendaría también por otra parte revisar si los datos son correctos revisar si los datos no tenemos nulos, ¿vale? revisar si hay alguna variable que a lo mejor me esté molestando y que no tenga que estar ahí por ejemplo, ¿de acuerdo? serían mis consejos en este punto cuando estamos hablando hay también otra forma que se utiliza por ejemplo mucho en los no supervisados que es básicamente bueno, más bien que más en los supervisados casi, en los no supervisados se utiliza otra cosa pero bueno, que sería la valorización cruzada con k iteraciones ¿qué quiere decir esto? que en lugar de dividirlo en dos conjuntos lo dividimos en cinco conjuntos ¿vale? o seis o siete, digo cinco porque habitualmente se hacen cinco entonces, la primera vez que ejecutamos el modelo vamos a seleccionar los cuatro primeros conjuntos para crear el modelo y el último conjunto para testearlo La segunda vez que voy a hacer, pues voy a seleccionar los tres primeros y el último para crear el modelo y el cuarto para testearlo. Así hasta que haya hecho el entrenamiento con los cuatro conjuntos, uno de cada uno y luego tendríamos el testeo. bueno esto lo veremos más cuando estemos haciendo creo que lo entenderéis mejor y al final es básicamente entender o si ni siquiera con esto funciona pues seleccionamos una observación una fila e intentamos mirar como el resto fuese el conjunto de entrenamiento vale un poco lo que os he comentado es que iré un poquito ya más rápido porque esto os lo he comentado anteriormente Básicamente, ¿qué es el aprendizaje automático? Pues que la máquina aprenda de los datos y me dé una solución. Entonces queremos crear modelos con nuestros datos que resuelvan determinadas tareas. Lo que estamos escribiendo en R es lo que se llamaría un algoritmo. Existen dos, el supervisado y el no supervisado. En supervisado comentábamos datos etiquetados, es decir, tenemos la solución, es decir, tenemos ejemplos o tenemos un histórico. En el caso de aprendizaje no supervisado no tenemos esa solución, yo tengo una amalgama de datos pero no tengo ningún tipo de solución ni categoría establecida, nada, es decir, quiero que el ordenador haga lo que sea con esos datos pero yo no le estoy ayudando en nada, no tenemos ejemplos ni nada. dentro del aprendizaje supervisado lo que os comentaba también clasificación o predicción algoritmos de regresión redes neuronales regresión logística algoritmos de logic árboles de decisión máquinas de soporte vectorial intentaremos ver hasta donde podamos y aprendizaje no supervisado pues básicamente es el tema de los componentes principales si estamos hablando de reducción de dimensión o los de clustering aquí por ejemplo distinguimos el Cummins o los algoritmos jerárquicos que ya los veremos en su determinado momento ¿alguna duda de la parte teórica? o todo más menos bien ¿TSN no entra como no supervisado? vale en principio aquí estamos viendo simplemente los principales no vamos a avanzar a TSNE ¿de acuerdo? pero juraría que sí, si no recuerdo mal de todas formas se utilizan muchísimo más últimamente otro tipo de algoritmos que el TSN TSN empezó con muchísima fuerza pero a día de hoy creo que se utilizan más otras formas ¿vale? pero sí, sí lo que pasa es que no vamos a a analizarlo ese concretamente ¿sí? sí, sí, por eso te digo que en principio sí pero sí que es cierto que por ejemplo el clustering avanza más yo creo que en la dirección del jerárquico K-means, K-medians son los que se suelen estudiar que serán los que veremos también es cierto que se utiliza mucho algoritmos de reducción, por ejemplo el PCA que es el de reducción de dimensión es muy bueno, pero también últimamente se utiliza mucho uno que se llama UMAP entonces hay muchas veces en el día a día que directamente se aplica al algoritmo de reducción y cuando utilizas el algoritmo de reducción de UMAP casi que normalmente se suele utilizar más un HDBSCAN o un DBSCAN o algoritmos de clustering muchísimo más potentes. Pero bueno, no entraremos en el TSNE aunque también es uno de los grandes. Pues empezamos con la parte de teoría, bueno, la parte práctica, perdona, esta sería toda la parte de teoría y dadme 5 minutos que os ponga el fichero R lo tenéis ya por cierto colgado, el de hoy no pero el de la semana pasada lo tenéis entonces voy a poneros el R estudio y repasaremos un poco la anterior clase, este es el fichero que os comento que siempre os lo intento enviar a completamente comentado para que nadie tenga ningún tipo de problema, lo subí hoy si alguien lo echa en falta vale el impacto de un programa de educación financiera para afiliados próximo a cumplir la edad de pensión con tres grupos poblacionales vale, el impacto como lo mides lo mides con un número o lo mides como en plan de impacto alto, medio, bajo con número vale, y tienes un histórico es decir, tienes ya ejemplos de que por ejemplo esta persona ha tenido un impacto alto en esta otra persona ha tenido un impacto medio individual no, pero por los grupos sí grupo 1 impacto medio, grupo 2 impacto alto esto sí me imagino aquí en este caso lo que harías como tienes este histórico lo que intentarías hacer sería una regresión lineal para intentar predecir eso. Pero también hay otra opción, que es, tú has hecho ya los grupos, ¿vale? Porque no lo tienes individual. Pero si lo tuvieses individual, podrías hacer una segmentación y decir, vale, hazme una relación con todos estos y encuéntrame personas o que se comporten semejante. Y a lo mejor puedes encontrar casuísticas raras dentro de los grupos que has comentado, de que se comportan de manera distinta o que tienen un impacto diferente. Y luego habría otra opción que es si tú lo que dijese es por ejemplo el alto medio bajo que comentábamos o quisieras por ejemplo una probabilidad de afectación aquí si fuesen dos una clasificación binaria de por ejemplo impacto sí y no aquí podrías utilizar algoritmos de clasificación como una regresión logística te daría la probabilidad de impacto de que haya impactado o no haya impactado. pero aquí tienen que ser cuando tienes impacto venido como sí o no y luego habría la tercera y última que si fuese impacto alto, medio y bajo y tú quisieras predecir esto aquí estaríamos hablando de una clasificación pero más un random forest no te serviría la regresión logística porque es sólo para cuando son sí, noes o dos variables entonces si lo que quieres es predecir un número aquí sería una regresión lineal en base al histórico quien dice regresión lineal dice cualquiera de regresiones y si tuvieses fechas estaríamos hablando probablemente de una arima porque estaríamos hablando de una serie temporal que depende como enfoques el problema te da una solución o otra y te puede ser interesante de una forma o de otra por ejemplo si vosotros queréis ver el comportamiento de los clientes ¿qué haréis? pues lo que haréis será una clasterización y esto nos dará grupos de clientes que se comportan semejantes y a lo mejor podéis ver, ah pues fíjate este grupo de clientes que es el grupo de clientes uno, me compran más y son menores de 18 años y entonces yo creo que esto puede deberse entonces podéis sacar de ahí conclusiones si lo que queréis es por ejemplo predecir lo que comentábamos yo que sé, el dinero que el dinero que hay que pagar a cada persona afectada por ejemplo por la dana aquí en españa si queremos predecir esto pues que haríamos haríamos regresiones algoritmos de predicción vale que sería por ejemplo predecir un número para que nos hagamos una idea y si lo que queremos es predecir categorías vale pues aquí tendríamos eso vale la esterilización con qué algoritmo se haría el que vamos a ver en clase que es el más sencillo es el CAMI, uy perdón que estoy en mayúsculas, el CAMINS, pero tenéis también el CAMIDIANS existe también el TSN que se ha comentado el compañero existe también por ejemplo los algoritmos que sería HDBSCAN DBSCAN, existen varios ¿vale? entonces la forma de aplicarlos es muy parecida de hecho también tienes, no, mal no este es de clasificación entonces tienes todos estos y aparte, ¿vale? la forma de aplicarlo de todos es bastante parecida ¿Vale? Sabiendo aplicar uno sabrás aplicar el resto. ¿De acuerdo? Vale, entonces, nada, para guardar el fichero, comprueba primero que lo estás haciendo desde aquí R script, ¿vale? Y si lo copias aquí y lo ejecutas aquí mediante este RAM, ¿vale? Luego podrás guardarlo simplemente haciendo clic aquí a guardar, ¿vale? bueno entonces muy rápidamente al final no me ha dado tanto tiempo a la práctica pero bueno intentaremos alguna clase hacerla entera práctica esto es lo que habíamos visto lo que pasa que con todos los textos que siempre os doy para asegurarme de que se entiende vale entonces y muchísimos más ejemplos así que pues podéis hacerlo aquí por ejemplo también os puse un poco de como aunque creo que lo vimos en clase pero para dibujar también vale como se dibuja etcétera, ¿vale? habíamos terminado en esta parte de aquí ¿vale? que era aplicar el modelo Arima ¿vale? que era con el auto Arima, en este caso auto Arima de la serie temporal que habíamos creado la serie temporal sí que la habíamos llegado a crear y aplicar el auto Arima creo recordar que también, ¿vale? representarlo o no, entonces ¿cómo lo representamos o cómo hacemos la predicción? de nuestro modelo Arima para hacer la predicción lo que hacemos es poner la palabra Forecast, ¿vale? Forecast y le ponemos el número de meses, días o lo que queráis que queréis predecir, ¿vale? Si hemos creado la serie temporal Arima en frecuencia mensual ¿qué serán estos 18? Pues serán 18 meses. Vale, no hicimos auto Arima, vale. Entonces auto Arima, entonces nos quedamos aquí, ¿no? nos quedamos aquí y luego hablamos del tema de las significancias que hablaremos hoy también ¿qué sería autoarima? pues esta serie temporal, vuestra TS que habíamos creado lo que vamos a hacer es aplicarla en el autoarima para ello necesitamos esta librería que es la librería forecast si no tenéis la librería forecast, es decir, si nosotros ejecutamos esta línea y vemos que nos sale que no existe la librería forecast, ¿qué tenemos que hacer? La línea que tenemos aquí arriba, ¿vale? De install.packages.forecast. Esto lo que hará será instalarla, ¿vale? Y luego lo que tendremos que hacerlo es cargarla, como con esta library forecast, ¿sí? Una vez que tenemos esto, hacemos el modelo autoarima. No tenemos que justificar nada más, es decir, ni añadir nada más, porque autoarima ya nos va a dar el modelo automáticamente. Nos ajusta los datos y ya nos dice qué modelo es el mejor en nuestro caso. Entonces, ¿cómo hacemos este modelo? Pues autoarima de la serie temporal que queramos. En nuestro caso estábamos hablando de la serie temporal precio Brent, que también la habíamos puesto por aquí. Aquí, que era nuestra serie temporal precio Brent. Entonces, ¿cómo hacíamos? Pues autoarima de precio Brent. cuando nosotros ejecutamos esto y lo guardamos bajo el punto de vista de modelo ARIMA lo siguiente es decir, oye, dime cuánto van a ser los siguientes 12 meses pues hacemos los 12 meses y esto si yo miro lo que hay aquí, aquí dentro hay datos ¿qué datos hay? los que hay la predicción para cada uno de los siguientes meses en este caso en 2020 tenemos los datos que nos han ido dando ¿Lo puedo dibujar? Lo puedo dibujar como lo que he guardado aquí lo pongo dentro de plot. Entonces de esta forma tendríamos el dibujo. ¿Dónde sé que empieza la rima? Pues la parte en la que empiezan estos intervalos de confianza. Realmente aquí la línea azul es el valor que nos ha dado y luego las líneas más oscuras, digamos que sería con el primer intervalo de confianza. es decir, tengo esa línea azul pero me puedo mover con bastante probabilidad en el gris oscuro y también podría llegar al gris claro pero con menos probabilidad. Podemos incluso hacer la media de las predicciones igual que hemos hecho antes, de hecho podemos hacer el summary que os interesará para la entrega, ¿Por qué? Porque el summary, recordad, que me da el valor de acuerdo de lo que sería el resultado. En este caso ha utilizado un modelo ARIMA de parámetros 0, 1 y 2. Esto viene siendo la P, la Q y la D. Que podríamos entrar en ello, pero no es el objeto de este curso en principio. Es básicamente cada uno de estos puntitos que se representaría una característica concreta. Si estás utilizando diferenciaciones, si no estás utilizando diferenciaciones, pero esto ya es un nivel bastante más elevado, por eso recomendamos hacer el autoarima porque lo ajusta automáticamente. Después tendríamos los coeficientes dentro de lo que sería la expresión de la serie temporal creada con arima, tendríamos la variabilidad, tendríamos la probabilidad de lo clickhood y criterios como el A y C. esta a priori en el caso de la métrica de Arima que es más complicado de analizar que el resto en todo caso fijaros en lo que sería el error absoluto medio que sería este de aquí por ejemplo o el cuadrático, el RMS serían los dos que más veríamos para ver si es correcto o no es correcto igualmente a este nivel de, o sea en esta asignatura no se va a analizar tanto los errores, se analiza muchísimo más en la asignatura que tenéis el siguiente cuatrimestre que ahí sí que se ve los errores concretos de cómo evaluar este error, sé que sale bien, sé que no sale bien, aquí es más bien que aprendáis a lo que sería el autoarigma saber aplicarlo entonces evidentemente no me ha generado un buen resultado también lo podemos ver con la gráfica al final si vemos la gráfica lo podemos hacer zoom realmente fijaros el comportamiento que tiene la serie temporal y lo que me ha salido a mí es raro, la parte azul vemos que es bastante diferente al comportamiento habitual de la serie entonces con eso podemos ver que a lo mejor no es del todo fiable No solo se puede hacer de manera matemática con los números, sino también se puede hacer de manera gráfica. Vale, entonces, aquí tenéis... Sí, claro, dime. Si es que le pone al view modelo Arima precio Brent, ¿le sale una tabla con los datos? Sí, mira. Si yo voy aquí, predicción ventas casa Brent. aquí tendrías los datos con los intervalos de confianza este sería el dato como tal que te ha dado la predicción luego con un 80% de confianza el valor mínimo de ese intervalo de 80% de confianza sería el de aquí y el alto sería el de aquí y con un 95% de confianza este de aquí y este de aquí pero el valor realmente que te ha predicho es este de aquí ¿vale? para cada uno de los de los puntos ¿era esto? sí prof vale y de hecho estos son los cositas que os veí los que comentaba, las áreas que os comentaba que eran más gris oscuro, más gris claro y la línea azul ¿vale? la línea azul correspondería al point forecast y luego las otras son el de con un 80% y con un 95% de grado de confianza. El grado de confianza está muy relacionado con la significancia que hablamos el otro día. Entonces, la clase que lo veamos pues veremos tanto la significancia como el grado de confianza. De acuerdo, aquí tenéis, en la que os he subido, tenéis otro ejemplo, que sería en este caso con otra serie temporal que se crea e intentando mirar otra vez, pues en este caso una predicción a 18 meses, pero lo tenéis ahí también. El día de hoy lo que quería que viésemos eran más temas de modelos. Voy a aprovechar que nos quedan cuatro minutitos para hacerlo, intentar llegar a la parte en la que podemos analizar la significancia con el primer modelo que vamos a estudiar. Entonces, ¿qué vamos a hacer? En principio el primer modelo que tenemos es el modelo de regresión lineal que habíamos visto aquí. Recordad que para la entrega es importante que representéis estos modelos de regresión lineal. Y la semana pasada hubo un comentario respecto a la significancia. Entonces, como modelos no me da tiempo a explicaroslo en cinco minutos y si no recuerdo mal tampoco era el objetivo de la clase de hoy si miramos la programación, os voy a explicar por qué este primer modelo, la primera regresión lineal es ya un modelo en sí. ¿Por qué no está del todo hecho como un modelo? Porque no hemos descompuesto en conjunto de entrenamiento y conjunto de testeo. Es lo que nos quedaría hacer. Aquí hemos aplicado una regresión lineal considerando todo el conjunto de los datos. Para que se considere como un modelo de predicción tendríamos que haber descompuesto este dataset en un conjunto de entrenamiento y un conjunto de testeo. que se hace bastante sencillo con un train test split, pero vamos a analizar el resultado. Entonces, hagáis el train test split, es decir, hagáis el testeo y el entrenamiento o no lo hagáis, el modelo tiene un resultado, que es lo que la semana pasada, corregidme si me equivoco, comentamos del tema de las significancias y que queríais conocer un poco más el tema de las significancias, ¿cierto? o me estoy confundiendo juraría que sí, pero como doy a varios grupos puede que sea de otro grupo entonces la regresión lineal es el primer caso esta regresión lineal sería el primer caso de modelo de acuerdo de Machine Learning que vamos a ver en este caso es cierto que todavía nos falta hacer la parte de descomposición en entrenamiento y testeo pero lo hagáis o no vuestro modelo siempre va a tener un resultado ¿Vale? Entonces, ¿cómo hacíamos el tema del summary del modelo? ¿Cómo lo podemos hacer? Pues de la siguiente forma. Nosotros tenemos esta regresión, ¿vale? Y esta regresión nos daba un intercept y un coeficiente, ¿vale? entonces, en este caso ¿vale? no porque la fórmula es bastante sencilla, pero en algunas ocasiones nos salían las valores de significancia que eran los de que comentábamos de que salían valores muy muy pequeños ¿sí? ¿os acordáis o no? por favor comentadme si os acordáis o no sí gracias ¿qué venía haciendo cuando lo hacíamos aquí? ¿vale? cuando guardábamos el resultado de nuestra regresión es decir, aquí no hemos guardado ningún resultado, hemos aplicado únicamente la regresión, pero después decíamos, a lo mejor a mí me interesa saber qué valores ha predicho, entonces me interesa saber o me interesa guardar, sí, ahí vamos, me interesa guardar el valor, ¿por qué? porque luego hacemos el summary de lo que yo he guardado, ¿vale? Entonces, vamos a hacer este caso, vamos a hacer el summary. Importante es, si no lo guardáis, luego el summary no lo vais a poder hacer, ¿vale? Entonces, importante guardarlo y después hacer el summary para ver el resultado, ¿vale? Entonces, esperad un momento que lo vuelva a ejecutar hasta aquí, ¿vale? Y vemos eso hoy, que lo veremos hoy y nos servirá para todos los modelos que hagamos, ¿vale? Pero como queréis un poquito más de profundidad y he terminado un poquito antes de explicaros eso, pues me sirve para comentároslo. Vale, dame cinco segundos. Que es que no he hecho la copia uno. Un momentito, eh. Vale. Un momento, eh. vale, no me está leyendo el archivo y me lo ha leído antes vale, no se puede encontrar la función porque no tengo la función esta vale ahora sí, vale entonces le hago la copia a uno no sé por qué no ha funcionado al principio pero debería haber funcionado vale, y ahora vamos a la parte que nos interesaba que era cuando definíamos aquí estos de aquí. Perdonad que hoy tengo el otro teclado. Vamos así. Vale, ahora sí que parece que está yendo bien. Vamos por aquí. la estimación y el vale, aquí nos habíamos quedado ¿de acuerdo? y me preguntabais ¿qué eran los valores de significancia? lo que tenemos que fijarnos ¿vale? es en esta columna ¿de acuerdo? y ver si es mayor o menor que los valores que nos indiquen, por defecto se suele utilizar si es mayor o menor que 0,05 ¿por qué? porque por defecto se suele utilizar un intervalo de confianza que se llama del 95%. Este valor lo que nos va a decir es si nuestra columna realmente afecta o no afecta. Dicho de otra manera, si podemos despreciar la columna, es decir, yo tengo el impacto que tiene esta columna, por ejemplo, si nos vamos a la columna en este caso del log Haringey Average Price, tenemos que tiene un efecto de menos 0,017. ahora yo puedo querer saber si ese efecto es verdadero o no es verdadero es decir si yo lo puedo despreciar o no lo puedo despreciar esto como se hace con una cosa que se llama el contraste de hipótesis vosotros en este curso ¿qué es el contraste de hipótesis? en este caso la hipótesis sería ¿esta variable es relevante o no es relevante? Y vosotros con el valor que tenéis aquí, en este caso este 0,9283, podemos decir si la variable es relevante o no es relevante. Entonces, si lo comparamos con el 0,05, que era el valor que os he comentado que es por defecto, en este caso, ¿qué es? ¿Mayor que 0,05 o menor que 0,05? Comentadme. ¿es mayor o menor? efectivamente, es mayor cuando es mayor, de acuerdo, lo que tenemos es que no es importante, que es despreciable es decir, digamos que al compararlo si te da que es mayor que el valor con el que estás comparando en este caso 0,05 que sería este valor de aquí nos diría que es mayor y por tanto esta variable en realidad podría no tenerla en cuenta para mi cálculo, ¿vale? Porque en principio es despreciable, no es significativa, ¿de acuerdo? Cuando hablamos, por ejemplo, de esta otra variable, ¿vale? Que sería el logaritmo de las casas vendidas, fijaros que es 2 por L a menos 16. Esto es lo mismo que 2 por 10 elevado a menos 16. Es decir, que es 0,0000000015 veces y luego un 2. Consideráis que ese valor, fijaros que tiene 3 estrellitas. Por lo tanto, ¿con cuál lo voy a comparar? Con el de 3 estrellitas. ¿Ese valor es menor que 0 o no? Si es 0,000000000000002, ¿es menor que 0 o mayor que 0? ¿Qué opináis? vale, veo que estáis respondiendo casi todos, perfecto, es mayor, por lo tanto tendríamos el mismo caso, ¿vale? Sí que es cierto que, por ejemplo, si nosotros lo considerásemos como el 0,05, aquí nos daría que es menor que 0,05, ¿vale? Es decir, cuidado con cómo lo definimos en el sentido de que si utilizamos el 0 estamos utilizando un 100% de confianza, ¿vale? Es decir, tú tienes un 100% de confianza de que eso es así. Si utilizamos el 0,001 lo estamos diciendo con un 99,999% de confianza. Si lo decimos con un 0,01 con un 99% de confianza. Si lo decimos con un 0,05, con un 95% de confianza. Decirlo con un 100, un 99, un 90% en Machine Learning casi no se utiliza. Entonces, yo lo que os aconsejo siempre es compararlo con el 0,05 que es lo que se hace en el sector. Entonces, en estos casos, si lo comparamos con el 0,05, ¿me sabríais decir qué variables serían las que serían significativas? Os pregunto. ¿O cuántas variables significativas habría si lo hacemos con el 0,05? Que es el del sector. Las tres últimas, efectivamente. Y el resto serían las despreciables. entonces esto muy importante en principio cuando os adentráis en este tema del data science en principio debería ser básico os voy a decir la verdad la dura realidad de esto y es que al menos en la mayoría de las empresas se prima más el resultado que veamos que da parecido a ir analizando cada una de las significancias no debería ser el caso pero la mayoría de las veces las significancias lo miramos los estadísticos o los matemáticos y sí que es cierto que la mayoría de personas que hacen Data Science en una empresa la mayoría en muchísimas empresas y muy grandes empresas aquí de España de estos temas pasan un poco más ¿qué métricas se van mirando? se van mirando más lo que vendría siendo el R cuadrado o el adjusted r cuadrado. Pero aún así, de cara del punto de vista del modelo, sí que es cierto que son importantes. Y sí que es cierto que os otorgan un conocimiento bastante significativo. Por ejemplo, yo sé que el 2 por elevado a la menos 16, que es prácticamente cero, yo sé que esta variable al compararla con el 0,05 me va a dar que con un 95% de confianza esta variable es muy significativa ¿de acuerdo? porque es prácticamente 0 es mucho más pequeña sin embargo el 0,92 yo lo que voy a saber es que prácticamente no es significativa a ver, esperad en este caso el modelo final incluiría solo las tres últimas variables y no las cinco eso es lo que estaba comentando ¿debería ser así? sí, en el día a día ocurre eso en las empresas al menos la mayoría de las grandes empresas de aquí de España, no ¿debería ser así? sí, aquí en España lo que sí que es cierto es que no se llega a tanto nivel, la mayoría de las veces consiste en ¿funciona mi modelo? sí, ¿me veo que predice bien que el error más o menos está bien? sí, pues no lo toco pero si lo queréis hacer súper súper bien y genial, que es lo que deberías querer hacer todos y lo que deberían querer hacer todos en realidad debería ser debería ser lo que os he comentado ¿vale? ¿que ha entendido esto más o menos? en el examen si tenéis examen de esto o en las entregas si tenéis entregas de esto no se os va a pedir ¿vale? analizar los p-valor ¿de acuerdo? porque no es el objeto de esta asignatura esto os lo he comentado porque salió el tema ¿vale? y la curiosidad y entonces por eso os lo he comentado pero en el examen como os comento no se va a medir el tema de los valores ¿Vale? Más cosas que nos pueden Interesar por aquí, habíamos comentado También ¿Vale? Que nos podía interesar el tema Del R cuadrado David, coméntame, perdón Me perdí porque No entiendo por qué las últimas tres Si se tendrían en cuenta, si con sus estrellitas Ellas igual No son significantes ¿Vale? Sí, porque realmente, aunque os pongan estrellitas Casi siempre, y eso sí que os lo digo en el 99% de los casos realmente tendremos que siempre utilizáis el 0,05 ¿vale? es muy raro el caso en el que utilicéis un código de significancia entre 0 y 0,01, os lo tienen que decir específicamente y por defecto todo el mundo considera el 0,05 es decir, aunque aquí veáis las estrellitas que os recomiendan que lo comparéis con uno y con otro ¿vale? sí que es cierto que en vuestra vida diaria, que yo creo que Al final os tengo que preparar más por vuestra vida diaria que para lo que sería la teoría pura, porque al final estamos en principio preparándoos para eso. En la vida diaria se utiliza siempre, por no decirte que el 99% de las ocasiones, el 0,05. Sí que hay veces que se reduce o se aumenta. Yo creo que más se aumenta que se reduce, ¿de acuerdo? Porque al final sí que es cierto que tú lo que vas a querer es tenerlo todo con muchísima probabilidad y con muchísima seguridad. es decir, para ti lo ideal sería el 100% de seguridad en lo que dices pero estamos hablando de algoritmos automáticos que nunca van a ser el 100% se basan en las probabilidades y nunca son el 100% entonces sí, exacto yo sí que os aconsejo esto si queréis ir completamente puristas tendríais que hacerlo con lo que os marca pero realmente en vuestro día a día se utiliza siempre el 0,05 o sea, los otros tienen que decir expresamente eso si lo utilizáis pero si lo otros dicen especialmente quiero con un 90% de confianza pero un 95% es justo es bastante alto y también es un poquito suelto porque todo lo que hagáis con inteligencia artificial, con machine learning nunca es exacto, siempre es probabilístico por lo tanto siempre vas a tener una probabilidad de error con lo cual un 100% de seguridad nunca vas a tener vale, eso ya sí que sí que os lo comento, que el 100% de seguridad nunca, nunca, nunca, pero nunca pero jamás lo vais a tener ¿de acuerdo? ¿ha quedado más o menos claro? uy, perdonad, que se ha activado esto, resumiendo básicamente perdonad, que cuando voy a ir al chat por algún motivo vale, pues hago esto, a ver si así ya, vale el tema, el p-valor, ¿de acuerdo? si es menor que el 0,05 bueno, si es menor que el 0,05 ¿de acuerdo? hacemos una cosa, ¿vale? y si es mayor o igual que 0,05 hacemos la otra si es menor que 0,05 tendríamos que podemos considerarla y si es mayor la descartamos ¿sí? hasta aquí se ha entendido la clase de hoy más contentos con el tema de la significancia o sea, básicamente sería significativo o no significativo, si queréis las palabras exactas, ¿vale? significativo cuando es menor que 0,05 y no significativo cuando es mayor o igual que 0,05, ¿vale? pero sí que es cierto que la mayoría de las ocasiones lamentablemente no se utiliza ¿vale? yo tengo que ser completamente honesta y es un problema, pero veréis que hay muchas cosas que os diré en la teoría es esto y en la realidad es esta otra ¿vale? hay muchísimas cosas en este tema del machine learning que ocurre así ¿de acuerdo? y que no debería ser el caso pero pues bueno, es como ocurre ¿vale? y al final las empresas siguen funcionando y todo pero esto, si alguien ha estudiado matemáticas y luego sabe lo que es la física o la ingeniería pasa exactamente igual o sea que bueno, ocurre esto ¿vale? simplemente eso, entonces por hoy esto sería a todo, ¿de acuerdo? La entrega, en principio, está todo hecho. Sí, sí, sí, sí, de hecho la voy a subir ahora mismo. La entrega, en principio, lo tenéis todo hecho, ¿vale? En la entrega no se os va a pedir que valoréis temas de errores, ni p-valores, ni nada de esto, ¿vale? Que daroslo como un conocimiento que tenéis con esto, pero no hace falta que lo analicéis, ¿vale? Con esto quiero decir que si a alguien no le ha quedado muy claro, no hay problema porque no se va a examinar de eso en la entrega, ¿vale? si alguien tiene cualquier problema con la entrega por favor comentadmelo en el foro y si tenéis problemas al ejecutar el código, por favor, captura de pantalla, me lo subís como imagen y me decís, me pasa esto y yo os voy ayudando, que la idea es que aprendéis a programar y no todo el mundo tiene por qué saber programar ya, así que todo error es completamente normal entonces, por hoy esto sería todo, el próximo día bueno, hoy hemos visto Arima, que era la última parte que nos quedaba para ver bien la entrega vale y en principio si vais mirando más o menos los códigos deberíais ser capaces de hacer la entrega vale no hay que cambiar muchísimas más cosas sí entonces por hoy esto es todo cualquier cosa me comentáis vale y muchísimas gracias adiós
