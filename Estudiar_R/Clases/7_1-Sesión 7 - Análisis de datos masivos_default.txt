Hola, buenos dias, buenas tardes. Primero de todo, como estais? Muchisimas gracias por estar aqui en esta sesion un dia mas. Hoy la clase va a ser un poco distinta porque vamos a hacer una clase completamente practica. Al final os voy a explicar la regresion logistica y haremos tambien arboles de decision, en este caso random forest. Y la idea es un poco que sea completamente practica para practicar todo lo que hemos visto hasta ahora. Sí que quería preguntaros la entrega, qué tal os habéis sentido, cómo habéis visto la actividad, más o menos lo que habíamos visto, comentadme un poco. vale, entiendo que bien, si no hay ningún tipo de comentario así que bueno voy a compartiros aplicamos lo que nos enseñó y vale, perfecto, pues con eso en principio ya debería estar para la asignatura, al final el día de hoy también vamos a hacer una clase práctica, con lo cual a lo largo de esta semana os irán poniendo o de la siguiente, os irán poniendo las calificaciones. Voy a compartiros pantalla y vamos a repasar lo que vimos el otro día del aprendizaje no supervisado porque la clase va a ser completamente y totalmente práctica, así que si queréis hacerlo a la vez que lo voy haciendo yo, bienvenido sea, podéis hacerlo. Y si no, siempre os quedará esta clase para que lo podáis vosotros poco a poco mirar. Entonces, la semana pasada vimos lo que sería el aprendizaje supervisado y dentro del aprendizaje supervisado comenzamos con los algoritmos de clasificación. ¿Qué pretendían los algoritmos de clasificación? Que nosotros tenemos unas categorías definidas y queremos que el algoritmo aprenda a dirigir cada dato a la categoría correspondiente, de forma que cuando nosotros no sepamos a qué categoría pertenece el dato que estamos mirando, podamos con el algoritmo deducir si es a la categoría sí, a la categoría no, a cualquier otra categoría. Pero son categorías que conocemos y de las que tenemos un histórico o ejemplos, por lo tanto, sabemos exactamente el modelo espacial. Técnicamente hoy tendría que dar esto, pero de todas formas, ahora volvemos también al tema del modelo espacial sin ningún problema. Después de clase, si me da tiempo a enseñaros el algoritmo, el Random Forest y la regresión logística, vemos un poco de nuevo el modelo espacial sin problema. Vale, entonces, ¿qué hacíamos? Lo primero, hacíamos el install packages para instalar el paquete porque en este caso partíamos de una librería que era la librería Titanic. Luego también hoy os quiero enseñar una plataforma que se llama Kaggle que os va a ser muy útil para cualquier tipo de dato que queréis o de problema. Si queréis encontrar datos es bastante útil. Entonces instalábamos la librería Titanic, en este caso porque es una librería que nos da ya este dataset, que es un dataset que se utiliza habitualmente de prueba para poder hacer el resto. Después de esto, ¿qué habíamos hecho? Habíamos dicho que primero hay que analizar los datos. Siempre que analicemos los datos, lo primero es leer los datos y después decir si hay nulos, si no hay nulos, el tema de las estadísticas, el tema de qué tipo de variables existen. Entonces aquí haríamos data y pondríamos aquí el nombre de este caso que sería Titanic. Primero tenemos que instalarlo y cargar la librería como leer el libro de las funcionalidades que va a haber además de las que estamos viendo. Vamos a hacer aquí la librería y ahora tenemos que leer los datos. Para leer los datos vamos a tener que utilizar el nombre del dataset, que esto nos lo tendríamos que saber, que en este caso es Titanic Train. y con esto serviría. Si hay un examen, nos va a caer de este estilo que nosotros ya tenemos datasets prefiltrados. En el examen os daremos nosotros el archivo y haríais el archivo como lo hemos estado haciendo antes de que empezásemos con este de Titanic, sino cargando el archivo desde un Excel, etc. Luego ya hablaremos del examen cuando sea un poquito más cerca. Entonces, cargamos el fichero y una vez que hemos cargado los ficheros, tenemos que asignar estos ficheros a algo. Entonces vamos a generar, por ejemplo, la variable llamada datos, en la que guardamos la información de Titanic. Si nosotros queremos leer los datos, ¿qué hacíamos? Hacíamos el view de datos. Esto viene bastante bien, sobre todo cuando es la primera vez que los cargáis, porque tenemos una visión como tal de tabla estilo Excel de la que estamos acostumbrados y podemos llegar a ver los tipos de variables que existen. Entonces, esta sería la primera fase, la fase de carga de datos y verlos para mirar un poco qué tipo de dato tenemos. Ahora hay dos cosas que también son muy importantes, que estaríamos hablando del STR de datos. Esto ya lo habíamos visto, nos decía para cada tabla, digamos que la columna que tenemos y si era número sin decimal, si teníamos aquí números estilo texto, si eran números que pueden tener también NA si os dais cuenta, entonces un poco el tipo de dato que podríamos tener en cada una de nuestras columnas. Y otra cosa bastante interesante, perdonad que creo que estaba escribiendo ahí, como siempre tengo el chat abierto en esta pantalla, entonces si veis que cualquier cosa no os responderé rápido si me escribís por el chat porque estoy casi constantemente mirándolo. entonces ahora tenemos el summary de datos ¿qué es lo que hacíamos con este summary? hacíamos las métricas estadísticas ojo de las variables que eran variables numéricas hoy era la clase también de repaso así que estamos aprovechando para repasar un poquito todo variables numéricas porque por ejemplo el caso de name lo que me dice es tienes 891 caracteres pero realmente no nos dice nada nos dice simplemente que son de tipo texto Sin embargo, por ejemplo, si vamos a la variable de edad, pues tendríamos que toma un valor mínimo de 0.42, primer cuantil de 20.12, mediana de 28, media de 21.70, tercer cuantil de 38, máximo de 80 y tenemos incluso el número de no aplicas o el número de nulos que teníamos que en este caso eran 177. ¿podemos hacer más cosas? podemos hacer más cosas, por ejemplo podemos decir que yo quiero únicamente ver cuántos nulos hay en cada columna me salió raro instalar el paquete el argumento pkgs está ausente, vale, lo has instalado de esta manera, ¿no? con las comillas vale, si las has puesto arriba ¿y el library, si utilizas el library, no te funciona? no, no no funciona Vale, te voy a dar un sitio para descargarte porque a veces hay una plataforma, porque si la estudias desde el sitio oficial de RStudio, yo me he dado cuenta que da bastantes más problemas. entonces hay una plataforma que se llama Posit que es esta de aquí y aquí tienes el RStudio que están bastante bien y no he encontrado ningún tipo, tienes funcionalidades, te diría que incluso más que si lo descargas y el lenguaje R sí que es exactamente oficial entonces te voy a pasar la página tengo que desinstalar y instalar de nuevo Sí, porque el estudio me imagino que lo descubriste desde la visión oficial del estudio, ¿verdad? Lo tienes ahora Vale, entonces, con esa visión también me daban muchos problemas los paquetes porque me daba como que tenía un paquete que me faltaba, ¿vale? Entonces, la solución que he encontrado la más rápida es utilizar el que se me ha puesto aquí el Google Lens ahora a ver, la solución que encontré la más fácil es descargártelo de aquí porque es exactamente igual de oficial o sea es el R exactamente igual pero sí que es cierto que el R Studio desktop funciona mucho mejor y tiene todas las librerías, entonces no te va a pasar el tema de que el argumento PKGS esté ausente, a mí me pasaba esto con otro paquete, con el ggplot2 creo que era el de hacer las gráficas si lo descargaba tal cual super oficial de la página web me daba problemas pero aquí lo que tienes que hacer es que descargas el free, el normal el pro, no sabía ni que existía o sea, el free normal es exactamente igual y con el error que tengas ya te funcionará de hecho creo que si vamos aquí a este download a otra página que es esta y tienes el install R y el install R studio entonces vas descargándote install R y luego te descargas el install R studio y bueno, lo ejecutas. Es rápido, no tarda mucho en instalarte R y R Studio. Y para aquellos que lo estéis utilizando con la versión completamente oficial, si también os está dando problemas la instalación de paquetes porque depende de la versión que tengáis en cuanto a la que se descarga de R Studio, pues también, ¿vale? Yo os recomiendo utilizar este porque este, la verdad, va súper bien y está muy actualizado. Si veis aquí, el último release es de octubre del 25 entonces está más actualizado ¿qué? que yo instale ese y va bien o sea, el paquete de TetaniWaz va bien vale, es que depende que tengáis que versión tengáis, porque al final pensad que las versiones de R igual que pasa con las versiones de Python tenemos por ejemplo Python 3.1, 3.2 3.3, 3.4, 3.5 ahora estamos con la 3.12 claro, a lo mejor el paquete para la 3.12 funciona, pero el paquete para la versión 3.10 necesitamos algo que no tenemos y yo creo Irina que esto es lo que puede estar pasándote, que tengas una versión que justo en esa versión no encuentre bien el paquete entonces yo te recomiendo que si te pasa eso por aquí te funcionará mejor primero instalo R y después instalo R Studio desde esta página Exacto. Es de esta página que te la voy a pasar, que al final te he dicho, ah, sí, sí, te la he pasado. A mí esta me funciona muy bien y a día de hoy es la que más utilizo porque hasta cuando sale una versión nueva me avisa y me dice que tengo que, o sea, que la descargue. Entonces, tiene sus recordatorios. y a la práctica es lo mismo el RStudio es exactamente el mismo que tenéis que es completamente igual ¿de acuerdo? no vas a tener ningún cambio, ni se programa distinto, no es literalmente el mismo lo que pasa que aquí te pone la última versión siempre que lo abras a mí con esto me dejó de dar todos los problemas de instalación de paquetes que tuve en otras ocasiones espero que te funcione si después cuando lo hagas ves que te sigue dando el error que no debería pasar pero ya te pediré entonces que me hagas captura de pantalla de la versión que tengas de R y todo para poder hacerlo de cara al examen no os preocupéis porque lo que os comento los datos los vamos a dar en un fichero, entonces será pues que hagáis lo de siempre del import dataset y lo subáis por aquí No vais a tener que, en principio, hacerlo desde una librería, ¿vale? Pero sí que te puede dar problema, Irina, para el tema de ggplot2, que por ejemplo a mí es el que me dio problema, ¿vale? Vale, entonces, estábamos diciendo que nos interesaba ahora el tema de llegar a entender cuántos valores nulos había por cada una de las columnas, ¿vale? Es lo que a priori estábamos haciendo. Entonces, para esto existía una función, bueno, lo habíamos visto con el is.na, pero os voy a dar también otra función. Tenemos la siguiente función que podemos hacer que es callSums, que lo tenemos aquí, callSums, que es básicamente hazme la suma de la columna y ahora tengo que indicar qué tipo de datos. En este caso yo quiero hacer, cuéntame cuántos valores ISNA tengo en mis datos. Ahora os explico un poco por qué ponemos suma y por qué no ponemos un contador. Cuando nosotros hacemos ISNA datos, lo que estamos haciendo es un conjunto, además hoy como puedo explicaroslo todo como mucho más calmado y mucho mejor. ¿Qué nos da el ISNA datos? Nos da un conjunto de verdaderos y falsos. ¿Qué ocurre? Que el ordenador solo entiende unos y ceros. Entonces, aunque nosotros estamos viendo aquí verdadero y falso, para el ordenador verdadero es el valor 1 y falso es el valor 0. ¿Qué hacemos con este colsums? Lo que hacemos es decir, recordemos, los falsos son ceros y los verdaderos son unos. Por lo tanto, cada vez que veamos falso en la columna me va a sumar un 0 y cada vez que veamos verdadero me va a sumar un 1. Entonces, ¿cómo es la forma más sencilla de decir cuántos nulos hay? Pues mirar con el ISNA a decir si son verdaderos o falsos y luego hacer esta suma de ceros y unos. Como solo cuentan los que son verdaderos, el resultado que será los que son nulos, para que tengáis un poco más de conocimiento de por qué se utiliza esta función. Esto nos hace que al final tengamos todos ceros y en este caso la única que teníamos valores nulos era la edad. Se puede hacer de otras formas. De hecho, realmente si os dais cuenta con el summary ya las variables que son categóricas nos lo daba correcto. Pero ¿qué pasa si tenemos un dato que es NA en las variables que son de tipo texto? Bueno, pues si no tenemos mucha confianza en el summary siempre podemos hacerlo de esta forma. Siguiente, tenemos columnas con datos nulos, no es lo que más nos interesa, entonces ¿qué vamos a hacer? Vamos a intentar eliminar estos datos nulos. Pregunta, nosotros tenemos 891 filas, de estas 891 filas teníamos que 177 eran valores que son nulos en la columna edad. ¿vale? Primera pregunta que os tenéis que hacer cuando os enfrentéis a este tema de hacerlos nulos es ¿realmente creéis que la columna edad es importante cuando lo que queremos hacer es predecir si alguien ha sobrevivido o no ha sobrevivido? ¿qué pensáis? ¿creéis que una persona de 80 años por lógica, creéis que una persona de 80 años tiene más probabilidades de sobrevivir que una persona de 10 años o no? Os pongo un ejemplo de extremos ¿qué pensáis? pues los más jóvenes tienen más probabilidad de sobrevivir quizás vale, efectivamente ¿de acuerdo? creía que al revés, uno de 10 años tiene más probabilidad, efectivamente, ¿vale? entonces sí que a priori debería ser o lo que nos dice la lógica es, oye parece que sí que puede tener tener relevancia Sí que en clase vimos que al final nos daba que realmente no lo tenía, pero esto lo sabemos después de aplicar el modelo. Hay algunas personas que comentan que si nosotros hacemos la matriz de correlación podríamos tener una información. En el Titanic le daban prioridad a mujeres y niños, ¿no? Sí, efectivamente. Hay gente que lo que dice es que si nosotros tenemos la matriz de correlación, dice, bueno, pues cojo las que son, selecciono las variables que son con una correlación más alta y esas serán las que afecten. No siempre ocurre. Cuidado con la correlación. La correlación lo que me dice es que si una variable aumenta, la otra también aumenta. o si es fuerte positiva, o que si una variable disminuye, la otra aumenta, si es fuerte negativa, o que las dos van por cualquier cosa, o sea, si una aumenta, la otra no hace nada, no hay una relación clara, ¿vale? ¿qué ocurre con la correlación? que no implica una causalidad es decir, pongamos un ejemplo o sea, nos puede dar una idea pero siempre tenemos que comprobarlo al final ¿por qué digo que nos puede dar una idea? porque al final lo que tendemos a pensar es que por ejemplo si aumenta el precio disminuye la demanda pues parece que hay una relación lógica que el motivo por el que disminuye la demanda es porque aumenta el precio entonces aquí tendríamos un caso de que decimos pues sí, tiene lógica que estén relacionadas pero yo puedo decir aumenta la temperatura y yo qué sé y resulta que ha habido una epidemia y entonces está todo el mundo enfermo porque aumenta la temperatura está todo el mundo enfermo no, no es una relación simplemente que ha habido la casualidad de que está todo el mundo enfermo y ha aumentado la temperatura en la calle pero la correlación será alta porque en ese periodo que estamos analizando en cuanto ha aumentado la temperatura han empezado a aumentar los casos. ¿Me explico? Es decir, que la correlación es interesante calcularla, siempre es interesante calcularla, pero no implica causalidad exacta. En la mayoría de los casos sí que hay causalidad. Si aumenta una cosa, aumenta la otra porque hay una relación entre ambas. Pero no siempre. Sobre todo el tema de la causalidad no siempre la cumple. Esto es algo que, por ejemplo, en las carreras de matemáticas se hace mucho hincapié porque muchas veces se explica de manera diferente y al final llegas a ver casos que no tienen por qué ser iguales. El 90% de los casos sí que os digo que sí, por eso la mayoría de las veces que dicen no seleccionamos las variables por correlación muchas veces tienen razón, pero en otros casos no va a ser el caso. Entonces, en este caso tenemos dudas, no sabemos si quitarla o dejarla. para buscar un dato en específico si podéis hacer, bueno podéis hacer filtros si queréis y de esa forma podéis tener la búsqueda de datos por ejemplo, vale es decir, si vosotros queréis por ejemplo, sabéis el dato que queréis hacer y sabéis la columna en la que está si, podemos hacer un filtro con la librería por ejemplo de Prir que os comentaba y por ejemplo digamos que queremos encontrar a Eugene entonces el otro día vimos más o menos una librería que se llamaba de pli y con esta librería que se llamaba de pli podríamos encontrar aquellos casos en los que los que estaban todos podemos hacer por ejemplo datos yo que sé datos eugén vale por ejemplo y de aquí que teníamos que hacer recordemos que poníamos el primero aquí vamos a guardar algo empezábamos con nuestros datos, luego porcentaje, mayor porcentaje y aplicábamos la librería filter. El porcentaje, mayor porcentaje es interesante porque para utilizar este porcentaje, mayor porcentaje tenéis que utilizar la librería de PIL. Entonces siempre que veáis un porcentaje, mayor porcentaje vendría siendo este de PIL y básicamente es a datos quiero que le apliques un filtro. En este caso el filtro que sería que la columna name sea igual a mi dato que he seleccionado, que sería RISEMASTER EUGENE. Tanto en RStudio como en Python se ponen siempre dos iguales. entonces por ejemplo vosotros hacéis esto veis que ya ha terminado y luego podéis hacer esto y veréis que en este caso la persona es Eugene el pasajero número 17 en este caso tenemos que subió a cero luego en este caso el tipo de pasajero era el 3 el nombre, el género, la edad, etc. que era un niño en este caso Prefiero una pregunta Claro En ese caso que estás buscando ¿Se tiene que colocar el nombre exacto? ¿O se puede colocar, por ejemplo, si sabemos que solamente es Eugene Poner solo la palabra Eugene ¿O tiene que ser el dato exacto? Si vosotros ponéis el nombre igual, igual Tiene que ser el dato exacto ¿Vale? Sí que hay otras opciones de poner Contains O ponemos, por ejemplo, búsquedas como sería Regex ¿Vale? Pero en este caso, si lo ponemos igual, igual, tiene que ser exactamente el valor, ¿vale? Exacto. Es decir, si por ejemplo yo hubiese cogido este dato, ¿vale? Y yo hubiese terminado y hubiese puesto aquí un espacio, veréis que no me lo reconoce, ¿vale? Porque aquí lo tenemos, no debería reconocérmelo efectivamente. Efectivamente. ¿Por qué? Porque tenemos un espacio. Es importante que tanto RStudio como Python, me está gustando muchísimo la clase porque se la puedo dar completamente práctica que a mí al final es lo que más me gusta, pero bueno, tanto RStudio como Python distinguen mayúsculas y minúsculas y distinguen también el tema de los espacios. Entonces, por ejemplo, si nosotros pusiésemos eugen con la e minúscula, tampoco tendrías la información. ¿Por qué? Porque no hay un eugen con la e minúscula, tienes un eugen con el valor u. Luego existen otras cosas, por ejemplo, me se me viene todo el rato en Python, pero es porque es lo que más utilizo al final. En Python, por ejemplo, tienes la función lower, que lo que hace es ponértelo todo a minúsculas. Entonces vas a saber que aunque yo busque la palabra en minúsculas, como todo mi texto lo he convertido a minúsculas previamente, no voy a tener problema. O tienes la parte de contains de, oye, esta palabra está en este punto. A ver si puedo hacerlo. En Python sería así, en RStudio. El int no está. Lo intento mirar porque en RStudio lo he utilizado muy poquitas veces, entonces necesitaría mirarlo. pero bueno, aquí tienes datos Eugene, que en principio de momento, y para vuestro nivel deberíais solo tener el nombre entero, pero que sepas que sí que existe la opción de contain que te permite encontrar una palabra dentro en este caso de una columna pero de momento, en principio con nombre entero vale, ¿te he ayudado Juan? no sé si prefieres que te llame Juan o Cristóbal, pero si te he ayudado sí, gracias profe me dejó una gran duda solo era porque tenía curiosidad de encontrar el capitán del Titanic si es que estaban los nombres pues así lo podrías hacer, sí que es cierto que aquí bueno, sí, aquí puedes hacer también filtro ¿vale? y aquí no estoy segura mira, aquí sí te lo detecta se llamaba Edward John Smith ese era su nombre, Edward John Smith pues si vamos a Edward porque creo que van primero los apellidos entonces si buscamos por igual por ejemplo pues aquí no parece que esté Smith, ¿no? me has dicho no, por Smith no está y por John tampoco, o sea que a priori no estaría está solo de los pasajeros parece, ¿vale? a menos que yo me esté confundiendo lo haya escrito mal no, está bien escrito está bien escrito Genial. Pensad de todas formas que esto lo podéis hacer, ¿por qué? Porque hemos puesto aquí antes un view datos, ¿vale? Si vosotros no ponéis este view datos, no vais a ver los datos en formato tabla y por lo tanto no vais a poder hacer filtros dinámicos de esta manera, ¿de acuerdo? Que también viene muy cómodo porque hay veces que es lo que dices, no sé si Ugin está con mayúscula o con minúscula. Lo busco aquí, veo que ya está con mayúscula y entonces ya voy aquí, ¿vale? Y lo puedo reemplazar en este punto. de acuerdo, entonces ¿qué vamos a hacer? tenemos los valores que son nulos y aquí lo que vamos a hacer además es reemplazar estos valores nulos o eliminarlos de momento vamos a continuar eliminándolos porque las siguientes clases ya utilizaremos todo el rato lo de reemplazar entonces espero que quede bien bien claro como se eliminan también porque si no ya veréis que empezaremos a dar clases que solo son reemplazar y cuando de verdad solo tenéis que eliminarlos decís es que no me acuerdo o no hemos practicado mucho de eliminar. Entonces en este caso vamos a crear datos limpios, de nuevo una caja donde voy a guardar el que pues eran mis datos, pero voy a decirle que no quiero los valores nulos, que no quiero que me dé los valores nulos. Entonces tenemos esta opción que sería los corchetes y aquí tenemos que decirle isNA de datos. esto buscábamos los datos que eran nulos y los datos que no eran nulos los datos que eran nulos nos los devolvían a verdadero y los datos que no eran nulos nos los devolvían a falso y con esto es parecido al filtro, lo que decimos es quiero quedarme aquellos casos en los que la condición que es lo que yo tengo dentro sea verdadero ¿qué ocurre? que entonces me va a quedar solo las filas de datos nulos de hecho si queréis lo podemos mirar lo hacemos, vamos ahora aquí y fijaros, son solo las filas que tienen datos nulos. ¿Por qué? Porque le estoy diciendo que lo verdadero o lo que yo me tengo que quedar es cuando es nulo. En RStudio para decir lo contrario, es decir, yo al final lo que no quiero que me aparezcan son los datos nulos, se puede hacer con una exclamación. Entonces nosotros hacemos la exclamación, hacemos el datos limpios y aquí tendríamos ya toda la información y aquí ya sí que no hay datos nulos. De hecho, podemos hacer si queréis también el view de datos limpios y se me generará como una tabla o como una columna completamente nueva. Hasta aquí todo entendido. Por cierto, también podéis hacer otra cosa. Os he dicho esta manera del filter porque para mí es la más cómoda en el sentido de que yo creo que es la que mejor se entiende. Pero a priori también deberíais poder hacer... Deberíais poder hacer lo siguiente. Y debería funcionar. Esta es mucho más fácil y con esta estoy segura de que todos, o sea, al final tienes la palabra filtro, sabes lo que estás haciendo, etc. Perdón, hay que poner aquí que son todas las columnas. Lo que pasa es que aquí ya implica que pongáis la coma rara, etc. Pero podéis hacer también el filtro de esta manera. personalmente creo que es mucho más sencillo esto porque no te tienes que acordar de la coma esta que a mí siempre se me olvida vale que vendría siendo quiero seleccionar las filas en las que tenga un jean pero quiero seleccionar todas las columnas cuando quieres seleccionar todas las columnas simplemente pones una coma y no pones nada porque por defecto esto significa que seleccionas todas las columnas si nosotros no queremos seleccionar todas las columnas si quisiésemos seleccionar solo por ejemplo la columna 0 pues a priori vale esto lo que me generaría es una columna fijaros dice oye tienes un data frame con cero columnas y una una una fila porque no tenemos si yo aquí pongo 1 vale pues será la primera la primera columna que era la del 17 si nos fijábamos aquí era la del 17 vale el pasan de haití si ponemos que queremos 2 pues que me va a salir me va a salir en este caso la segunda columna vale que en este caso era de supervivencia que es cero si queremos todas lo dejamos directamente de esta manera vale lo mismo me pasa aquí como antes he seleccionado esto sin la comida que me ha pasado me ha pasado que cuando he ido a mirar los datos limpios me salía en un formato raro vale si yo le pongo la coma el indicó que quiero todas las columnas ahora ya me sale como un formato más normal vale cuidado con esta coma que la verdad es que se pierde con mucha facilidad y nos puede hacer que el algoritmo deje de funcionar siguiente cosa que teníamos que hacer volvemos a nuestros datos limpios y lo que veíamos era que si yo hacía el str de datos limpios teníamos variables que eran en formato categórico y variables porque si no vale porque este signo sea si yo le quitas este signo lo que estaría diciendo es que quiero quedarme todas las filas en las que esto es verdadero que me indicaba es y cenea de datos lo que me indicaba es que me seleccionaba todos los nulos vale recordad que es is punto en ya lo que me dice son los variables que son nulos en res y yo no pongo esto y hacemos el view que tengo fijaros tenemos todo anuló porque porque el isna lo que me ha dicho es que me seleccione los nulos en r se niega yo quiero lo contrario justo quiero los que no son nulos se niega con una exclamación delante vale entonces conocemos esto aquí ya no tendré ningún nulo porque la exclamación niega la condición vale entonces en vez de buscar por lo que es verdadero buscamos por lo que en este caso sería falso si queda más claro francés si está claro una pregunta no sé si se suele usar si no no sé si se suele usar mucho pero en la parte de arriba cuando dices el call sums si aplicáramos el signo de admiración no ser nos de exclamación perdón nos agregaría algo o sea te agregarían los datos que no son nulos Mira, si lo hacemos, los datos que no son nulos en cada una de las columnas. ¿Qué es lo que me pasa con esto? Realmente lo que nuevamente solemos querer ver son los nulos, pero porque son los que nos dan problemas. Porque al final los que no son nulos aquí lo tenemos igualmente. Fíjate que aquí tenemos el 891 observaciones y yo aquí ya sé que tengo 891. Entonces, generalmente tendemos a mirar exactamente cuántos nulos son para poder decir, son muchos, son pocos, los puedo eliminar, los puedo no eliminar, me los puedo quedar, los puedo reemplazar, ¿vale? Para decidirlo. Y muchas veces se decide en función del número de nulos que tengas. Por ejemplo, si yo tengo 891 casos de los cuales 600 son nulos, pues evidentemente no los voy a quitar porque me estaría quitando prácticamente todo mi dataset. Entonces, esto se suele hacer, por eso se puede hacer sin la exclamación. Pero si lo hacéis con la exclamación tendréis otro dato que también es interesante, que es cuántos valores tenéis. Y de hecho, incluso por ejemplo, ya aprovechando el CallSums, como hoy estamos más de repaso, imaginemos que lo que queremos hacer es calcular, me lo estoy inventando, pero calcular el número total de supervivientes. Pues en principio En principio, si hacemos Datos Dólar, Survive ¿Vale? No, te pide que sea Que sea al menos Hacemos el filtro, primero hacemos un filtro Bueno, espera, es más fácil Hacemos el Sunlight Y aquí le ponemos la columna Como tal, que sería en este caso Datos Survive para no complicaros y ver que podéis utilizar las mismas tablas que hemos visto. Si vosotros dentro de esa marica que hemos puesto todo el dataset, seleccionáis la columna, podéis tener los valores de la propia columna. Entonces, más cosas que vamos a hacer. Tenemos el strDatosLimpios, que esto era para indicar que había algunas que eran de estilo categórico. Aquí tenemos las que son de estilo entero, que tenemos valores, números y decimales, tenemos números en un formato más general y tenemos texto. Lo que habíamos comentado es que los textos generalmente no nos interesan utilizarlos, la mayoría de algoritmos dan problemas en los textos. Entonces, existe un tipo de dato que es el tipo de dato factor. ¿Qué es el tipo de dato factor en RStudio? De hecho, solo existe en RStudio, en otros lenguajes de programación como Python no existe. El tipo de dato factor lo vais a ver muy fácil, por ejemplo, en el caso de hombre y mujer. En este caso de hombre y mujer solo tenemos dos valores, o bien es hombre o bien es mujer en este dataset. Entonces, cuando solo tenemos dos datos, es decir, que solo puede ser dos datos o tres datos o cuatro datos, pero tenemos un número finito de datos, Es decir, no hay 891 datos diferentes, sino que hay un número concreto de posibilidades que tenemos. Entonces, esto lo podemos convertir a un tipo de dato que se llama factor. El factor lo único que nos está diciendo es que tenemos un conjunto, una variable que es texto, pero ese texto solo puede tener cuatro posibilidades o dos posibilidades. posibilidades. ¿Cómo hacemos esto? Perdonad que si lo escribo ahí luego nos lo puedo entregar. Vamos a seleccionar, por ejemplo, la columna en este caso de género y, por ejemplo, también la columna del tipo de pasaje. Es importante también entender que el factor no solo se aplica, la mayoría de las veces lo aplicamos única y exclusivamente a las variables que son de tipo texto, ¿vale? Pero si alguien lo aplica a la variable que es de tipo numérico, pero por ejemplo a Survive, a Pclass, no habría tampoco ningún tipo de problema, ¿vale? nosotros ahora mismo lo vamos a aplicar únicamente a las variables que son de tipo de texto vale porque al final son las que sí o sí la mayoría de las veces vais a tener que cambiarlas a número vale entonces tenemos datos limpios vamos a seleccionar en este caso la columna del sexo de las personas y aquí lo que vamos a hacer es decirle que lo queremos convertir a un factor as punto factor es decir como factor el que pues tenemos que volver a poner la columna que estamos considerando vale porque pongo la misma columna en ambos lados porque quiero modificar esta columna es decir que me la considere como un factor y guardarlo en la misma columna no quiero crear una columna nueva sino lo que quiero es modificar la columna que ya tengo cuando hacemos el std de datos limpios fijaros que aquí ya tenemos factor. Además me dice factor que tienes solo dos posibilidades, que es o bien mujer o bien hombre. Y además me dice, fijaros, que mujer lo considera como un 2 y hombre lo va a considerar como un 1. ¿Por qué es uno, dos y otro uno? Generalmente porque el primero que me sale lo pone como uno, o sea, va por orden de aparición. Si el primero ha sido un hombre, pues le da un 1 y el segundo ha sido una mujer, pues le da un 2. Si hubiese sido al revés y la primera fila fuese una mujer, muy probablemente la mujer tendría el 1 y el hombre tendría el 2, simplemente por orden. ¿Hasta aquí todo correcto? Sí, efectivamente, porque aunque tú veas que realmente sigue siendo texto, en realidad para RStudio, y solo existe en RStudio, es como un tipo intermedio. Es como un texto, pero un texto que solo tiene unas determinadas posibilidades y por lo tanto puedes enlazarlo o puedes cambiarlo a numérico. Entonces, sí, sí, esto es un tipo de dato que única y exclusivamente existe en RStudio. Si trabajáis, por ejemplo, en Python, o no existe o no se utiliza casi nunca. Al final haces el encoder y lo cambias al número directamente. ¿Vale? ¿Pero no suelta la duda, Víctor? Espero que sí. De hecho, es por asfactor. El as lo que hace es cambiar y aquí pones el nuevo tipo de dato que quieres. En este caso, asfactor. Luego existen asphonumeric y existen otras opciones. Vamos a hacer ahora el completo, me interesa haceros el completo porque así aprovechamos que me lo preguntasteis el otro día, os lo subí pero realmente no os expliqué cómo hacer el split de los datos, ya tenéis el completo explicado. Entonces, para esto, ¿qué tenemos que hacer? Recordemos que estamos ante un algoritmo supervisado. En el caso de los supervisados, lo que se hace es que se toma un 70, un 80% de datos para el entrenamiento y un 20, un 30% de los datos para el testeo. ¿De acuerdo? Entonces, ¿qué vamos a hacer? Vamos a decirle al algoritmo cómo hacer esa división, cómo hacer este 70% para entrenamiento y este 30-20% para testeo. Entonces, para ello necesitamos una cosa. Primero, vamos a querer darle una cierta aleatoriedad. Esto se hace con el set seed. Existen muchísimas más. Al final no existen números aleatorios en informática, son todo números pseudo aleatorios que vienen de determinadas fórmulas matemáticas, fórmulas matemáticas bastante complejas, por lo tanto al final no lo puedes prever. Entonces, ¿qué hacemos con este set seed? Es decir, básicamente, muy en general, la semilla o la fórmula muy entre comillas que vamos a utilizar para generar esos números aleatorios. existen muchísimas, ¿vale? Por ejemplo, si alguien conoce Python, esto es lo mismo que el random, ¿vale? Cuando pones el random a 42, el random state a 42, solo que aquí en RStudio se llama setSeat, pero es una forma de generar números aleatorios. ¿Por qué? Porque a mí lo que me interesa no es seleccionar las 70 primeras... Imaginemos que tenemos un conjunto de 100 filas, ¿vale? Que yo quiero clasificar. A mí no me interesa seleccionar siempre las 70 primeras filas. No me interesa. ¿Por qué? Porque imaginemos que estamos diciendo si una persona va a ser cliente o no va a ser cliente. Y los 70 primeros clasos siempre son cliente. ¿Qué va a pasar? Que mi modelo de entrenamiento siempre va a predecir que es cliente. ¿Por qué? Porque todos mis datos han sido cliente. Entonces el algoritmo va a decir, pues a ver, lo más sencillo, si todos mis casos son cliente, cuando me des un caso nuevo también será un cliente. Entonces, para poder evitar estos problemas que vienen de órdenes, por ejemplo, tenemos siempre arriba los de poder adquisitivo mayor. A lo mejor resulta que no le da importancia a la variable de dinero, a la variable de ingresos. ¿Por qué? Porque los que hemos utilizado son todos con ingresos mayores. y luego voy con uno de ingreso menor y me dice que también es cliente cuando resulta que no, pero es porque no tiene en cuenta la variable de ingresos. Entonces, para que no nos pase esto, se hace la partición de manera un tanto aleatoria. Y para ello vamos a decir primero qué filas tienen que ir al conjunto de entrenamiento. Esto serían las filas, el número identificativo de las filas que vamos a ponerlo al conjunto de entrenamiento. Para crear este conjunto de filas existe una función que la verdad el nombre tampoco es tan diferente, es CreateDataPartition, es decir, estamos haciendo una creación de partes, estamos partiendo los datos y queremos decir por qué columna lo vamos a querer romper. En este caso queremos hacer una partición por la columna de survive. Es decir, yo quiero que me hagas en la columna que yo quiero avanzar o que yo quiero analizar, quiero que me hagas la partición. A veces aquí simplemente se pone solo datos limpios o antes ya se ha definido qué es la X y luego qué es la Y y entonces haces XY. El orden de los factores no va a alterar el resultado. O primero descomponéis en qué es la variable que quiero predecir y qué son las variables que me van a servir para ello. O primero descomponéis en entrenamiento y testeo y luego hacéis lo otro. El orden nos va a afectar en el resultado. Ahora yo quiero que este conjunto de entrenamiento tenga un 70% y que no me lo dé en formato de lista. Esto simplemente para que yo luego pueda hacerlo bien, dejarlo en formato de índice porque lo siguiente que voy a hacer es hacer los filtros. Algo parecido a lo que hemos hecho con esto de aquí. De hecho va a ser prácticamente igual. ¿Qué va a ser mi conjunto de entrenamiento? Por ejemplo, vamos a llamarlo train de entrenamiento. Pues va a ser seleccionar mis datos, en este caso es datos limpios. ¿Y qué quiero? Quiero seleccionarlos en los índices, en las filas que me está dando mi programa, mi train index. Entonces, será train index y muy importante, no se os olvide la coma y el espacio, porque queremos todas las columnas de momento. ¿Qué será test? El test será el resto de filas. Entonces, el resto de filas, aquí la mayoría de vosotros me diréis, pongo la exclamación y ya está. Estoy segurísima. Pero la exclamación sirve cuando estamos haciendo una condición. Aquí estamos haciendo la condición de que queremos algo diferente a los datos nulos. aquí sin embargo bueno vamos a creo que es mejor que lo ejecutemos vale he escrito mal la función createDataPartition creo recordar vale, que no he puesto la librería caret, efectivamente para esa función necesitáis una librería nueva que es la librería caret vale, que se me ha olvidado library caret vale, entonces nosotros ponemos la librería caret, si nos funciona necesitáis sí o sí instalarlo aquí está cuando termine hasta aquí lo vais siguiendo más o menos, estoy intentando explicarlo mucho más detallado porque al final tenemos una clase completamente de práctica y quiero que quede todo muy muy claro es mi objetivo porque veréis que luego conforme sea otro tipo de regresión o otro tipo de clasificación, al final el procedimiento es prácticamente el mismo en todos los casos o sea no hay una diferencia enorme entonces si entendéis ahora el procedimiento ya veréis que el resto es exactamente igual no hay mucha no hay mucha diferencia vale de acuerdo ya tenemos la librería caret en mi caso la tengo instalada si no lo tendréis que instalar como habéis hecho el titanic pero en lugar de poner aquí titanic pondréis careto esto porque porque el create data partition vale es específicamente de esta librería. Me ha dicho que tengo valores nulos. ¿Por qué? Porque he ejecutado esto probablemente sin la exclamación. Un momentito. El dato limpio survive no tiene ningún tipo de valor nulo. no tiene ningún valor nulo vale, el create data partition lo he escrito bien, create data partition de la columna data limpios, tenemos el p que hemos seleccionado de 0.7 y que la lista sea falsa vale y tenemos aquí el train index nos da un error en una parte del create data partition vale, los valores nulos no están permitidos Sí, vale, pues vamos a hacer caso al error. Vamos a ver si hay suerte y podemos decir que me manejen los nulos. Vale, y no lo reconoce, o sea que no hay suerte por aquí. Vale, pues esto no debería pasar. Un momentito, ¿eh? Aquí hay data partition, los datos los tenemos bien desde el inicio, tenemos aquí los datos de Titanic Train, tenemos aquí también datos de Survived que en principio no debería pasar y si nosotros hacemos de datos limpios, esto creo que va a dar un error, efectivamente porque hay muchos elementos, entonces aquí tenemos el data Survived, la probabilidad es de 0,7 no debería haber problema, el list debería ser igual a 0 vale, cuando os pasa esto de acuerdo, si os seleccionáis arriba veréis un poco la información tienes que dar la variable y hasta aquí está bien y nos dice que nos recomienda list igual a true vamos a probar con list igual a true aunque creo que no es el caso pero por si acaso vamos a poner a falso entonces vamos a hacer una cosa que es vamos al help momentáneamente ¿vale? y aquí podéis buscar cualquier librería que queráis, esta es como el manual de instrucciones realmente, entonces tenéis aquí la librería caret y aquí deberíais tener la información del create data partition create data partition RStudio viene bien porque tiene esta información aquí os pone distintos ejemplos que podéis tener, cómo lo podéis hacer y tenéis lo único que está siempre en inglés pero esto debería ser bien porque no tenemos valores nulos datos limpios ya lo hemos hecho para que no tenga los valores nulos si no recuerdo mal código, código sí, sí, sí estoy ahora mismo un poco un momentito voy a hacer una cosa voy a eliminar esto Bueno, tranquilo, tranquilo. Sí, esto os pasará muchísimas veces, porque al final pasa siempre. Estás haciendo una cosa y dices, no tiene ningún sentido porque son 891 filas y no tiene sentido que te aparezca 9000. entonces algo he hecho por aquí que no le ha gustado a ver por qué es tenemos esto hasta aquí está todo bien tenemos los 891 casos vale aquí le hemos hecho el view datos limpios y si vamos a datos limpios datos datos datos datos no si vale entonces si hago de datos esto me va a salir que son 891 y 12 vale si hago de datos limpios son 10.515 entonces el problema está que aquí me los duplica completamente vale y si en lugar de hacer esto un momentito hago esto datos que debería funcionar exactamente igual 714 vale y si lo hago de esta forma es otra forma de hacerlo en lo que pasa que en la que en la unir utilizan esta forma entonces por eso os he dado esta forma pero y sene a la trata de datos y aquí ponemos esto esto debería funcionar para un motivo cuando hacemos esto y miro la dimensión a ver no, esta no me va a funcionar bueno, como esto nos está fallando os doy la fórmula que yo siempre utilizo que es diferente, vale y funciona igual, que es básicamente omíteme los datos que son nulos, vale, y con esto en principio, ahora si tienes los 714 vale, y aquí podemos tener ya los factos library y todo bien, y este ya nos debería de haber, efectivamente me he quedado con ¿por qué esto nos da? bueno, lo miro en casa que tampoco es el motivo ahora mismo para enseñaroslo porque me está sí que debería funcionar, al final son datos haces el is.na de datos, esto sí que debería funcionar sin ningún tipo de problema si nosotros hacemos ahora el dim lo que hace es mirarte cuántas filas y cuántas columnas Entonces lo que no tiene sentido es que cuando elimine filas me salgan muchos más datos que al inicio. Entonces si lo hacemos sin la exclamación, ¿lo está haciendo bien? Si lo hacemos con la exclamación, no. Vale, pues aquí es donde está el error. Vale, puede que me esté haciendo un factorial. Y si lo hacemos con el menos, no me... Hacerlo con el menos, ¿vale? Sé que en la UNIR lo que se enseña es hacer esto, ¿vale? Lo que tenemos dicho, pero hacedlo con el menos. Funciona mucho mejor, ¿vale? Con el menos, o ya si queréis no complicaros la vida, el na.mix es como una forma más fácil de decirlo y funciona todo mejor, ¿vale? Porque sí que sé que supuestamente esto debería funcionar, que para eso os damos este, pero si a mí ya me ha dado error en un caso que no debería dar error, me fío poco. Entonces, utiliza mejor el eneomit o cualquier cosa. En realidad, os van a corregir bien si tenéis la exclamación, pero no es... Sí, efectivamente, es para eliminar los nulos todo, tanto esta como esta como el eneapuntomit. Todas son para eliminar los nulos. la que yo más utilizo es el enea.mit porque me es más cómodo en cuanto a que es más la forma en la que nosotros estamos acostumbrados es omite los valores nulos es prácticamente más sencillo y esto siempre al final acabas con alguna cuestión porque si le pongo sin esto tampoco lo hará bien no, ¿veis? no te da ningún caso entonces mejor hacerlo con el enea.mit y ya está si en la entrega la habéis hecho con la exclamación no os preocupéis porque estará bien igualmente o sea que en principio ellos quieren que lo hagáis con esta funciona mejor esta, yo personalmente siempre utilizo la segunda, la primera es mucho más cómoda vale, entonces cuando hacéis esta parte ahí tenéis el train index, de acuerdo, ahora sí que ha funcionado, entonces ¿qué es el train index? el train index lo que me va diciendo son las filas, el número de filas que lo voy a tener entonces por ejemplo Aquí tenemos, a ver, que las filas que he introducido, pues fijaros que no empiezan la 1. La primera fila es la 5, la 6, la 8, la 10, la 11. Es decir, de estas filas que tenemos aquí, que tenemos aquí un numerito, lo que nos dice este train index es cuáles son las que va a haber para que lo vayáis pasando de un lado a otro. ¿Vale? Entonces, de aquí lo que decimos es que queremos que se queden las que son del índice, por tanto aquí traen. Si miramos la dimensión, pues aquí tendréis que son 500. Si calculáis de las 891 el 70%, debería ser aproximadamente estos 500. Y en este caso el resto, por eso ponemos el menos, iría al test. Entonces, cuando miramos la dimensión del test, pues tendríamos que son el resto de filas o al menos deberían ser las 214. El dataset original venía de 714 cuando le quitábamos los nulos. Entonces, ahí tenemos el 70 y el 20. ¿Se ha entendido todo? Salvo ese momento de exclamación que al final son temas Sí, el resto como el resto de cosas ¿Vale? Que al final son cosas del directo de estar haciéndolo aquí con vosotros, ¿vale? Siempre hay alguna cuestión que siempre falla, ¿vale? Entonces, ahora vamos a aplicar En este caso ¿Dónde selecciono el porcentaje que va a hacer el entrenamiento? Aquí que tienes el 0,7 es el 70%. Siempre se pone en valores entre 0 y 1. Exacto. Y por defecto el resto ya vendría a ser el 30 cuando pone el menos. Exacto. Comentamos pasos, sin problema. Primero, las librerías. ¿De acuerdo? Vale, repasamos. Primero las librerías. Luego, cuando cargas los datos hay que mirar dos cosas. Primero, qué tipo de datos tienes. las principales métricas estadísticas y luego cuántos nulos tienes. Cuando decidas cuántos nulos tienes, lo siguiente que pensarás es cómo puedo eliminar los datos. Mi recomendación, ya basada en mi experiencia, es pitar el nea.mit. Funciona muchísimo mejor y es mucho más claro de entender. Una vez que ya has limpiado los valores que son valores de tipo nulo, lo que haces es que las variables que son de tipo categórico las vamos a cambiar a un tipo que se llama factor. ¿Por qué? Porque las variables que son tipo texto no suelen funcionar muy bien cuando tenemos ciertos algoritmos como son los algoritmos de Machine Learning. ¿De acuerdo? Entonces, una vez que has cambiado y has hecho ya este tipo factor, lo siguiente es descomponer tu dataset. ¿En qué lo descompones? En conjunto de entrenamiento y conjunto de testeo. Pero para ello tienes que decidir qué filas van a ir a tu conjunto con el que creas el modelo y qué filas van a ir al conjunto con el que luego vas a aplicar el modelo. Esto lo hacemos desde esta parte, el set seat hasta esta parte del test. ¿Qué es el set seat 1, 2, 3? Esto es que nosotros necesitamos escoger estas filas de manera aleatoria. ¿Para qué? Para poder evitar ciertos riesgos que pueda haber en nuestros datos. Si nosotros predijésemos que si alguien es cliente y pusiésemos arriba las personas que siempre han sido clientes, cuando yo coja el 70% de los datos, cogeré que todos son clientes. Entonces, ¿cuál va a ser el problema en este caso? Que no me va a predecir nunca que no es cliente porque todos los de arriba resulta que sí que lo eran. Entonces, para evitar problemas que se dan debido al orden que pueda haber en los datasets de partida, se hace de manera aleatoria. Y el setSeed123 es que llamas a una función que para ellos es la función 123, que te genera el número aleatorio que le estás diciendo que vas a hacer cosas de manera aleatoria. Después, el train index. Lo que hacíamos básicamente es la columna que queremos predecir, pues de eso me vas a partir en un 70%, en este caso, las filas que van al entrenamiento y un 30% las que irían para el testeo. Entonces, lo que hacemos con este create data partition es que train index va a tener el índice, el número de la filita que tenemos aquí, el número este de fila, de las 700 filas, perdón, del 70%, o sea, de las filas que tú quieres para el conjunto de entrenamiento. Por eso aquí lo que le decimos es, oye, quiero que las filas que me ha dado este train index, que son las de mi conjunto de entrenamiento, me las guardes en train. Y el resto, por eso ponemos aquí el menos, me las guardes en test. ¿Sí? ¿Os he resuelto tanto Sebastián como Víctor las preguntas? Sí, profesora, muchas gracias. Se resume al final en carga de datos, control de los nulos y entendimiento de los datos. Después, variables categóricas, las pasáis a factor. Después creáis conjunto de entrenamiento, conjunto de testeo. Y ahora ya podemos aplicar, por ejemplo, el modelo. En este caso, el modelo que habíamos propuesto de una regresión logística que se llamaba GLM. Y decimos, yo quiero predecir la columna Survive en función de qué. en función de, por ejemplo, la edad y, por ejemplo, el sexo. Podemos hacer más, podemos hacer todas las variables que queráis. También podemos decir, por ejemplo, que lo queremos de el tipo de categoría, por ejemplo, P-Class. De aquí vamos a simplificar que data tenemos que decir que viene de nuestro conjunto de entrenamiento porque para crear el modelo le voy a dar mi conjunto de entrenamiento y siempre se suele poner que la manera en que lo encuentre es binomial porque al final es o sobrevive o no sobrevive. Hay dos opciones, por tanto binomial. De aquí, cuando creamos el modelo, no he encontrado la pclass porque básicamente se llamará de otra manera. Efectivamente, por la C mayúscula. ¿Veis que he puesto aquí una C mayúscula? De aquí la importancia. Y cuando vamos aquí es una C minúscula. Solamente por eso ya no me detecta la columna. Entonces hacemos esto. De esta manera tenemos el modelo creado. Ahora voy a mirar cómo de bien ha funcionado. De esta forma podemos tener... Si no se te instala la librería Caret, te recomiendo lo mismo que le he recomendado a Irina. Descárgatelo de post-it.co. ¿Vale? Te copio el enlace. Lo mismo, tanto el R como el RStudio. Dime. En la creación de la partición de los datos, ¿por qué se le pone list igual false? ¿Cuál es la diferencia de ponerle verdadero? o cual podrán de truco. Mira, una vista vale más, o sea, una imagen yo creo que vale más que mil palabras. Entonces, hacemos esto, hacemos esto. Al final con R y con todos estos lenguajes de programación, las dudas que tengáis, no tengáis ningún tipo de problema en preguntarlas. Esto me lo genera así. ¿Qué se les pasa si yo pongo aquí truco? Que era la pregunta, pues bueno, veamos qué pasa. ¿Qué me lo genera más en un formato más compacto? De esta forma es un formato más estilo tabla y de esta forma es un formato más completo. Entonces, a priori, fíjate que lo que me dice cuando voy a, dices, bueno, lo veo mejor así, voy a ejecutar la siguiente línea. Me dice, no puedes porque es una lista, porque él está acostumbrado al tipo formato. Entonces te dice, oye, no puedes porque el índice que me estás poniendo no es un índice, sino que lo tienes en formato lista. Entonces por eso hacemos aquí el list igual a false. Es porque así con este no me va a dar error, porque él está acostumbrado a verlo más formato tipo tabla, cuando hacemos las localizaciones de filas dentro del filtrado del dataset. ¿Se entiende? Sí, profe, gracias. inversión vale aparte de los los errores en este caso son bastante grandes pero bueno lo que me interesaba es que veis es esto ahora también me voy a extender un pelín el mgl m lm es regresión lineal vale y g lm es regresión logística la regresión lineal que es la que hacíamos con el m es que tú quieres predecir un número en función del resto por ejemplo el precio del precio de un producto tú quieres predecir el precio del producto en base a un histórico de precios que ha tenido y a una serie de columnas como puede ser la oferta la demanda etcétera vale entonces ahí utilizaría es una revolución lineal porque tu objetivo es predecir un producto un número vale estaríamos dentro de los algoritmos de regresión en el caso de la regresión logística aunque también se llama regresión lo que se suele utilizar es mucho más para predecir categorías que ya conoces junto con el random forest que hoy me extender un poquito pero sí que quiero que veáis tanto la regresión logística como el random forest y la matriz de confusión vale entonces era el anterior vez también me habéis dicho no que queremos también hacer el tema de las predicciones vale no pasa nada una vez que tenemos creado el modelo nosotros podemos decidir hacer predicciones con que generalmente se hace se suele hacer con el testeo porque porque luego voy a comparar la predicción que me ha dado con el valor de testeo que yo sé que es el correcto y voy a poder decir cuántas veces mi modelo ha acertado sí entonces para esto vamos a utilizar simplemente la palabra predict predict de que de con mi modelo en mis nuevos datos que mis nuevos datos yo sé que es mi conjunto de test y el tipo lo ponemos en este caso que me quiere dar una respuesta tipo respuesta vale generamos esto y cuando veamos la palabra predicciones fijaros lo que me da me da valores entre 0 y 1 pero no exactamente 0 y no exactamente 1 ¿por qué? porque la regresión logística en realidad es una fórmula matemática que da valores generalmente entre estos dos pero no acaba de dar ni uno ni otro entonces ¿qué normalmente se hace? se redondea si es más de 0,6 redondeamos a 1 y si es más de 0,5 redondeamos a 1 y si es menos o igual redondeamos a 0. Entonces, ¿cómo podemos hacer? Binomial es una fórmula matemática, una distribución matemática de probabilidad en la cual tienes dos valores. Por ejemplo, cuando tú lanzas una moneda al aire que puede salir cara cruz y consideremos, por favor, que es una moneda justa, que luego la probabilidad es un medio, eso es una distribución probabilística que se llama distribución binomial. Esto es un tema de matemáticas ya cuando hablamos de todo el tema de las distribuciones. Que bueno, básicamente es que tú tienes dos posibilidades, ¿vale? Y esas posibilidades hacen que al final tú tengas una determinada probabilidad asociada a la ocurrencia de uno o de otro. ¿Sí? ¿Se entiende? Es uno de los motivos por los que la regresión logística se utiliza cuando hay solo dos posibles opciones. Entonces, tenemos ahora la parte de predicciones. y lo que os comentaba, lo que generalmente se hace es redondear, entonces vamos a decir, si es mayor que 0.5, redondeamos a 1 y en caso contrario, redondeamos a 0. En este caso utilizamos una función que se llama if-else. En el caso de if, en inglés, ¿qué significa? Sí, y else se utiliza para el resto. entonces vamos a decir que mis predicciones realmente esto lo que me está dando son probabilidades entre 0 y 1 por eso luego cuando lo redondeo al valor más exacto pero bueno predicciones y le voy a decir si es mayor que 0.5 dame un 1 por lo tanto si es estrictamente mayor que 0.5 me das un 1 y en caso contrario me generas un 0 en este caso cuando yo ponga de nuevo la palabra predicciones ya no sean estos números tan raros sino que serán mis ceros y unos entonces aquí hemos hecho una predicción lo último que nos queda que ya es con lo que probablemente bueno sí que quería enseñar el random forest así que luego os haré lo mismo con random forest que es aplicar aquí una línea más no es más entonces lo que como se mide esto con la matriz de confusión vale entonces sí que me interesa y sí que la veremos más veces porque vamos vamos bien de temario pero hice que me apetecía daros una clase entera de práctica y un poco hacerlo todo con un poco más de calma vale entonces qué decimos decimos matriz de confusión le voy a dar lo que el modelo me ha predicho y le voy a dar el resultado real yo realmente tengo el resultado real porque la columna del test survive es el resultado real teníamos vale vale se me ha olvidado poner que antes tengo que convertirlo en data refresco de factos with the same levels esto suele pasar porque porque si hacemos predicciones efectivamente porque predicciones no es una lista sino que lo puedo mirar por aquí precios tiene 214 valores y no es un data set sin embargo el test cuando hago test survived Cuando hago esto, aquí sí que si yo esto lo pongo como reales, aquí tenemos que también son de 1 a 214, pero fijaos aquí se llama nu y aquí se llama int. Esto viene porque tengo que convertirlo en 2 de la verdad, que hora es? Vale, 10.08. vale, entonces vamos a hacer entonces la matriz de confusión de manera correcta, vale, entonces lo que vamos a poner es, fijaros el error que nos ha salido, nos ha salido un error que nos dice, oye que deben ser los mismos factores, cuando se haga este tipo de errores lo que podemos hacer es intentar hacerlo lo más rápido o lo más sencillo posible, me está hablando de factores y nosotros recordamos que tenemos los factores de la manera STR porque era un tipo de dato Si hacemos str de test, veré que en este caso tenemos el survive que es un entero y si hacemos str de predicciones no lo vamos a poder hacer porque básicamente es una lista, pero el error viene de algo que tiene que ser un factor. ¿qué haría yo si me encuentro con esto? si me encuentro con esto lo que diría es el problema es que tienen que ser factores pues hagamos los dos factores entonces vamos a hacer predicciones me interesa también que veáis los errores porque al final cuando vosotros estáis haciendo estas cosas el problema es que muchas veces no vais a ser capaces os va a dar un error y el tema es que cuando os da un error no tenéis que bloquearos, tenéis que leer el error y entender el error Es uno de los casos que más problemas suele dar cuando estamos hablando de Data Science, que realmente ellos luego, la gente no sabe, ve un error y se pierde, ¿vale? Entonces, aquí pondríamos Survive. ¿Qué me ha dicho? Que el problema es que tienen que ser todos factores. Bueno, pues no pasa nada, vamos a cambiar los dos a factores. si lo escribo bien quizá entonces cambio factor cambio factor vale y ahora me da otro error que tiene que tener los mismos niveles vale ¿por qué? porque si nosotros hacemos predicciones vale tenemos esto así y si hacemos aquí el test survive bueno de hecho el datos limpios Ah, vale, porque es datos limpios y no test survive. Test survive. Vale. Y me da error. ¿Por qué? Ah, porque este también tiene que ser test survive. Vale. Vale, fijaros que ya me ha funcionado. Es decir, nuestra idea de, nos ha dicho que es un problema de factores, cambiemos las dos cosas a factores, ya con eso funciona. Y ahora vais a poner la matriz de confusión, la confusion matrix. Vale. Y os da el resultado. ¿Qué me dice este resultado? Fijaros que tenemos aquí la predicción y el valor de referencia, es decir, mi valor real. Lo que me dice esta matriz de confusión y lo que deberéis poder analizar es que en la predicción me ha dicho 0 cuando el valor real era 0. ¿Cuántas veces? 108 ocasiones. Esto se llama verdaderos positivos porque ha acertado 108 veces el valor 0. Sin embargo, cuando el 1, aquí teníamos los verdaderos negativos. Es decir, aquellos en los que el modelo me ha dado que es 1 y realmente es 1. Estos 26 que son aquellos en los que realmente el valor era 1, es decir, era negativo pero me ha dado que es positivo, los falsos positivos por tanto, y este valor lo veremos más en detalle porque esto lo sacaremos muchísimas veces. Pero este último que sería el 13, que vendría a ser que el valor real era un 0 y el modelo me ha dado un 1. Es decir, la diagonal directa del 108 y el 67 el modelo lo ha hecho perfecto y la diagonal inversa del 13 y el 26 vendría siendo que el modelo se ha equivocado. Con esto hay una métrica que es la más importante que se la cura así, se calcula sumando las veces que ha acertado el modelo 108 más 67, de hecho lo podemos hacer aquí, 108 más 67, dividido entre la suma de todas las veces que lo ha intentado, que viene siendo 108 más 26 más 13 más 67. Ojo, no es dividido entre las veces que ha fallado, sino dividido entre todas las veces que lo ha intentado. Y de aquí saldría este 0,81 que es el mismo valor redondeado que el de aquí arriba, que es el de la cura. Hay dos valores también que son importantes, que es la sensitividad y la especificidad. La sensitividad viene siendo cómo de bien predice los valores positivos. positivos y se calcula con las veces que ha acertado el positivo dividido entre todos los valores que eran positivos es decir 108 entre la suma de 108 más 103 lo que nos dice es de todos los positivos en cuántos ha acertado como de fiable me puedo creer yo este positivo vale y luego tenemos aquí este 67 que perdón este especificidad que es lo contrario es decir de Todos los negativos, cómo de bien o cómo de fiable es. Es el negativo. ¿Os acordáis? Y con esto ya termino. ¿Os acordáis del test COVID? Los test de COVID que os decía, no, es que el test de COVID da muchos falsos positivos. ¿Vale? Todo test médico es básicamente esto. Y cuando os dicen que da muchos falsos positivos o que da muchos falsos negativos, lo que os están diciendo es el valor de la sensitividad o de la especificidad. Entonces, para ciertas cosas como los test médicos, ¿qué es lo que más importa? A veces para una enfermedad importará el hecho de que, bueno, lo ideal es que siempre lo predica todo perfecto, ¿vale? Pero ¿qué pasaba en el caso del COVID que era tan contagiosa? Que lo que interesaba era que a la mínima sospecha que hubiera nos dijese que estaba enfermo. Por eso daban muchos falsos positivos. Sin embargo, si el test te decía que no estabas enfermo, era muy probable que no lo estuvieras de verdad. Es decir, tenía una sensitividad baja porque detectaba muchos falsos positivos, pero a cambio tenía una especificidad alta. Es decir, si te decía que eras negativo y no tenías la enfermedad, es que de verdad no la tenías. ¿Sí? ¿Queda claro? Así para que veáis un caso ya ajeno a todo el tema de Python y todo que os puede servir. Bien. Más o menos. Profe, ¿y cuál es un valor aceptable de la cura? Vale, para mí, en la vida real, para mí está siempre por encima del 75%. Es que depende un poco de lo que quieras y del grado. Por ejemplo, no es lo mismo que yo esté diciendo que tengo que operar a alguien y, por tanto, el grado de acuracy tiene que ser perfecto, prácticamente tiene que ser por encima de un 90%, a que yo esté haciendo una cosa para mí, completamente para mí, que no tiene ningún tipo de trascendencia y es simplemente para analizar unos datos que a lo mejor ahí, pues que me dé un 60% de precisión, te digo, es correcto. Pero generalmente para mi gusto casi nunca, en la vida real casi nunca, ya os lo digo, se adquieren acurací de 90%. Es súper complicado porque casi nunca existen. Y yo te diría que entre el 75% y el 80%, entre el 75% y por supuesto para arriba, sería lo más aceptado. Pero si sale una acurací del 90% es que tenéis muy pocos datos y lo más probable es que cuando añadas más datos te va a bajar esa precisión. Porque tienes que pensar que nunca los datos son exactamente perfectos, nunca todo el mundo se comporta igual. Habrá gente con 30 años que sobreviva y habrá gente con 30 años que muera. Entonces, realmente, aunque siempre hay patrones ocultos, nunca hay un patrón exacto y perfecto. Respecto a con qué vais a practicar. Profesora, ¿y la línea inferior del 95% de intervalo? El intervalo de confianza. ¿Y si sale 60, bueno, 0,6, o sea, intervalo es muy amplio, influye también? Sí, aquí influye. El intervalo de confianza se calcula también con ese 0,05 que os había comentado, que es el grado de incertidumbre que tú quieres tener. O sea, yo en este caso estoy diciendo que tengo una precisión de 0,80, dando por supuesto que puedo desviarme 0,75, o sea, que siempre mi solución va a estar entre estos dos valores. Es decir, aunque mi precisión de normal sea 0,81, sí que es cierto que en algunas ocasiones será de un 0,75 y en otras ocasiones será de un 0,86. Básicamente, este está diciendo, o sea, al final la cura sí no es exacto, pero el modelo te dice, oye, con casi una seguridad del 95%, o sea, con una seguridad muy alta, casi siempre te va a estar rondando estos valores. Entonces, aquí un poco más de lo mismo. Depende mucho del contexto, de la gravedad y sobre todo a quién se lo vayáis a enviar. Yo a mi director necesito darle las cosas súper bien, entonces yo si veo que es menos de un 80% sigo trabajando para que lo sea, ¿vale? Pero, por ejemplo, cuando trabajo para mí y simplemente quiero ver si se pudiese hacer con una regresión logística, ¿qué hago? Directamente pues empiezo, si me sale alrededor de un 70% le doy una oportunidad al modelo porque digo, ¡ostras! pues a lo mejor el modelo tengo que configurarlo, pero a lo mejor puede llegar a ser un buen modelo, ¿vale? Y esto lo que te dice es un poco la seguridad, es como el límite inferior y el límite superior porque no siempre va a ser un valor exacto. Entonces te dice, oye, yo te he dicho que es una precisión del 0,81, pero bueno, sí que es cierto que a lo mejor en estos redondeos he podido ir a 0,75 o a 0,86 por este tema de las probabilidades de los redondeos a 0, ¿vale? ¿Más claro? casi siempre es la matriz de confusión, saber identificar dónde están los aciertos y dónde están los fallos, la métrica de la cura sí, y yo sí que siempre pongo mucho en valor la métrica de la sensibilidad y la especificidad. Porque es lo que os digo, en el test COVID la cura sí no era muy buena y la sensitividad no era muy buena. Pero ¿por qué se utilizaba? Porque lo que interesaba era decir que predijese bien los valores negativos. Vale, ¿cómo se calcula la especificidad? Coges las veces que ha acertado en principio, en este caso el 1, y lo divides entre las veces que lo ha intentado. Es muy parecido. Existe luego otra métrica que es el recall, que si queréis otro día lo vemos, que también en el mercado anglosajón, más que la especificidad se suele utilizar también una cosa que se llama recall. Entonces el siguiente día si queréis también os digo cómo se calcula el recall porque esa no la vais a tener aquí, tenéis que hacer como otra cosa aparte o calcular la mano. Pero sí que es cierto que en Estados Unidos se habla muchísimo más de recall que de sensitividad o especificidad. A mí me gusta bastante como matemática y creo que se utiliza bastante la sensibilidad y la especificidad porque es lo que te digo, te dice cuántas veces o cómo de bien te predice los negativos y cómo de bien te predice los positivos, ¿vale? Entonces, bueno, es haciendo la división entre las veces que ha acertado y entre las veces que era negativo, ¿vale? Y después, el tema, dime Irina, que creo que querías decirme algo. Sí, por favor, cuando va a subir este archivo, incluir el cálculo tanto para sensibilidad como especificidad, por favor. Vale, sin problema, lo supongo. Igual que como lo puso para accuracy. Vale, perfecto, os lo pongo de este estilo, ¿no? Sí, sí, sí, de este mismo estilo, para tener un poco... que es cierto que si alguien luego quiere dedicarse a empresas que tengan que ver con eeuu el récord se utiliza muchísimo vale otra cosa respecto a plata nada así para eso para eso estamos vale respecto a plataformas que podéis utilizar ya no sólo para el estudio yo os las digo ya de manera general para vuestro día a día vale para mí es la mejor herramienta de acuerdo de hecho de hecho para mí es un básico aquí veis que yo para poder recomendaros siempre pruebo algunas cosas, más o menos estaban perfectas y es la herramienta Kaggle, tenéis que haceros una cuenta es una herramienta completamente gratuita y por qué digo que está bien porque el tema es que tenéis competiciones que son pagadas y si las resolvéis bien ya veis que podéis tener bastante dinero sí que es cierto que pensad que esta plataforma hay gente de todo el mundo, de hecho surge hay muchas empresas que ponen sus ejercicios en esta plataforma, ¿vale? Porque como saben que hay tanta gente resolviéndolo, pues en lugar de tener a una única persona resolviéndolo, tienen a lo mejor a millones de personas intentando resolver el ejercicio, siempre con datos anónimos, ¿vale? Pero que aquí tenéis bastantes opciones, ¿vale? De competiciones. Después tenéis lo que para mí es muy interesante de Kaggle que es que tenéis un montón de posibilidades de datasets, ¿vale? Tenéis prácticamente todo lo que queráis. Para un trabajo final de grado no me lo utilicéis porque no tiene, o sea, al final son personas que suben los datos, por lo tanto, no tienen, digamos, esa calidad que si es de una oficial, ¿vale? Pero bueno, aquí, por ejemplo, decís, pues, por ejemplo, quiero saber los préstamos de libros que ha tenido tu bus, ¿vale? Pues aquí iríais a la parte de abajo. os cuento un poco todo lo relacionado con el dataset y podéis descargarlos siempre y cuando tengáis cuenta. Si no lo podéis, no me lo quitéis aquí porque si hacéis este download os hace cosas muy raras. Solo necesitáis esta parte de aquí. Pero sirve para buscar datasets y también podéis tener ejemplos de códigos que a lo mejor os interese. Podéis tener aquí, por ejemplo, muchas veces o la mayoría son con Python pero si buscáis y ponéis aquí que queréis con R os saca también de R ¿vale? y luego tiene la parte del Learn, sí que es cierto que Learn, que si os dais cuenta algunos los empezó a hacer para poder recomendar a los alumnos, el Learn es mucho relacionado con sobre todo Python ¿vale? no tiene tanto de RStudio pero al final lo mismo que se hace con Python lo podéis hacer vosotros con con RStudio ¿vale? Entonces, bueno, yo os lo recomiendo. Aquí tenéis la guía de R. Entonces, aquí tenéis tutoriales. Empezando con R. ¿Cómo hacer gráficos con R? El tema de interactivo. Utilizar, bueno, estos son regresiones un poco más avanzadas que en todo caso se ven en el siguiente módulo. pero tenéis aquí también por ejemplo videos tutoriales, de acuerdo, en este caso pues de TensorFlow hay de todos los niveles, desde el nivel más básico hasta el nivel más avanzado entonces a mí me parece que es una página la verdad bastante bien, bueno, tenéis cosas vuestras, etcétera pero es una página que si vosotros queréis hacer cualquier cosa, para mí es la mejor página para aprender Data Science de manera más o menos autónoma o encontrar ejercicios, mirar cosas, por ejemplo, a ver si os lo puedo enseñar, pero por ejemplo imaginemos que os interesan mucho las series temporales y queréis hacerlo en R. Yo personalmente soy más fan de Python que de R, pero bueno, queréis hacerlo en R. Pues decís, bueno, time series en R y tenéis, vale, pues libros sobre R o tutoriales de R. Tenéis ejercicios de predicción con Machine Learning, pues lo tenéis aquí, por ejemplo. aquí tenéis el fichero que podéis explicar aquí y tenéis aquí el caso no lo he cumplido muy bien, que según lo he visto esto es Python efectivamente esto es Python pero algo que tenéis de R es simplemente que cuido el que no debía pero Time Series Forecasting en R si vais aquí y vais aquí a input tenéis los datos y si le dais a nocturne tendréis el código si que os digo que por favor no se abobíéis cuando veáis códigos de este estilo porque son códigos ya muy profesionales es decir, de gente que ha estado trabajando muchísimos años. Y aquí al final os estamos dando la base para sembrar esa semillita de también saber cómo utilizar RStudio y luego ser capaces de ir cada vez mejorando las cosas que hacéis. Pero bueno, fijaos que utiliza librerías más diferentes, también lo podéis hacer con las vuestras, pero os puede venir bien. Ya no de cara a la asignatura, que veréis que al final todo es muy parecido a lo que hacemos en clase, por no decir que igual, tanto el examen como el SPAC, sino ya directamente para vuestro diario si queréis continuar, tenéis esta plataforma que ahora está muy bien. Existen otras, pero bueno, aquí está muy bien y te las suelen investigar. Otra cosa que también os recomiendo, que también me gusta mucho, no es tan interactiva como esta de aquí, pero son dos hace mucho que no entro pero era a a a momento porque generalmente las utilizo en determinadas ocasiones cuando tengo dudas entonces esta que es Micio esta página que es Micio que básicamente es un montón de artículos de investigación que es cierto que algunos como este por ejemplo necesitas una cuenta pero hay otros que no vas a necesitar hay algunos que sí que son como muy de pago y otros que no son de muy de pago es también tenéis un montón de guías de cómo podéis empezar a hacer las cosas o mirar si le interesa analizar esto es el vídeo y te solía salir antes antes al menos entre las áreas como tal porque me tiene un poco de todo y luego hay uno de data science momento lo que pasa es que actualmente hay mucho más para para Python que para RStudio, eso sí que os lo diré a ver, pues esa otra página yo creo que la han quitado se habrá fusionado porque al final eran prácticamente iguales hasta los mismos colores, o sea que me imagino que al final se habrán fusionado pero Medium es muy buena también, ¿vale? Porque nos explica todo, os analiza todo y ya vais a conclusiones. Entonces, respondiendo a tu pregunta Olga, Kaggle y Medium. Kaggle es mucho más práctica, ¿vale? Tenéis un montón de datasets. Medium es más una especie de guía si tenéis alguna duda concreta. Os va a salir casi siempre Medium, ¿vale? Cuando lo busquéis por internet. Me ha quedado por recomendar, dime ¿cómo utilizar el modelo encontrado? Supongamos que tenemos una mujer de 45 años y compro pasaje de clase 1 ¿vale? pues podéis hacerlo en una predicción generamos aquí una lista ¿vale? en este caso tendríamos una tenemos una lista primero tenemos que asegurarnos como lo has hecho si también tenéis códigos por ejemplo en esa parte ¿vale? tenemos en este caso que estamos seleccionando la edad, el sexo y el tipo de pasaje, entonces se haría exactamente con el predict y aquí pondréis una lista de los valores que tenéis en este caso sería la mujer que lo representas con la F de female pondrías los 45, pondrías el 1 lo ejecutarías y a priori te debería salir también os lo añado en el código varios ejemplos me ha quedado enseñaros hacer el rm, el random forest que al final es haciendo una library que se llama tree y una línea exactamente igual de esta, pero con una cosa diferente. No es GLM, sino que es otra palabra, pero al final es lo mismo. Pero he querido sobre todo incidir en la matriz de confusión por varios motivos. Porque si utilicéis el Random Forest o utilicéis este, la matriz de confusión la vais a utilizar en ambos. Entonces el próximo día repasamos la parte de la matriz de confusión, el predict y hacemos la parte de esto. y lo que me has comentado Irina del modelo que me habías comentado que te interesaba espacial del dataset ¿vale? de la teoría ¿sí? y utilízame el Neomid, por favor ¿recuerda? porque sí que es cierto que en teoría debéis utilizar este y este debería funcionar perfecto pero este sé que funciona de seguro ¿vale? y este otro el día de hoy me ha dado errores entonces casi prefiero que me utilicéis este que en realidad es el que más se utiliza porque es el más semejante a como hablamos. Y hablaré yo con los profesores correctores para que utilicéis ese y no tengáis problema. Sí, exacto. El LM es regresión lineal y GLM es regresión logística, efectivamente. Y no se utilizan para lo mismo. LM, que es regresión lineal, predicción numérica. GLM, que es regresión logística, aunque puedes hacer predicción numérica también, porque al final es una regresión. GLM casi siempre, por no decirte que el 90% o 99% de las ocasiones se utiliza para clasificar. Ojo, cuando solo hay dos posibles opciones, o 0 o 1, como en este caso que tenemos el caso de Survive, o 0 o 1. O por ejemplo que tengas sí y no, o rojo y negro, pero tienes que tener solo dos posibles opciones para la regresión logística. ¿Sí? Hacen cosas distintas. Aunque el nombre sea regresión los dos, en realidad en Machine Learning la regresión logística no se utiliza para predecir números, sino categorías. ¿Sí? ¿Queda claro? Es un poco confuso el nombre, soy consciente, pero no lo puse yo, lamentablemente. ¿Queda claro más o menos, Víctor? Vale, entonces el próximo día para eso estamos, para que preguntéis todo lo que podáis, el próximo día tratamos el tema que me has comentado Irina del modelo espacial que hoy no nos ha dado tiempo y hacemos el Random Forest de momento mi la programación y vamos bien así que probablemente la semana que viene también sea entera práctica quiero daros bastante práctica estas semanas porque aparte de temario, estamos todavía en la parte del temario que os expliqué el otro día, que fue el aprendizaje supervisado. El siguiente temario ya sería el no supervisado. Explicaremos el temario en una clase y comenzaremos a hacer algoritmos de clase y reducción de dimensiones en otra clase. Pero para tener también clases más prácticas, que podéis hacerme todas las preguntas que me estáis haciendo ahora. Así que espero que la clase os haya gustado, una clase un tanto diferente, pero espero que la clase os haya gustado y nos vemos el próximo día ¿de acuerdo? Muchísimas gracias Adiós
