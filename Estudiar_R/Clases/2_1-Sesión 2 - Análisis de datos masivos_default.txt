Hola, buenos dias, buenas tardes, muchisimas gracias por asistir a esta sesión en directo, tambien muchas gracias por aquellos que la veais despues en grabacion. Que vamos a ver hoy, como va a estar estructurada la clase. Primero veremos un poco de la teoria y por ultimo practicaremos la teoria que veamos, la practicaremos en lo que seria el reestudio, de acuerdo? Entonces, en este sentido, buen dia. Os voy a compartir pantalla para poder empezar con la presentación. Como siempre os digo, para empezar he añadido en la carpeta de documentación, tenéis el archivo de datos que vamos a utilizar hoy, que sería el archivo de datos de casas en Londres. Será el archivo que utilizaremos de cara a la práctica en RStudio. De nuevo, cualquier duda que tengáis, cualquier cuestión que queráis comentar, podéis levantar la mano e intervenir en la sesión sin ningún tipo de problema, podéis ponerlo durante el chat, veréis que en muchas ocasiones miro hacia este lado, es porque tengo aquí vuestro chat y vuestras imágenes por si acaso alguien quisiera comentar algo, ¿de acuerdo? Dicho esto, vamos a empezar con la parte que sería el tema 2, que sería cómo podemos extraer, preparar y almacenar los datos, ¿de acuerdo? Entonces, ¿cuáles son los objetivos de este tema que vamos a ver? Pues conocer diversas fuentes de datos y qué tipos de archivos de almacenamiento existen para luego poder extraerlos. En siguiente lugar, tendríamos que conocer todas las alternativas que podemos tener para validar, transformar o reducir los datos. también conocer qué recursos tenemos disponibles para almacenar los datos si los vamos a almacenar en una base de datos, si vamos a almacenarlos a lo mejor en un archivo Excel, etc. Por último lo que sería extraer estos datos de alguna fuente para ser capaces de procesarlos, almacenarlos lo cual sería la tarea inicial para análisis de datos. Esta parte de extraer los datos de alguna fuente, sobre todo cuando son Excel o archivos CSV es lo que vamos a ver cuando hagamos la parte práctica. Entonces, uno de los casos más concretos y más difíciles que tenemos es el tema de las fuentes de datos. Cuando nosotros hacemos una serie de tareas, es cierto que las fuentes de datos son muy diferentes. Podemos tener fuentes de datos como podrían ser vídeos, imágenes, películas, que extraemos información de ellas. podemos tener fuentes de datos que vienen de archivos Excel, de inventarios, de tablas podríamos tener fuentes de datos que vengan de páginas web entonces al final tenemos diversas fuentes de datos y no solo tenemos diversas fuentes de datos sino que tenemos datos muy diferentes y con orígenes muy distintos entonces en este sentido interpretar bien una fuente de datos y ser capaz de analizar la fiabilidad de la misma es uno de los principales problemas que tenemos a la hora de hacer análisis masivo de datos. Por lo tanto, a la hora de plantearnos un ejercicio de análisis masivo de datos o cualquier ejercicio que involucre datos, necesitamos entender muy bien qué quiero resolver, es decir, cuál es mi problema, cuál es el objetivo de mi problema y por lo tanto, para cumplir ese problema, qué datos voy a necesitar. no porque utilicemos muchas más columnas y muchos más datos el modelo va a ser mejor hay que hacer un previo análisis de qué columnas o qué tipo de datos serían los más adecuados en nuestro caso ¿tenemos alternativas para su extracción? sí, existen muchísimas, dependen mucho del tipo de fuente de datos si tenemos imágenes utilizaremos técnicas de reconocimiento de imágenes Si tenemos textos o tenemos archivos Excel, pues vamos a utilizar técnicas de datasets como pueden ser dataframes, que es lo que utilizaremos a lo largo de esta clase. Pero ocurre, por ejemplo, si tendríamos información de páginas web, pues en ese caso utilizaríamos otros métodos de extracción que se llama el web scrapping. Entonces, existen diversas técnicas de extracción encaminadas a diversos tipos de datos. cuando hemos obtenido estos datos y hemos decidido qué tipo de datos queremos, qué vamos a hacer, pues hay que preparar estos datos, es decir, ¿tiene valores nulos? Sí, no, elimino en caso de que los tenga o los reemplazo, ¿tiene valores duplicados? Sí, no, pues en caso de que los haya, tiene sentido que los haya, no tiene sentido, entonces ahí iríamos preparando estos datos tratándolos, por ejemplo, para la mayoría de algoritmos que veremos no podemos utilizar variables que son de tipo texto, entonces tendremos que pasar estas variables de tipo texto a variables que sean numéricas. O por ejemplo queremos modificarlos, yo veo un error, un gran error, consigo el dato correcto, voy a modificar y solventar el error que tengo. por último los hemos extraído, los hemos preparado los hemos tratado, los hemos modificado estamos a gusto con cómo tenemos los datos, pues pasaríamos a almacenarlos ¿almacenarlos para qué? por ejemplo, imaginemos que tenemos un modelo de inteligencia artificial y viene un organismo regulador o la agencia, por ejemplo la agencia de inteligencia artificial europea ¿qué suele pedir? suele pedir saber con qué datos hicimos los modelos, por lo tanto lo bueno es primero almacenarlos y luego ya los utilizaremos para poder aplicar el modelo que sea necesario, ¿vale? Dentro de lo que sería la recolección de datos, ¿vale? Os he puesto más teoría aquí porque al final os voy a subir también este archivo y así creo que tenéis también más información ¿de acuerdo? que puede ser útil, entonces cuando hagáis por ejemplo trabajos final de grado, trabajos final de máster, ¿de acuerdo? la información de una diapositiva tiene que ser mucho menos que la que os doy aquí, ¿de acuerdo? Lo que pasa que, porque se llaman las presentaciones minimalistas, lo que pasa que de cara a luego poder ser capaces de tener esta información en texto y poder estudiarla, pues en este caso sí que os damos bastante bueno, con bastante texto las diapositivas ¿de acuerdo? Entonces ¿qué super etapas existen dentro de la recolección de datos? Porque la recolección de datos como etapa es bastante grande, ¿vale? Entonces al final es propiamente dicho la extracción de los datos desde qué datos voy a necesitar qué fuente de datos tengo, es decir dónde los voy a encontrar y qué tipos de archivo tenemos o utilizaremos, ¿de acuerdo? Después sería la validación de los mismos, es decir, ¿son correctos? ¿no son correctos? ¿tengo valores nulos? ¿tengo valores duplicados? ¿qué realmente mi dato si lo podría utilizar ya o si tengo que hacer alguna cosa para mejorar, ¿vale? En el caso de que tengamos algún problema con estos datos o necesitemos por ejemplo no tener variables tipo texto y tenerlas por ejemplo tipo número, pues estaríamos hablando de la transformación de los datos que sería la siguiente etapa. y cuando tenemos un conjunto muy grande imaginemos que yo tengo una tabla de más de 2000 columnas, claro no voy a utilizar las 2000 columnas para hacer cualquier tipo de algoritmo porque son demasiados datos y al final podemos llegar a confusión al modelo. Entonces existen algoritmos que me permiten reducir estos datos, es decir, bueno pues de estos 2000 datos lo voy a dejar en estas 50 columnas que yo sé que son las más importantes, por ejemplo, ¿vale? Y aplicar el algoritmo sobre esas 50 columnas que me ha dicho que mediante el ordenador he podido conseguir en lugar de aplicarlo sobre las 2.000 casos. Por último, cuando hemos hecho estas reducciones de datos, ¿qué faltaría? Pues almacenarlos, ¿de acuerdo? De nuevo, si hay algún tipo de problema o alguna cuestión, por favor comentádmela, ¿vale? Entonces, al final, los datos no es más que realmente donde se sustenta todo el proyecto de inteligencia de negocio. Si el dato es incorrecto, da igual que mi modelo sea perfecto, da igual lo bueno que sea mi modelo, que si el dato no es correcto, mi modelo de inteligencia de negocio tampoco lo va a ser. Es muy importante. De los datos, ¿de los datos que tenemos? Bueno, pues cuando nosotros tratamos esos datos, obtenemos una cierta información. Por ejemplo, si trato los datos de temperaturas, ¿qué puedo tener? Puedo tener información de cómo ha aumentado la temperatura global a lo largo de estos años. Y esta información nos proporciona conocimiento, es decir, de esta información yo puedo tener una información de unos datos que sea interesante y puedo tener una información de los mismos datos que no me sea interesante para mi problema. Entonces, cuando nosotros utilizamos la información, le damos una vuelta, pensamos el por qué ha podido pasar eso, por qué me está dando esta información, acabamos adquiriendo un conocimiento. Por ejemplo, un caso que lo vais a entender muy fácilmente. Si nosotros vamos a la época de la pandemia en 2020 y miramos los datos de, por ejemplo, los hoteles, nos vamos a ver que en comparación con años anteriores los hoteles ya tuvieron muchas pérdidas. Esta sería la información de los datos que son las reservas de los hoteles. Llegamos a la información de que los hoteles han tenido menos pérdidas. ahora yo no me quiero quedar solo con perdón, menos reservas, no me quiero quedar solo con la información de han habido menos reservas sino el por qué han habido menos reservas y aquí entraría el conocimiento pues porque estábamos en una época de pandemia por lo tanto y repetimos aquí, el tú saber qué quieres obtener que al final es un poco el objetivo, es decir qué quieres analizar, qué información vamos a querer obtener de un gráfico dice uno bueno, esta última parte es lo que os comentaba si los datos son erróneos es decir, si no tenemos un buen conjunto de datos, da igual que mi modelo sea perfecto, que haya funcionado siempre bien que me hayan dado un premio Nobel por mi modelo Si los datos son incorrectos, el resultado del modelo será incorrecto. Aquí, por ejemplo, ¿qué podemos pensar? Podemos pensar, por ejemplo, si hacemos una votación para sacar el delegado de la clase. Me lo invento. Y cambiamos las papeletas. Da igual que yo tenga un modelo muy bueno porque al final lo que va a pasar es que el cambio de las papeletas hace que a lo mejor no salga la persona que iba a salir. Entonces este es un ejemplo que podéis tenerlo para el tema de que si los datos son incorrectos el modelo también lo será. Por lo tanto, es lo mismo que hemos comentado antes que necesitamos saber las fuentes de información y generalmente hay tres fuentes, bueno hay bastantes más fuentes, pero las que vamos a utilizar relacionadas con archivos que se puedan almacenar en un archivo Excel, en una base de datos, ¿cuáles van a ser? Pues al final, el tener los datos de manera manual. Por ejemplo, todos aquellos trabajos que se corresponden a hacer encuestas y nos van preguntando, pues, ¿nos gusta esto o no nos gusta? Decimos, uno o dos. ¿Vamos a votar a este partido político o no lo vamos a votar? Uno o dos. O lo que queráis, ¿de acuerdo? Estos serían casos de captura manual. No los sacamos de ningún sitio, sino que hay una serie de personas que los están recopilando de manera manual. el segundo punto sería el procesado de documentos por ejemplo estamos haciendo un análisis de los inventarios las materias puras que va comprando mi empresa, por ejemplo las materias primas y esto no voy a tener un inventario en un Excel sino que a lo mejor lo que tengo son diversas facturas bueno pues si yo quiero extraer información de esas facturas como cuánto material he comprado cuánto ha sido el precio, cuánto han sido los impuestos pues los podremos hacer mediante este procesado de documentos. Procesaríamos el documento, extraeríamos la información y lo almacenaríamos en formato tabla. Por último, tendríamos las salidas de aplicaciones. Imaginemos, por ejemplo, que estamos hablando de un banco y ese banco tiene aplicaciones. La propia interfaz que utilizamos normalmente para comunicarnos con el banco vía online no deja de ser una aplicación. Esa aplicación genera datos. cuando nosotros hacemos transacciones estamos generando datos porque al final el dato es que esa persona quiere hacer una transacción por ese dinero etcétera ¿vale? entonces es una de las salidas más utilizadas porque toda esa información queda recogida normalmente en una tabla en la base de datos ¿vale? bueno de la captura manual lo que os he comentado el término más tradicional y más frecuente generalmente son las encuestas ¿vale? El proceso de documentos, lo que os he comentado, tenemos por ejemplo facturas, tendríamos por ejemplo imágenes, extraemos información de las imágenes, vamos a ciertas páginas web, que esto sería el tema de las web scrapping que os lo he comentado al inicio, o por ejemplo simplemente me doy cuenta que ha habido una actividad delictiva en una página web y yo quiero adivinar quién es. ¿Qué vamos a hacer? Vamos a buscar los logs. Cuando hablamos del término logs, ¿vale? En todo lo que sería informática, generalmente significa que esa persona o digamos que son que todas las acciones de una persona quedan recogidas en un sitio. Quedan recogidos siempre que una persona haga clic o rellene cualquier cosa, eso queda siempre en un archivo que normalmente, evidentemente, los clientes no ven, ¿de acuerdo? Entonces, cuando pasa algo, cuando hay un tipo de error, hay un problema, pues se pueden revisar estos logs para intentar entender cuál ha sido el problema. Y por último tendríamos las salidas de aplicaciones, que es un poco lo que os comentaba. ¿vale? de acuerdo, después también hay que distinguir mucho ¿vale? entre de quién viene el dato ¿vale? cuando queremos hacer tipo de proyectos de almacenamiento de datos masivos cuando estamos trabajando en empresas privadas ¿vale? si estamos trabajando en la empresa privada y esto lo digo también de cara a cuando hagáis pues trabajos final de máster si estamos trabajando en empresa privada asegurémonos ¿de acuerdo? bueno y bien pública también pero sobre todo asegurémonos de que no utilizamos datos que sean confidenciales. ¿De acuerdo? En el caso de que tengamos datos confidenciales existen técnicas para anonimizarnos. ¿De acuerdo? De forma que no habría problema. Pero bueno, que tengamos esto en cuenta. Entonces, cuando hablamos de empresas privadas pues al final solo podemos acceder a estos datos si estamos dentro de la empresa, lógicamente. Cuando hablamos de entidades públicas depende. Hay ciertos datos que nos dejan ver y hay ciertos datos que no nos van a dejar ver. En cuanto a entidad pública, valoremos la fiabilidad de estas empresas. Un Instituto Nacional de Estadística, por ejemplo en España, en teoría debería tener datos fiables. Creo que en México tenéis una página que se llama Datos Abiertos del Gobierno de México. A ver si en principio vienen del Gobierno México, en principio deberían ser fiables. Y me imagino que todos los países tendréis o vuestro Instituto Nacional de Estadística, que se llamará de la forma que se llame, que publicará datos, o bien plataformas como los datos abiertos del gobierno de México, también existen datos abiertos del gobierno de España, donde podréis tener la información más fiable. Sí que es importante que existan otras cosas, a lo largo de estas sesiones os enseñaré plataformas como pueden ser Kaggle, que es bastante útil, en Kaggle tenemos diversos datasets, pero la fiabilidad no es la mejor. ¿Por qué digo que la fiabilidad no es la mejor? Bueno, pues la fiabilidad no es la mejor porque al final el dataset lo sube una persona y no una empresa, con lo cual no podemos decir que a lo mejor la persona no haya falsificado o no haya cambiado algún dato. Entonces, es una plataforma que os enseñaré, es súper útil cuando queréis empezar a utilizar este tipo de programas. Aparte también creo que los datasets son tan interesantes y en general están bien, por lo tanto, pues si os interesa una temática concreta, podríais hacer un análisis perfectamente, ¿vale? Pero no perdamos de vista que de cara a un trabajo final de máster o de cara a un trabajo más serio, intentemos siempre fiarnos por entidades públicas o por empresas privadas. ¿de acuerdo? uno de los grandes problemas que tenemos es que necesitamos unos datos de un estilo y otros datos de otro estilo por ejemplo, nosotros podemos estar en una empresa y querer los datos de los empleados, ¿vale? y estos datos de los empleados a lo mejor están en una base de datos almacenada ¿qué ocurre? que a lo mejor yo esos datos de empleados me lo invento, ¿eh? pero quiero que me los relacionen con las distintas oficinas o con las distintas sedes que puede tener nuestra empresa. Y a lo mejor estas sedes ya no están en mi base de datos, sino que a lo mejor estas sedes están en un archivo Excel. Y además quiero a lo mejor mezclarlo con las reseñas de Google respectivas a esta sede. Y estas reseñas de Google ya no es un archivo Excel, ya no está en base de datos, sino que ahora nos encontramos con un web scrapping porque es una información que tenemos de la web, en este caso de Google Maps. Con lo cual, tenemos por un lado datos de una base de datos, por otro lado datos de un Excel y por otro lado datos de la web. Claro, ¿cómo gestionamos todo esto? Lo que solemos hacer es, a través de lenguajes de programación, como pueda ser Python, como pueda ser RStudio, los pasamos todos al mismo formato. o bien utilizamos lo que se llaman ficheros planos, es decir, intentamos quitarle o guardarlos todos en el mismo tipo de fichero. Cuidado cuando utilicéis Excel porque Excel a partir de millón y medio de filas empieza a dar problemas. Entonces, por ejemplo, cuando tenemos archivos tan grandes, yo lo intentaría hacer más con RStudio, más con Python, más que quizá con Excel. Excel, ¿vale? Porque a partir de determinada cantidad, Excel la verdad es que falla bastante, ¿vale? Por lo tanto, ¿qué tipo de ficheros solemos utilizar? Pues solemos utilizar los que tenemos aquí, ¿vale? Los CSV, los JSON y los XML, ¿vale? CSV y XML se pueden utilizar sin problema con el guardar como de Excel utilizamos un guardar como en Excel y nos da la posibilidad de XML y CSV XML es más parecido a Excel, de hecho la estructura es prácticamente igual pero cuando hablamos de CSV en lugar de tener distintas columnas lo que hace el CSV es que une las distintas columnas separando los valores por una columna Es decir, voy a tener una de tres columnas, tendré una única columna y las celdas de esa columna se corresponden en valor de la celda primera, valor de la celda segunda, valor de la celda tercera. A lo largo de la clase y de la asignatura veremos ejemplos de archivos CSV. Cuando hablamos de JSON, no estoy segura de si vamos a necesitar JSON a día de hoy. JSON tiene una estructura un poco más rara. A ver si consigo mostraroslo. Y eso lo que haríamos es que imaginemos que tenemos el archivo que es, perdonad un momento, para que lo veáis. Creo que esto es interesante, al menos que lo sepáis. un archivo Excel, yo puedo tener la columna A, la columna B y la columna C, por ejemplo, ¿vale? Y aquí puedo tener el valor 1, 2, 3, aquí puedo tener el valor A minúscula, B minúscula y C y aquí puedo tener, por ejemplo, el valor Z, Y y X, ¿vale? Esto sería un archivo Excel normal. Cuando pasamos a un archivo CSV, lo que yo veré será A, B, C, siguiente fila 1, 1, a, z, siguiente fila 2, b, y, cuarta fila c, 3, c, x, este sería el archivo CSV. Cuando hablamos de un archivo JSON es un poco diferente porque lo que hacemos es abrir como una llave, pondríamos en este caso la columna A, luego pondríamos dos puntos y volveríamos a poner llave. Es un formato un poco distinto. No creo que lo veamos en la clase, pero ¿para qué se utiliza? Se utiliza muchísimo cuando estamos haciendo web scrapping. existe un lenguaje de programación aparte de Python y DR que se llama JavaScript que utiliza bastante los JSON entonces tenemos los ficheros de comma separated value que al final son los más comunes luego tenemos si me deja avanzar en la diapositiva lo agradeceré los ficheros JSON que se suelen utilizar cuando utilicemos web scrapping, en principio creo que no está pensado esta asignatura para que hagamos web scrapping pero bueno si nos diese tiempo pues haríamos un poquito y veríais un ejemplo de un fichero JSON y luego tendríamos los XML que serían los más detallados y cada vez se utilizan más, a día de hoy lo que más se utilizan son CSVs o directamente los datos en base de datos, pero sí que es cierto que los XML empiezan a utilizarse cada vez más, aquí tenéis los ejemplos por ejemplo del formato CSV del forchero JSON y del fichero xml, al final es prácticamente parecido lo único que uno es más estilo lenguaje de página web como sería xml otro sería más lenguaje de programación como sería el json, en programación lo que más utiliza es el fichero json y luego tendríamos el formato csv que suele utilizarse bastante y sobre todo cuando no estamos hablando de programación puramente dicha, suele ser el que más se utiliza Entonces, ¿cómo podemos saber la calidad de los datos? Es decir, realmente, como hemos comentado, si los datos son incorrectos, ¿cómo podemos ver que los datos son correctos? ¿Cómo podemos hacer una métrica de esto? Pues al final hay que probar a definir cinco propiedades del conjunto de datos. La primera sería la completitud. Os digo todas y luego os las explico. La segunda, la credibilidad, es decir, la fiabilidad. Después, la precisión, la consistencia y la interpretabilidad. ¿Qué significa la completitud? Que no tengamos valores nulos, es decir, que todos los valores estén informados, que no tengamos valores nulos y en caso de que los tengamos, que sepamos cómo modificarlos, que sepamos por qué han pasado y podamos recuperar el dato. ¿qué significa la credibilidad? bueno, va muy enlazado con el tema también de qué fuente tengamos evidentemente datos por ejemplo que vengan del gobierno de España sobre la situación de España a priori me las debería creer mucho más que si pregunto a alguien por la calle un dato, entonces tendríamos que esta sería la credibilidad siempre intentar ir a organismos oficiales la precisión, pues que no haya errores, que no tengamos datos, por ejemplo, muchas veces pasa que cuando tenemos un error, pues estamos calculando un importe y me sale un valor menos uno. ¿Qué es ese menos uno? El menos uno no es que yo haya comprado algo por menos uno, es que ha habido algún error que hace que haya este menos uno. ¿Vale? Esto es un problema porque sí que puede haber un importe menos uno, con lo cual, encontrar el tema de la precisión yo creo que es de las cosas más difíciles que hay. Consistencia. Si yo tengo el mismo dato en dos archivos que no sea contradictorio. Es decir, por ejemplo, si yo tengo el dato en la segmentación de clientes y a mí me sale que yo estoy casada, por ejemplo, luego no puede ser que en otra tabla yo aparezca como soltera. Es decir, necesito que el dato sea el mismo en todos sitios. E interpretabilidad. Si nosotros tenemos una columna que se llama A, una columna que se llama B y una columna que se llama C y os estoy hablando de un tema de ventas, me vais a decir, bueno, ¿y qué es A, qué es B y qué es C? O sea, realmente, ¿qué significan? Pues no sería interpretable. Cuando tenemos datos y tienen que ser interpretables, significa que o las columnas se interpretan bastante bien o tenemos un manual o una documentación extra para definir esto, ¿vale? Para poder entenderlo. entonces bueno como resumen podemos tener problemas de falta de información los valores nulos de integración de los datos lo que comentaba que no tenemos partimos de diversas fuentes falla algún punto del proceso y por lo tanto tenemos un problema de integración de los datos vale es decir que tenemos errores o tenemos datos que son equívocos vale ruido en los datos es decir básicamente errores o que el dato no sea preciso, inconsistencia de los datos, el mismo dato en dos tablas diferentes toma valores distintos y serían los cuatro principales problemas que podemos tener. Entonces, ¿qué hacemos cuando nos encontramos con estos casos? Pues primero evidentemente intentar encontrarlos, identificarlos y en el caso de que sean un valor nulo lo que vamos a tener es que sustituirlo. Aquí veréis que hay cuatro posibles opciones. Primera opción, quiero utilizar un modelo de datos o quiero hacer una aplicación o quiero hacer un gráfico. Me doy cuenta que hay una columna que tiene por ejemplo 50 nulos, pero yo no quiero representar esa columna. A mí esa columna me da igual. ¿Qué haríamos? Pues no tener en cuenta esa columna y abrir un ticket a las personas responsables de estos datos informándoles de que tienen una columna con 50 datos vacíos. Segunda opción que podemos tener. Volvemos al mismo caso. Tenemos 50 nulos. Yo sí que quiero tener esa columna, pero mi conjunto total de datos es un millón de datos. Bueno, pues sobre un millón de datos, 50 datos no son nada. Puedo eliminar las filas en las cuales este valor tiene nulos. Tercera opción. Tengo 100 datos y 50 de ellos en una columna tienen nulos. Y además quiero quedarme con esa columna. ¿Qué haríamos? Reemplazar estos valores, estos valores nulos, por un número concreto. Este número puede ser un 0, puede ser un 0. Puede ser la media, puede ser la media. puede ser otro valor, puede ser otro valor para esto depende mucho del contexto lo que realizaríamos entonces bueno, estos son los tres pasos que os he comentado que se puede hacer por media por regresión, como queramos lo siguiente que procedan de múltiples fuentes es un poco lo que habíamos comentado también anteriormente que normalmente ocurre, que podemos tener datos repetidos, podemos tener datos secundarios, podemos tener además el caso en que para una tabla un dato sea súper importante y tengamos una fiabilidad tremenda y para otra tabla ese dato sea sin importancia y por tanto no lo controlen tanto, también se da de acuerdo de forma que tenemos el mismo dato, columnas distintas y diferentes, podemos tener datos duplicados, es decir, en la misma tabla podemos tener dos filas, por ejemplo, que sean las mismas e incluso podemos encontrar datos anómalos. Cuando hablamos de datos anómalos se suelen denominar outliers, es la terminología que se utiliza y para que os pongáis una idea, imaginemos que estemos hablando del número de las notas de una persona. Aquí en España las notas de los niños van del número 0 al número 10 y de repente me encuentro que tengo un número 50. Este evidentemente es un dato anómalo. En este caso es claro que es un dato anómalo y que es un error porque no puede haber un número 50. Pero, por ejemplo, si yo estoy hablando de los clientes de una tienda y yo veo que todos los clientes de esa tienda están entre los 10 y los 20 años y de repente me encuentro una persona con 60 años, quiere decir que sea outlier porque evidentemente es atípico, ¿no? Quiere decir que sea un error. No tiene por qué. Puede haber habido una persona de 60 años que de verdad haya ido a la tienda, ¿vale? Entonces los outliers hay que intentar mirarlos con bastante cariño, ¿vale? Para ver si tienen sentido o no. Por último, podemos tener inconsistencia de los datos. Aquí repetiré siempre, ojo con las fechas, ¿vale? Tratar las fechas con lenguaje de programación nos suele dar bastantes quebraderos de cabeza. Así que ojo con las fechas. Nosotros podemos tener en una tabla, ¿vale? Un dato que lo consideramos como un número, ¿vale? Por ejemplo, podemos tener la edad, ¿vale? Y en una tabla la edad la considero como un número, pero puedo tener otra tabla en la que he codificado mal el dato y la edad la considero como un texto. ¿Qué va a ocurrir? Que cuando yo quiera comprobar si la edad de una tabla es la misma que la edad de otra tabla, nunca me va a salir que sea igual. ¿Por qué? Porque en un sitio la edad la estaba considerando como un número y en otro lado la estaba considerando como un texto. Así que cuidado con cómo los codificamos. ¿Hasta aquí todo entendido? Antes de continuar con la transformación. Si veis que también me enrollo mucho, también me lo podéis decir. Básicamente quiero que quede esto claro porque es un poco la base. Ahora, la transformación de los datos. ¿Qué viene siendo cuando nosotros queremos... Gracias. Cuando nosotros queremos crear una columna nueva. porque por ejemplo, tenemos la fecha de nacimiento ¿vale? cuando nosotros tenemos una fecha de nacimiento, sospechad no nos gustan las fechas por norma general, ¿de acuerdo? fechas que son en plan de luego para analizar cómo va cambiando algo en el tiempo sí, esas fechas, ok, ¿vale? pero fechas estilo fecha de nacimiento no, ¿vale? cuando tengáis una fecha de nacimiento para todo tipo de modelos personalmente os recomiendo siempre pasarlo a edad, ¿vale? entonces ¿qué tendríamos que hacer? una columna en la que restásemos la fecha actual menos la fecha de nacimiento ¿vale? de esta forma siempre tendremos un número que son los años y es mucho más fiable ¿vale? así que fechas de nacimiento por favor os lo pido siempre intentad pasármelo a edad ¿vale? más que nada todos los algoritmos cuando son números funcionan siempre mejor y las fechas para que os hagáis una idea ¿vale? Yo puedo tener, y esto pasa en Excel, si yo tengo el Excel en español, ¿qué será la fecha? La fecha será en formato día, mes, año, ¿vale? Si yo le paso este Excel a una persona que tenga el Excel en inglés, tenemos un problema, porque internamente él tiene que cambiar el día, mes, año a mes, día, año, ¿vale? ¿Qué suele ocurrir? Que cuando tenemos que los meses es octubre, noviembre, diciembre, el cambio lo suele hacer bien, pero cuando los meses tienen un dígito, es decir, estamos de enero, febrero, marzo, abril, mayo, junio, julio y agosto, no suele cambiar la fecha. pero internamente él sí entiende entonces lo que te hace es tú no ves el cambio pero internamente el ordenador está cambiando el día por el mes y es un desajuste tremendo, sobre todo cuando tenéis que hacer cierres por ejemplo a 30 de junio esto es un caso que me ha pasado personalmente con Microsoft Access en este caso, yo cerraba a 30 de junio y entonces de repente vi que había cosas de septiembre que se me estaban metiendo, ¿por qué? la fecha ponía 1 de septiembre con lo cual no tenía ningún sentido que me apareciera pero internamente el ordenador le había dado la vuelta internamente yo no lo veía pero lo estaba considerando como el 9 de enero, ¿vale? entonces por eso las fechas tened cuidado os aviso, ¿de acuerdo? más cosas cuando tenemos una columna, por ejemplo que estamos hablando de de cuántos niños tiene una familia ¿vale? que podemos decir puede tener un niño, dos niños, tres niños hombre, más de 10 niños es raro, pero hay casos, ¿vale? Pero bueno, más de 100 niños es imposible, creo, ¿eh? Si alguien los tiene, pues me gustaría, la verdad, saberlo, pero bueno, es algo muy, muy atípico, ¿vale? Entonces suele ser de 1 a 100, pongamos. Pero si yo hablo de ingresos o de sueldos, ya no va a ser de 1 a 100, sino que va a ser a lo mejor de, me lo invento, ¿eh? Pero 20.000 a 100.000, ¿vale? ¿Qué puede ocurrir? Puede ocurrir, y lo que no queremos que ocurra, es que como la variable de sueldo va de 20.000 a 100.000 y la variable de niños va de 1 a 100, el algoritmo interprete que es más importante la variable de 20.000 a 100.000 que la variable de 1 a 100. Como no queremos que esto pase, muchas veces se procura estandarizarlo. Si habéis estudiado un poco de estadística, la estandarización o normalización que es seleccionamos cada uno de los elementos de cada una de las filas, le restamos la media, lo dividimos por la desviación típica de la columna y al final lo que obtenemos es que todos los números van a estar entre, me lo invento, pero entre menos 2 y 2, pero todas las columnas. Ya no vamos a tener una columna de 20.000 a 100.000 y una columna de 1 a 100, sino que ambas tendrán la misma métrica, entonces las dos serán de menos 2 a 2, por ejemplo. De esta forma estamos consiguiendo que el algoritmo interprete las dos columnas con la misma importancia. existen varias, la minmax, la zscore el escalamiento decimal, nosotros sobre todo utilizaremos la zscore creo que es la más fácil y la que luego es mucho más útil y por último tendríamos la extracción de los atributos de los datos por ejemplo, imaginemos nosotros tenemos unos productos que compramos y yo tengo el nombre del producto. Entonces yo puedo tener yogur de fresa, yogur de melocotón, yogur de plátano, ¿vale? Por ejemplo. ¿Qué ocurre? Que a lo mejor a mí no me interesa tal grado de detalle. Yo quiero saber los que han comprado yogures, pero yo no tengo yogur solo, tengo yogur de plátano, yogur de fresa, yogur de melocotón, no sé cuál dije. Bueno, me habéis entendido, creo, ¿vale? Entonces, ¿qué suele ocurrir? que para cualquier algoritmo el hecho de que se llamen distinto implica que son cosas distintas por lo tanto a veces tenemos que hacer una nueva columna en la cual extraigamos la palabra principal o la categoría principal de esas cosas, entonces en este caso extraeríamos la palabra yogur y tendríamos una nueva columna con yogur, yogur, yogur o carne, carne, carne porque no me interesa saber carne de ternera carne de cerdo, carne de pollo o carne de cordero, ¿vale? necesito saber cuántas personas han comprado carne en general. En realidad, la mayoría de las veces vamos a hacer sobre todo transformación de variables que son texto, es decir, de columnas que almacenan texto a variables que son número. Ya veremos por qué más adelante. ¿De acuerdo? Bueno, como os he comentado aquí, lo que os decía, si tenemos 2.000 columnas no vamos a utilizar las 2.000 columnas. Estaríamos tardando años y tendríamos información que a lo mejor no es relevante. ¿Qué vamos a intentar? Pues encontrar o redecir el número de columnas que utilicemos a las columnas que de verdad sean importantes. ¿Cómo vamos a hacer esto? Pues asegurándonos de que perdemos la menos información posible. Hay algoritmos que nos seleccionan todas estas cosas de manera automática. Si tenemos tiempo lo veremos. Como es una primera fase no sé si nos dará tiempo a verlo, pero si tenemos tiempo me aseguraré de que lo veamos, que esto sea a precisión. Después tendríamos la eficiencia, es decir, cuántas menos columnas mejor, ¿por qué? porque también tardará mucho menos en darme un resultado y la sencillez ¿vale? que viene siendo que todo lo que yo haga se puede interpretar y se puede entender el por qué lo hago ¿vale? de acuerdo, entonces hasta aquí de reducción de datos que luego vamos a ver esto, pero hasta aquí ¿se entiende todo? Sí, una consulta el método ACP es digamos es bueno para la reducción de datos es digamos metodológicamente usado mucho sí, es bastante usado lo que pasa es que suelo utilizar más las islas en inglés, por lo tanto ha habido un momento en que me has dicho CP y me he quedado un poco pero sí, análisis de los componentes principales o principal componente análisis sí, sí, es muy muy utilizado, tienes que entender bien lo que hace, ¿vale? el análisis de componentes principales lo que hace es que de esas 2000 columnas lo que hace es que te lo transforma te lo transforma o sea, te lo proyecta matemáticamente es como que tus columnas es decir, que te diga que tienes dos columnas que son importantes no es que sea tu primera columna y tu segunda columna del dataset es que le haces sumas y restas entre las columnas y llega una combinación en la cual puedes reducir esas 2000 columnas en dos columnas entonces cuando tú haces esto que las puedes reducir porque tienes un indicador que se llama varianza explicada que lo que te dice es justamente lo de la pérdida de información entonces tú te sueles quedar con todas las columnas que sean necesarias hasta que tengas como un 80% o más de la varianza explicada que esto vendría siendo que pierdes un 20% de la información que es útil pero ojo cuidado porque no es que te quedes con tus dos primeras columnas sino que te quedas como con un mix ¿vale? como que hace muchas operaciones entre las 2000 columnas y de ahí saca otras columnas distintas y de esas otras columnas distintas seleccionas dos, o sea hay que entender bien lo que hace el principal componente análisis pero sí, es el más utilizado ¿vale? si puede algún día os lo explicaré la parte de matemáticas, yo soy matemática personalmente y la parte de matemáticas detrás de todo esto es súper interesante ¿vale? pero bueno Sí, sí, sí, sí, es el más utilizado y el mejor. Vale, tengo una pregunta de Irina relacionada con la transformación, si se tiene fecha 30 de junio de 2025, ¿se puede transformar esta fecha en 2025,5? ¿2025,5 no? O sea, sí, pero con operaciones matemáticas, ¿vale? 2025,5 con operaciones matemáticas, sí, te diría, pero normalmente lo que se suele utilizar en el día a día, si lo quieres pasar a número, es poner esto, ¿vale? o 2025 06 30 vale se suele poner así esto para transformarlo suele ser y y y y y m m d d vale es el formato de fecha que se suele utilizar en vez de la típica que es de de m y y y y vale se suele utilizar más está que la de 2025 5 pero si tú de esta de aquí 2025 06 30 que podrías hacer Podrías dividir entre 100 y si te quedas con el cociente, ¿qué tendrías? Pues si te quedarías con el cociente tendríamos 2025, 06 y si te quedases con el resto tendríamos que es el día 30. Si 2025, 06 además lo divides entre 100, ¿qué te quedaría? te quedaría 2025 como cociente que sería el año y de resto te quedaría 0,6. Y como tú sabes que hay 6 meses, ¿qué harías? Harías 0,6, perdón, hay 12 meses. 0,6 entre 12 es lo que te saldría este 0,5. Entonces, ¿qué harías? Harías el cociente que es 2025 más 0,5. No sé si te he resuelto, Irina, la duda. O sea, por fórmula matemática sí podrías, ¿vale? Pero sí que es cierto que al menos en las empresas que yo he estado suele ser más el 2025-06, ¿vale? Nada, ya veremos cómo al final de estas clases sabréis hacer perfectamente todas esas funciones en RStudio, ¿vale? no sé si hay alguna duda más o continuamos de momento entiendo que no hay ninguna duda vale, entonces ¿cómo lo podemos hacer? por muestreo, ¿de acuerdo? es decir, de una población, esto lo haréis mucho, por ejemplo, si no habéis trabajado en auditoría, se hace mucho o sea, nosotros tenemos una población que puede ser mejor, o por ejemplo en política tenemos 100.000 habitantes ¿de acuerdo? o tenemos 47 millones de habitantes, pero yo no voy a entrevistar a esos 47 millones de habitantes ¿qué se hace? Se selecciona un subconjunto de a lo mejor 100.000 que evidentemente otra cosa, tienen que representar bien las diferencias de población, es decir yo no puedo preguntar por ejemplo a todo mujeres ¿de acuerdo? Porque no estaría representando fielmente la población, si mi población tiene un 40% de hombres y un 60% de mujeres, o un 60% de hombres y un 40% de mujeres, o como queráis decirlo, pues ¿qué tendríamos que hacer? Tendríamos que nuestra población mantener esa diferencia de 40-60, ¿vale? Para que fuese completamente fiable. Y con esto digo temas del género, el sexo, temas también de lo que sería el tema de la política, ideologías, religiones, etc. Y a esa pequeña población es donde yo aplico la encuesta, ¿vale? Entonces esta sería la técnica del muestreo. Selección de características. Aquí podemos hacerlo por correlación o como bien comentó, creo que fuiste tú Marco, si no recuerdo mal, por algoritmos como pueden ser el PCA, el principal componente análisis o análisis de los componentes principales, o hay otros como puede ser UMAP, que es Uniform Manifold Approximation y componentes o algo así, que esta es un poquito más compleja matemáticamente, pero que el significado vendría a ser lo mismo. y luego la discretización de los datos pues podemos meterle algún tipo de atributo categórico por último lo que veremos en cuanto a teoría antes de pasarnos ya los últimos minutitos a práctica es que las bases de datos sería lo que tenemos para almacenar y gestionar entonces ¿qué tenemos? ¿en qué se consiste una base de datos? pues evidentemente en los datos sin datos no hay base de datos después en el hardware ¿vale? ¿qué es el hardware? No sé si habéis visto las típicas imágenes donde hay un montón de ordenadores conectados que son con un montón de cables. Esto por ejemplo son los o un supercomputador. Si os interesa todos nuestros datos, luego os pongo una imagen, pero todos nuestros datos están dentro de un montón de cajitas, efectivamente servidores, exactamente Francis. son como una especie de cajitas apiladas llenas de cable y ahí se almacenan los datos todo lo que sea físico que yo pueda tocar, que yo pueda palpar es hardware cuando te damos la palabra hardware por hardware estoy hablando de un ordenador por hardware estoy hablando de una pantalla por hardware estoy hablando de un ratón por hardware estoy hablando de un teclado de un micrófono, de cosas que yo puedo tocar son físicas cuando hablamos de software es lo abstracto lo que está por debajo, el cómo piensa el ordenador Por ejemplo, el Word es software. Los programas que nosotros tenemos para almacenar las bases de datos y comunicarnos con ellos, el programa es software. El Moodle, por ejemplo, el Campus, por ejemplo, de la UNIR, eso es un software. para que entendáis, bueno sería más bien una página web pero si fuese una aplicación sería software para que entendáis el propio Windows o Apple lo que es el sistema operativo también sería software para que entendáis un poco la diferencia de hardware y de software y luego tendríamos el tema de los usuarios podremos tener programadores tendremos luego los jefes o los que administran todo y dan permisos que son los administradores de la base de datos y después los usuarios que son los que van accediendo y extrayendo la información generalmente y a día de hoy las que más se utilizan siguen siendo las que tienen un lenguaje SQL que no veremos aquí pero de verdad que es bastante sencillo son todas aquellas que tienen que ver con tablas aunque cada vez cobra más fuerza algunas que no son contables que serían las MongoDB ¿qué ha ocurrido? que ahora actualmente como tenemos tantos datos solemos hablar de data lake house o data warehouse warehouse en inglés es almacén Por lo tanto, lo que estamos diciendo es almacén de datos que vienen recogidos en esos servidores. Y a día de hoy, además, tenemos todo el tema de los servidores en la nube, que ya otro día os trataré con mucho más detalle, pues todo el tema del cloud, de qué ventajas tiene, qué no ventajas tiene, en qué se basan, etc. Entonces, bueno, ¿qué hemos tratado? Pues cómo extraerlos, qué técnicas son las más utilizadas para validar los datos, transformarlos, reducirlos y almacenarlos. ¿vale? ahora se cayó Azure pues yo Azure no la conocía la que sé que se cayó fue Amazon Web Services Azure no la conozco ¿vale? o sea, sí que la conozco pero que no conocía la caída, me refiero pero bueno, al final sí, sí, o sea, conozco lo que es Azure pero no conocía que se había caído al final, a mi modo de ver, se está dependiendo mucho de otras empresas entonces, ¿qué ocurre? que como hay tantas empresas que compran los servicios de la nube a, en este caso, Google, Amazon y Azure, ¿qué ocurre? que si antes una empresa fallaba o tenía un problema, cae una empresa. Ahora si cae Azure o si cae AWS, no cae una empresa, caen 200. Entonces ahí tiene sus riesgos y tiene sus ventajas. En esto entraremos más adelante. Hasta aquí todo entendido mientras voy preparando RStudio. Entonces, estos últimos 15 minutitos vamos a empezar a entender cómo se cargan los datos, ¿de acuerdo? En el caso de RStudio. Que ya lo vimos la anterior vez un poquito. Y también a cómo podemos empezar a tratar los datos. ¿De acuerdo? Entonces, os voy a compartir pantalla otra vez. ¿Vale? Y vamos a... Sí, en su experiencia, ¿qué tipo de nube ha trabajado más? Google, Microsoft o Amazon. En mi caso, Google Cloud, ¿vale? Es la que más he trabajado. Creo que empezó más rezagada probablemente que Microsoft o Amazon. pero a día de hoy te ofrece unas posibilidades de conectarte con las herramientas de Google que Microsoft y Amazon no tienen es cierto por otro lado que también es cierto que si tú trabajas en Azure y trabajas todo con Microsoft te permite una mayor comodidad a la hora de luego estar trabajando con herramientas de Microsoft Word, Excel, Access PowerPoint, etc. Pero en el caso de Google si os interesa reseñas de Google o si os interesa algún tema sí que es cierto que quizá está empezando a ser un poquito más fuerte, la que más mercado tiene es Amazon, y la que más mercado tiene, te diría, o la que más empresas me he encontrado que trabajan es el Amazon Web Services pero también considero que hay mucha más gente formada en Amazon Web Services que en Google Cloud, entonces aquí ya depende un poco, al final Google es Google, y Amazon es Amazon con lo cual en el fondo si tú sabes trabajar con una prácticamente sabrás trabajar con la otra exactamente igual o sea que sí, el Google Cloud está creciendo bastante la verdad porque al final es Google, o sea la gente dice oye es Google y todavía yo creo que le tienen un poco más confianza a Google que a Amazon aunque bueno, eso es relativo, ¿de acuerdo? pero bueno, vale, para cargar un archivo recordemos, ¿vale? que tenemos que, bueno, ahora os lo vuelvo a repetir voy a cargar el archivo que ya os he subido a vosotros de acuerdo que sería este de aquí vale entonces y las de cinco segundos bueno aquí tenemos que hacíamos importar data set desde excel y dadme un momentito o si no os lo hago en vale mira os voy a poner en ccv de acuerdo para que para que lo veáis la anterior vez vimos cómo hacerlo desde importar y hacer from excel y Y ya teníamos todos los datos. Vamos a hacerlo esta vez un poco distinto. Y vamos a hacerlo por código. El fichero que yo quiero cargar. Es un fichero que es CSV. ¿Qué voy a hacer? Primero. Python no es inteligente. R no es inteligente. ¿Qué quiero decir con que no sea inteligente? Yo soy matemática. Yo soy matemática. Y si a mí me preguntas algo de biología. Y no lo sé. ¿Qué voy a hacer? voy a ir a una biblioteca, voy a seleccionar un libro, voy a leer ese libro y entonces te responderé. Pues lo mismo le pasa a R. R puede hacer muchísimas cosas, pero hay otras que no domina. Entonces tendremos que decirle, selecciona este libro y una vez que hayas leído este libro podrás responder sobre estas preguntas. Entonces, un libro que vamos a utilizar en este caso sería ReadR. Este libro me va a permitir leer, por ejemplo, archivos CSV. Entonces estamos ampliando el conocimiento que tiene en este caso RStudio. Fijaros, yo lo he descargado y ya directamente aquí aparece la frase que yo he ejecutado y me parece ya que estoy en la siguiente línea. Por lo tanto, ha instalado bien. Si alguno tenéis un problema, yo sale en rojo que no lo reconoce para instalar algún tipo de paquete. siempre utilizaremos install packages y en este caso el nombre del libro en este caso readr lo haríamos instalando ¿vale? pero entre comillas ¿vale? cuando ya lo tengáis instalado en el caso de que os haya dado error volvemos a la línea de la librería y lo volvemos a ejecutar ¿vale? esto simplemente pensad que es que R está leyendo un libro para ser mucho más inteligente ¿vale? siguiente habíamos visto las variables las variables, que era creo que lo único que habíamos visto, pero bueno, ¿qué es una variable? Una caja donde guardo información, de forma que cuando quiera acceder a esta información voy a escribir la palabra del nombre de la caja, en este caso vamos a hacer una caja que se llama data y dentro de esta caja vamos a leer el archivo, ¿vale? Como es un archivo CSV, así que haríamos un read CSV. este de aquí arriba tienes que hacer aquí hay un símbolo que te pone un más con un con un una hojita por dentro y le das al reescript confirmame que te sale Jonathan perfecto entonces nosotros le hemos dicho lee un CSV pero evidentemente que tengo que decirle donde está ese CSV vale, donde está ese CSV es un texto, le voy a pasar la dirección de mi CSV, vale, entonces un momento vale, como puedo seleccionar la dirección de mi CSV vale botón derecho de acuerdo, en este caso en nuestro archivo y aquí tenemos la de copiar como ruta de acceso vamos a copiar como ruta de acceso y lo pegamos cuando lo copiéis como ruta de acceso las comillas se os añaden automáticamente solo si lo copiáis como ruta de acceso perfecto, pues veamos a ver si esto funciona en este caso tengo una U esta que me va a molestar entonces como he visto que me da un error es por la U, lo que vamos a hacer es un acceso relativo esto simplemente es para no tener que poner el cusers porque si ponéis cusers os va a dar un error porque la u digamos que esta barra la barra con la u es un carácter protegido o especial y entonces como que se confunde no sabe si quieres poner el carácter protegido o lo otro y lo mismo con la d de downloads entonces lo que voy a hacer es cambiarle la barrita a esta otra barrita a ver si así me lo pasa vale, no lo reconoce un momento, eh creo que esta era la ruta entera porque debería reconocérmelo vale, a ver copio otra vez como ruta de acceso vale, hace dos puntos y lo que he hecho es cambiarla vale A ver ahora. Perfecto. De acuerdo, entonces ahora parece que me ha dicho, oye, tienes 1.359 filas y 7 columnas, ¿vale? Fijaros que hemos cambiado la barrita porque esta barra con la U y con la D son caracteres especiales, ¿vale? entonces yo ya tengo que aquí me ha sacado pues ciertas características están separados por coma tengo carácter tengo números también me dice las columnas que son de números las columnas que tengo de de fechas y las en este caso el área y el código vale que son caracteres pero yo quiero querer ver mi mi data set como tal así que voy a decirle view data vale data a la caja y view lo que va a hacer con la V mayúscula es mostrármelo cuando yo ejecuto esto que me muestra toda la caja que tenía pero me la muestra si os dais cuenta en una pantalla o en una pestañita aparte hasta aquí como vais pregunto vale, bien perfecto, entonces ¿qué puedo querer saber de esta si queréis trabajar con R Commander podéis trabajar no hay problema la parte de subir archivos perfecto, necesitamos que R entienda las cosas hay cosas que R de inicio no lo sabe, entonces vamos a darle una librería, o sea un libro en el cual él puede ver cómo tiene que subir estos archivos en este caso nuestro archivo termina en .csv así que necesitaríamos esta librería que es readR y utilizamos la función readCSV, ¿vale? Cuando nosotros... A ver, el data es el nombre de la variable, efectivamente, ¿vale? Bueno, cuando nosotros dentro, ¿qué vamos a poner? Vamos a poner el fichero, ¿vale? Este fichero lo tenemos en descargas, le damos botón derecho, le damos copiar como ruta de acceso. Una vez que lo hemos hecho como copiar como ruta de acceso, ¿vale? Lo que voy a hacer es estas barritas, para que no me den problema, cambiarlas por las otras barras, ¿vale? En todas las barritas que tengáis. Y así podréis ser capaz de cargarlo. Vale, te apareció esto. A ver un momentito. Vale, si no hay paquete llamado, ¿qué es lo que te pasó a ti? Efectivamente, Omar. Muy bien. Haces el install.packages y pones el paquetito y luego no se te olvide de volver a ejecutar el library. Una vez que lo haya descargado. Vale. De acuerdo. Yo creo que hasta aquí. Está bien. Vale, perfecto. Entonces, ¿qué podemos saber? Podemos querer saber cuántas columnas tienes. Tenemos la opción de poner ncol. ¿De qué? De mi caja con los datos. Entonces tendríamos ncol de data. Aquí abajo. Si lo vemos bien. A mí es que me tapa aquí una cosita. Esto para aquí. Vale, pues tendríamos que son 7 columnas. y podríamos saber cuántas filas son. Sí, ¿cómo? Como n de número row de fila en inglés y data. ¿Qué me va a decir esto? Que existen 13.549 filas, ¿vale? ¿Existe otra opción? Existe otra opción. Yo puedo poner dim de data, dim de dimensión. ¿Y qué me va a dar? Pues filas, columnas, los dos datos a la vez, ¿vale? cuando os lo pase os pondré todo documentado también para que lo tengáis, ¿de acuerdo? bien, ¿qué más puedo querer saber? puedo querer tener una información general de mi dataset esto lo haría con str de data, en este caso fijaros, me dice tienes un formato fecha, me da los valores de las fechas, tienes una columna que es área, que son caracteres porque son las ciudades fijaros que es texto, viene entre las comillas average price es un Número no viene entre comillas. Code es un texto, CHR, que es un carácter, entrecomillado. Y House is sold, number of crimes, ember, flag, son todos números. Entonces aquí ya tendríamos la información de este dataset. Otra de las cosas que es interesante es ver qué métricas estadísticas existirían. Como existe otra función que se llama summary, ponemos summary de data y aquí veremos que tenemos cuál es el valor mínimo, primer cuantil, el valor mediano, la media, el tercer cuantil y el máximo. ¿Qué es el primer cuantil? Vamos a ir a uno que sea más fácil de explicar Primer cuantil Que el primer cuantil sea 132.380 Lo que significa es que El 25% de los datos Ordenados de menor a mayor El 25% de los datos Tienen un valor de 132.380 O menos ¿Qué sería la mediana? Que el 50% de los datos, ordenados de menor a mayor, tienen un precio medio de 222.919 euros o menos. ¿Qué significaría el tercer cuantil? Que el 75% de los datos tienen un precio medio de 336.843 o menos. Para que entendáis los cuantiles que pueden ser yo creo que un poco los más complicados. Entonces cuando hacemos el summary of data lo que me hace es calcularme las principales variables estadísticas. ¿Se entiende hasta aquí? o no se entiende perfecto vale, lo que voy a querer es, si podéis quedar un poquitín más os doy un par de funciones más, vale, y luego ya lo continuamos para lo de la ruta hay que cambiar el tipo de barrita sí Jonathan, vale sí, sí, sí, sí, porque si tenéis o sea, si no tenéis un users o una de no te va a pasar nada, pero si por ejemplo tú tienes como me pasaba a mí el users aquí te va a identificar esto como un carácter raro y no te lo va a leer, mientras que si pones esta otra barra te lo va a leer porque no lo considera un carácter raro esto es tema de ruta relativa y absoluta que bueno, es un tema de eso vale, entonces os quiero dar unas cuantas cositas más hoy pero espero no tardar muchísimo pero para que lo tengáis también para vosotros una de las cosas que hemos dicho es ¿cómo puedo encontrar valores nulos? vale, existe una función que es isNa, cuando hacemos Na ¿de acuerdo? estamos hablando de que es generalmente como se representan los valores nulos. En programación los valores nulos se representan bien con NA, bien con NAN. Son las dos opciones que tenemos. Entonces nosotros ponemos isNA y vamos a nuestro dataset. Fijaros, para cada uno de los valores me va a dar si es verdadero o si es falso. Por ejemplo, vayamos al number of crimes. number of crimes tenemos true, que quiere decir que aquí tenemos un valor nulo houses sold, tenemos un false por tanto aquí hay un dato podemos especificar que me mire solo una columna por ejemplo, quiero la columna houses sold fijaros como he seleccionado la columna la caja, el símbolo de dólar y el nombre de la columna así se seleccionan las columnas caja, símbolo de dólar nombre de la columna le damos a run Y estos son los valores, pero únicamente para la columna que hemos visto. Fijaos que aquí tenemos dos trues, ¿vale? O sea que sí que habría valores nulos, ¿vale? ¿Podríamos hacerlo con poner el nombre de la columna? Sí, pero prefiero que os acostumbréis a poner caja, dólar y nombre de la columna porque no siempre se puede acceder solo al nombre de la columna, ¿vale? aunque verlo así es horrible ¿vale? o sea, verlo así es encontrar una aguja en un pajar ¿cómo podemos hacer para encontrarlo exactamente? vamos a copiar esto ¿vale? y ¿qué queremos hacer? saber cuál es el valor nulo, cuál en inglés es which, por tanto, which y entre los paréntesis que vamos a poner todo lo que habíamos puesto antes, which is na data houses sold ¿vale? Fijaros, aquí me dice el número de fila en la cual teníamos estos valores nulos. ¿Se entiende? Tenéis que pensar que en todo lenguaje de programación funciona muy así. Cuando tenemos varios paréntesis, lo primero que hace es seleccionar la columna. Lo segundo, todo el dataset que teníamos de falsos, falsos, falsos. Y después selecciona el width, selecciona lo que tengamos true. vale pues por ejemplo imaginemos yo creo que con esto podemos terminar por el día de hoy os daré una cosita más y ya está pero imaginemos de acuerdo que nosotros queremos reemplazar la columna 300 quiero reemplazar la columna 300 y el valor de houses hold quiero el valor de la columna houses hold que yo sé que es la columna house hold pero también sé que es la columna esta sería la columna 0, 1, 0, 1, 2, 3, 4, vale, 1, 2, 3, 4, 5, vale, tendríamos la columna 5, vale, entonces voy a reemplazar ese valor, ¿qué haría? Diría, mi conjunto de datos, sé que es data, en la fila 300, fijaros que he añadido corchetes, en la fila 300, la columna 5, vale, Yo lo que quiero es que en vez de ser un valor nulo tenga el valor 11. Muy bien. Vamos a hacerlo. Ya lo tengo. Y si hago view the data. Y me voy a la fila 300. Uy, me he pasado. A ver. Vale, aquí voy a ir poquito a poquito porque si no vamos. 300. fijaros la fila 300 ahora en houses sold en la columna 5 tengo el valor 11 así podemos reemplazar cualquier valor, ¿necesito que sea nulo para reemplazarlo? no no hay necesidad de que sea nulo para reemplazar ¿sí? me alegro de acuerdo lo último que os quiero enseñar es que estoy mirando más o menos para que me cuadre bien para la siguiente clase que la siguiente clase empezamos con los filtros, entonces me interesa daros una cosita más para que también podáis practicar más que nada entonces hacerlo uno a uno sería horroroso estéis de acuerdo conmigo, es decir, hacer uno a uno sería horroroso entonces vamos a hacerlo de manera global con la media yo lo que quiero es reemplazar los valores, ¿de acuerdo? Por la media, ¿vale? Entonces, lo primero que tenemos que pensar es cómo calculamos la media. Y podríais decirme, bueno, con el summary estoy de acuerdo, pero con el summary tenemos muchos datos. Entonces, la media en inglés es mil, ¿vale? Luego hacemos min. Y lo que voy a decir es, ¿de qué columna? De la columna household, perdón, household. Y vamos a añadirle una cosita, ¿vale? Que es, básicamente... decirle que, ojo, mi columna tiene datos vacíos. Esto sería con este na, recordemos, na son valores nulos, y rm podéis recordar que es como un recordatorio. Es decir, estamos diciendo tiene celdas vacías. Entonces lo que vamos a hacer es true. Es decir, recordarle que tiene valores en celdas vacías. Y aquí me sale un valor. Si yo aquí hubiese quitado esto, cuando lo hago, me sale un enea. ¿Por qué? Porque hay valores nulos. Entonces, diciéndole esto estamos diciéndole, ojo, pero sin valores nulos. ¿Sí? ¿Se entiende? Más o menos. Aquí ya se ha perdido un poquito. Vale, bien. Entonces, ¿cómo puedo reemplazarlo? Si yo sé que estos son mis valores nulos, ¿vale? ¿De acuerdo? yo sé que esos son mis valores nulos y yo sé que para de pronto ir poniendo hashtag, a ver si ponéis hashtag tenéis comentarios pero os voy a pasar el código completamente comentado, ¿vale? efectivamente mantenemos en cuenta los valores nulos, con el na.rm igual a true, ¿vale? exactamente, muy bien Víctor, queremos cambiar todos los nulos por la media para no afectar es decir, para no prefijar un caso, ¿vale? Y lo que comentaba Jonathan, si ponéis aquí asteriscos, podéis añadir los comentarios, ¿vale? Igualmente el código que os voy a pasar, ¿vale? Tiene ya los comentarios puestos y todo puesto, con lo cual, digamos que aquí os lo estoy haciendo en tiempo real para que lo veáis, que lo veáis conmigo, que podamos explicarlo, pero luego tengo una cosa ya hecha toda súper comentada, ¿vale? Para que no tengáis ningún tipo de problema y se entienda. lo que pasa es que no puedo hacerlo en tiempo real tan documentado. De acuerdo, entonces, yo sé que estos son los valores que son nulos, necesito seleccionar de mi dataset, de mi caja, solo estos casos y recordemos cómo seleccionábamos, fijaros, con estos corchetes. Entonces, aquí también voy a poner estos corchetes. vale, esta parte lo que hacemos es decirle de mis conjuntos de datos selecciono, los cortetes es como selecciono entonces selecciono la fila 300, la columna 5 que es household y le añado el 11 era un valor nulo y le voy a añadir el valor 11 cambio ese valor nulo por el valor 11 que si tuvieses otro valor distinto también lo estarías cambiando, es decir Si yo, por ejemplo, la fila 305 fuese valor 100, lo que estás haciendo es cambiar ese valor por el valor 11. ¿Sí, Víctor? Los corchetes, pensad que es palabra seleccionar. Seleccionamos algo. Entonces, aquí quiero seleccionar los valores nulos de mi columna. Por lo tanto, ¿qué haré? Seleccionaré la columna. Perdón. Cuidado con mayúsculas y minúsculas. Esto ya sí que es lo último que os voy a enseñar, os lo prometo. seleccionamos la columna y filtramos porque por aquellos valores que son nulos si nos damos cuenta y hacemos esto no son todos los datos sino que son solo aquellos que tenemos los NAs entonces yo esto ¿con qué lo quiero reemplazar? con la media que he calculado antes si yo ahora le doy a view data para poder verlo fijaros que Household ya no va a tener valores nulos. Lo mire por donde lo mire. ¿Por qué? Porque lo he reemplazado con la media de la columna. Entonces, esto es todo por hoy. ¿Se ha entendido? Más o menos. Poco a poco. No espero que me lo sepáis hacer exactamente ahora mismo. Evidentemente. Lo iremos haciendo día a día, clase a clase. Iremos repasándolo clase a clase. Y al final de hacerlo, de hacerlo, de hacerlo, lo acabaréis entendiendo, ¿vale? Así que paciencia con esto. Sí que quería llegar aquí porque el próximo día vamos a hacer también, pues en vez de decir quiero los que son vacíos, voy a querer los que tienen número, ¿vale? Por ejemplo, que es muy sencillito también. Pero bueno, para que practiquemos, ¿vale? Y haremos pues más temas de filtraje, cómo calcular la varianza, cómo calcular media y continuaremos en este sentido, ¿vale? por lo tanto, muy bien hecho hoy, ha sido una clase un poco densa en el punto de vista de informática pero he visto que me la habéis ido siguiendo bien así que enhorabuena, ¿de acuerdo? siento que me he retrasado un poquito, intentaré que para la próxima vez terminar un poquito más en hora, ¿de acuerdo? Muchísimas gracias de verdad por haber estado tan participativos y por la intención que le estáis poniendo, espero que esta clase de verdad os guste, espero que disfrutéis que acabéis aprendiendo y cualquier cosa que necesitas lo subo por la página web de documentación del Moodle si alguien no tiene acceso a ello que por favor me lo diga para abrir incidencia y eso, así que muchísimas gracias yo espero de verdad que os haya gustado y que os guste la sesión, ¿vale? así que nos vemos el próximo día y en el caso de Victor, vale, exactamente, estaba mirando a ver si me respondías el tema lo subo siempre, todos los códigos y todo lo tendréis en vuestro Moodle en vuestro campus virtual, apartado documentación carpeta código R carpeta presentaciones a teoría y así que nos vemos, muchísimas gracias y adiós
